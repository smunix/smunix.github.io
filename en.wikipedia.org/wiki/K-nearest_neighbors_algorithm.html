<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">

<!-- Mirrored from en.wikipedia.org/wiki/K-nearest_neighbors_algorithm by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 31 Dec 2016 05:04:01 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="UTF-8"/>
<title>k-nearest neighbors algorithm - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"K-nearest_neighbors_algorithm","wgTitle":"K-nearest neighbors algorithm","wgCurRevisionId":751879613,"wgRevisionId":751879613,"wgArticleId":1775388,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 errors: external links","CS1 maint: Multiple names: authors list","Pages using ISBN magic links","All articles with unsourced statements","Articles with unsourced statements from September 2016","Articles with unsourced statements from March 2013","Articles with unsourced statements from December 2008","CS1 maint: Uses editors parameter","Classification algorithms","Search algorithms","Machine learning algorithms","Statistical classification","Nonparametric statistics"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"K-nearest_neighbors_algorithm","wgRelevantArticleId":1775388,"wgRequestId":"WGcDHApAIDMAAHCDK9gAAABN","wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","usePageImages":true,"usePageDescriptions":true},"wgPreferredVariant":"en","wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikibaseItemId":"Q1071612","wgCentralAuthMobileDomain":false,"wgVisualEditorToolbarScrollOffset":0,"wgEditSubmitButtonLabelPublish":false});mw.loader.state({"ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","user":"ready","user.options":"loading","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","mediawiki.page.gallery.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready"});mw.loader.implement("user.options@0j3lz3q",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens@1dqfd7l",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["ext.cite.a11y","ext.math.scripts","mediawiki.toc","mediawiki.action.view.postEdit","site","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.legacy.wikibits","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.gadget.featured-articles-links","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});</script>
<link rel="stylesheet" href="https://en.wikipedia.org/w/load.php?debug=false&amp;lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.page.gallery.styles%7Cmediawiki.sectionAnchor%7Cmediawiki.skinning.interface%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="https://en.wikipedia.org/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="https://en.wikipedia.org/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.29.0-wmf.6"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" href="https://en.wikipedia.org/wiki/android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/K-nearest_neighbors_algorithm"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit"/>
<link rel="apple-touch-icon" href="https://en.wikipedia.org/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="https://en.wikipedia.org/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="https://en.wikipedia.org/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="http://en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="copyright" href="http://creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="K-nearest_neighbors_algorithm.html"/>
<link rel="dns-prefetch" href="http://login.wikimedia.org/"/>
<link rel="dns-prefetch" href="http://meta.wikimedia.org/" />
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-K-nearest_neighbors_algorithm rootpage-K-nearest_neighbors_algorithm skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>

							<div id="siteNotice"><!-- CentralNotice --></div>
						<div class="mw-indicators">
</div>
			<h1 id="firstHeading" class="firstHeading" lang="en"><i>k</i>-nearest neighbors algorithm</h1>
									<div id="bodyContent" class="mw-body-content">
									<div id="siteSub">From Wikipedia, the free encyclopedia</div>
								<div id="contentSub"></div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="#mw-head">navigation</a>, 					<a href="#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><script>function mfTempOpenSection(id){var block=document.getElementById("mf-section-"+id);block.className+=" open-block";block.previousSibling.className+=" open-block";}</script><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%">
<tr>
<th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="Machine_learning" title="Machine learning">Machine learning</a> and<br />
<a href="https://en.wikipedia.org/wiki/Data_mining" title="Data mining">data mining</a></th>
</tr>
<tr>
<td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="https://en.wikipedia.org/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="232" /></a></td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="https://en.wikipedia.org/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="https://en.wikipedia.org/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
<div style="padding:0.1em 0;line-height:1.2em;"><a href="https://en.wikipedia.org/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br />
<span style="font-weight:normal;"><small>(<b><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;• <b><a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</small></span></div>
</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a> (<a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a>, <a href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a>, <a href="https://en.wikipedia.org/wiki/Random_forest" title="Random forest">Random forest</a>)</li>
<li><strong class="selflink"><i>k</i>-NN</strong></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">Neural networks</a></li>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="https://en.wikipedia.org/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">Expectation-maximization (EM)</a></li>
<li><br />
<a href="https://en.wikipedia.org/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="https://en.wikipedia.org/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Canonical_correlation_analysis" class="mw-redirect" title="Canonical correlation analysis">CCA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Graphical_model" title="Graphical model">Graphical models</a> (<a href="https://en.wikipedia.org/wiki/Bayesian_network" title="Bayesian network">Bayes net</a>, <a href="https://en.wikipedia.org/wiki/Conditional_random_field" title="Conditional random field">CRF</a>, <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" title="Hidden Markov model">HMM</a>)</li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_classification" class="mw-redirect" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">Neural nets</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Reinforcement_Learning" class="mw-redirect" title="Reinforcement Learning">Reinforcement Learning</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Q-Learning" class="mw-redirect" title="Q-Learning">Q-Learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/State-Action-Reward-State-Action" title="State-Action-Reward-State-Action">SARSA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Temporal_Difference_Learning" class="mw-redirect" title="Temporal Difference Learning">Temporal Difference (TD)</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Bias-variance_dilemma" class="mw-redirect" title="Bias-variance dilemma">Bias-variance dilemma</a></li>
<li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Vapnik–Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine learning venues</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a></li>
<li><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="https://en.wikipedia.org/w/index.php?title=International_Journal_of_Machine_Learning_and_Cybernetics&amp;action=edit&amp;redlink=1" class="new" title="International Journal of Machine Learning and Cybernetics (page does not exist)">IJMLC</a></li>
<li><a href="https://en.wikipedia.org/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a rel="nofollow" class="external text" href="http://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td class="plainlist" style="padding:0.3em 0.4em 0.3em;font-weight:bold;border-top: 1px solid #aaa; border-bottom: 1px solid #aaa;border-top:1px solid #aaa;border-bottom:1px solid #aaa;">
<ul>
<li><a href="https://en.wikipedia.org/wiki/File:Portal-puzzle.svg" class="image"><img alt="Portal-puzzle.svg" src="http://upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png" width="16" height="14" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28" /></a> <a href="https://en.wikipedia.org/wiki/Portal:Machine_learning" title="Portal:Machine learning">Machine learning portal</a></li>
</ul>
</td>
</tr>
<tr>
<td style="text-align:right;font-size:115%;padding-top: 0.6em;">
<div class="plainlinks hlist navbar mini">
<ul>
<li class="nv-view"><a href="https://en.wikipedia.org/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li>
<li class="nv-talk"><a href="https://en.wikipedia.org/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li>
<li class="nv-edit"><a class="external text" href="http://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li>
</ul>
</div>
</td>
</tr>
</table>
<p>In <a href="https://en.wikipedia.org/wiki/Pattern_recognition" title="Pattern recognition">pattern recognition</a>, the <b><i>k</i>-Nearest Neighbors algorithm</b> (or <b><i>k</i>-NN</b> for short) is a <a href="https://en.wikipedia.org/wiki/Non-parametric_statistics" class="mw-redirect" title="Non-parametric statistics">non-parametric</a> method used for <a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">classification</a> and <a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">regression</a>.<sup id="cite_ref-1" class="reference"><a href="#cite_note-1">[1]</a></sup> In both cases, the input consists of the <i>k</i> closest training examples in the <a href="https://en.wikipedia.org/wiki/Feature_space" class="mw-redirect" title="Feature space">feature space</a>. The output depends on whether <i>k</i>-NN is used for classification or regression:</p>
<dl>
<dd>
<ul>
<li>In <i>k-NN classification</i>, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its <i>k</i> nearest neighbors (<i>k</i> is a positive <a href="https://en.wikipedia.org/wiki/Integer" title="Integer">integer</a>, typically small). If <i>k</i>&#160;=&#160;1, then the object is simply assigned to the class of that single nearest neighbor.</li>
</ul>
</dd>
</dl>
<dl>
<dd>
<ul>
<li>In <i>k-NN regression</i>, the output is the property value for the object. This value is the average of the values of its <i>k</i> nearest neighbors.</li>
</ul>
</dd>
</dl>
<p><i>k</i>-NN is a type of <a href="https://en.wikipedia.org/wiki/Instance-based_learning" title="Instance-based learning">instance-based learning</a>, or <a href="https://en.wikipedia.org/wiki/Lazy_learning" title="Lazy learning">lazy learning</a>, where the function is only approximated locally and all computation is deferred until classification. The <i>k</i>-NN algorithm is among the simplest of all <a href="Machine_learning" title="Machine learning">machine learning</a> algorithms.</p>
<p>Both for classification and regression, it can be useful to assign weight to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/<i>d</i>, where <i>d</i> is the distance to the neighbor.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">[2]</a></sup></p>
<p>The neighbors are taken from a set of objects for which the class (for <i>k</i>-NN classification) or the object property value (for <i>k</i>-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.</p>
<p>A shortcoming of the <i>k</i>-NN algorithm is that it is sensitive to the local structure of the data.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (September 2016)">citation needed</span></a></i>]</sup> The algorithm is not to be confused with <a href="https://en.wikipedia.org/wiki/K-means" class="mw-redirect" title="K-means"><i>k</i>-means</a>, another popular <a href="Machine_learning" title="Machine learning">machine learning</a> technique.</p>
<p></p>
<div id="toc" class="toc">
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Statistical_setting"><span class="tocnumber">1</span> <span class="toctext">Statistical setting</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Algorithm"><span class="tocnumber">2</span> <span class="toctext">Algorithm</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Parameter_selection"><span class="tocnumber">3</span> <span class="toctext">Parameter selection</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#The_1-nearest_neighbour_classifier"><span class="tocnumber">4</span> <span class="toctext">The <span>1</span>-nearest neighbour classifier</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#The_weighted_nearest_neighbour_classifier"><span class="tocnumber">5</span> <span class="toctext">The weighted nearest neighbour classifier</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Properties"><span class="tocnumber">6</span> <span class="toctext">Properties</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#Error_rates"><span class="tocnumber">7</span> <span class="toctext">Error rates</span></a></li>
<li class="toclevel-1 tocsection-8"><a href="#Metric_learning"><span class="tocnumber">8</span> <span class="toctext">Metric learning</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Feature_extraction"><span class="tocnumber">9</span> <span class="toctext">Feature extraction</span></a></li>
<li class="toclevel-1 tocsection-10"><a href="#Dimension_reduction"><span class="tocnumber">10</span> <span class="toctext">Dimension reduction</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="#Decision_boundary"><span class="tocnumber">11</span> <span class="toctext">Decision boundary</span></a></li>
<li class="toclevel-1 tocsection-12"><a href="#Extension_of_k-NN_.28ENN.29_for_classification"><span class="tocnumber">12</span> <span class="toctext">Extension of <i>k-NN</i> (ENN) for classification</span></a></li>
<li class="toclevel-1 tocsection-13"><a href="#Data_reduction"><span class="tocnumber">13</span> <span class="toctext">Data reduction</span></a>
<ul>
<li class="toclevel-2 tocsection-14"><a href="#Selection_of_class-outliers"><span class="tocnumber">13.1</span> <span class="toctext">Selection of class-outliers</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#CNN_for_data_reduction"><span class="tocnumber">13.2</span> <span class="toctext">CNN for data reduction</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-16"><a href="#k-NN_regression"><span class="tocnumber">14</span> <span class="toctext"><i>k</i>-NN regression</span></a></li>
<li class="toclevel-1 tocsection-17"><a href="#k-NN_outlier"><span class="tocnumber">15</span> <span class="toctext"><i>k</i>-NN outlier</span></a></li>
<li class="toclevel-1 tocsection-18"><a href="#Validation_of_results"><span class="tocnumber">16</span> <span class="toctext">Validation of results</span></a></li>
<li class="toclevel-1 tocsection-19"><a href="#See_also"><span class="tocnumber">17</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-20"><a href="#References"><span class="tocnumber">18</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-21"><a href="#Further_reading"><span class="tocnumber">19</span> <span class="toctext">Further reading</span></a></li>
</ul>
</div>
<p></p>
<h2><span class="mw-headline" id="Statistical_setting">Statistical setting</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=1" title="Edit section: Statistical setting">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Suppose we have pairs <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo>,</mo>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (X,Y),(X_{1},Y_{1}),\dots ,(X_{n},Y_{n})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a23d2a835fc4b20b2f97f36190f39eb178ca1fee" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:29.818ex; height:2.843ex;" alt="(X,Y),(X_{1},Y_{1}),\dots ,(X_{n},Y_{n})" /></span> taking values in <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
        <mo>&#x00D7;<!-- × --></mo>
        <mo fence="false" stretchy="false">{</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>2</mn>
        <mo fence="false" stretchy="false">}</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbb {R} ^{d}\times \{1,2\}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/60671efc51865fcfac4e8939ab2acb643539302d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:11.375ex; height:3.176ex;" alt="{\mathbb  {R}}^{d}\times \{1,2\}" /></span>, where <span class="texhtml mvar" style="font-style:italic;">Y</span> is the class label of <span class="texhtml mvar" style="font-style:italic;">X</span>, so that <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>X</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>Y</mi>
        <mo>=</mo>
        <mi>r</mi>
        <mo>&#x223C;<!-- ∼ --></mo>
        <msub>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>r</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle X|Y=r\sim P_{r}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9e1b6a01b179f721be03ed284c21e2d3cbb70d4a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:14.192ex; height:2.843ex;" alt="X|Y=r\sim P_{r}" /></span> for <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>r</mi>
        <mo>=</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>2</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle r=1,2}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6dcdedf1b83451b9a51dc73b95f854eeaf83dc46" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:7.558ex; height:2.509ex;" alt="r=1,2" /></span> (and probability distributions <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>P</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>r</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P_{r}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3e7f814e80c6ff1469112bd4b7430e358e86c7d6" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.484ex; height:2.509ex;" alt="P_{r}" /></span>). Given some norm <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">&#x2225;<!-- ∥ --></mo>
        <mo>&#x22C5;<!-- ⋅ --></mo>
        <mo fence="false" stretchy="false">&#x2225;<!-- ∥ --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|\cdot \|}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/113f0d8fe6108fc1c5e9802f7c3f634f5480b3d1" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.035ex; height:2.843ex;" alt="\|\cdot \|" /></span> on <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \mathbb {R} ^{d}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a713426956296f1668fce772df3c60b9dde8a685" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.788ex; height:2.676ex;" alt="\mathbb {R} ^{d}" /></span> and a point <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>x</mi>
        <mo>&#x2208;<!-- ∈ --></mo>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mi mathvariant="double-struck">R</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x\in \mathbb {R} ^{d}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f351538c1465ec3881164b501f612b1f54cbfe7e" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:6.979ex; height:2.676ex;" alt="x\in {\mathbb  {R}}^{d}" /></span>, let <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <mo stretchy="false">(</mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>n</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo>,</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>n</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (X_{(1)},Y_{(1)}),\dots ,(X_{(n)},Y_{(n)})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/961b632956473933c75b8cbfc56f8a83ccd41524" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.171ex; width:27.302ex; height:3.176ex;" alt="(X_{{(1)}},Y_{{(1)}}),\dots ,(X_{{(n)}},Y_{{(n)}})" /></span> be a reordering of the training data such that <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">&#x2225;<!-- ∥ --></mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo>&#x2212;<!-- − --></mo>
        <mi>x</mi>
        <mo>&#x2225;<!-- ∥ --></mo>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mo>&#x22EF;<!-- ⋯ --></mo>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mo>&#x2225;<!-- ∥ --></mo>
        <msub>
          <mi>X</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>n</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
        <mo>&#x2212;<!-- − --></mo>
        <mi>x</mi>
        <mo fence="false" stretchy="false">&#x2225;<!-- ∥ --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|X_{(1)}-x\|\leq \dots \leq \|X_{(n)}-x\|}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/364eb25128db023e58f0dc8584f7e6de07e94907" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.171ex; width:30.77ex; height:3.176ex;" alt="\|X_{{(1)}}-x\|\leq \dots \leq \|X_{{(n)}}-x\|" /></span>.</p>
<h2><span class="mw-headline" id="Algorithm">Algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=2" title="Edit section: Algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright">
<div class="thumbinner" style="width:222px;"><a href="https://en.wikipedia.org/wiki/File:KnnClassification.svg" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/220px-KnnClassification.svg.png" width="220" height="199" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/330px-KnnClassification.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e7/KnnClassification.svg/440px-KnnClassification.svg.png 2x" data-file-width="279" data-file-height="252" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="https://en.wikipedia.org/wiki/File:KnnClassification.svg" class="internal" title="Enlarge"></a></div>
Example of <i>k</i>-NN classification. The test sample (green circle) should be classified either to the first class of blue squares or to the second class of red triangles. If <i>k = 3</i> (solid line circle) it is assigned to the second class because there are 2 triangles and only 1 square inside the inner circle. If <i>k = 5</i> (dashed line circle) it is assigned to the first class (3 squares vs. 2 triangles inside the outer circle).</div>
</div>
</div>
<p>The training examples are vectors in a multidimensional feature space, each with a class label. The training phase of the algorithm consists only of storing the <a href="https://en.wikipedia.org/wiki/Feature_vector" title="Feature vector">feature vectors</a> and class labels of the training samples.</p>
<p>In the classification phase, <i>k</i> is a user-defined constant, and an unlabeled vector (a query or test point) is classified by assigning the label which is most frequent among the <i>k</i> training samples nearest to that query point.</p>
<p>A commonly used distance metric for <a href="https://en.wikipedia.org/wiki/Continuous_variable" class="mw-redirect" title="Continuous variable">continuous variables</a> is <a href="https://en.wikipedia.org/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a>. For discrete variables, such as for text classification, another metric can be used, such as the <b>overlap metric</b> (or <a href="https://en.wikipedia.org/wiki/Hamming_distance" title="Hamming distance">Hamming distance</a>). In the context of gene expression microarray data, for example, <i>k</i>-NN has also been employed with correlation coefficients such as Pearson and Spearman.<sup id="cite_ref-3" class="reference"><a href="#cite_note-3">[3]</a></sup> Often, the classification accuracy of <i>k</i>-NN can be improved significantly if the distance metric is learned with specialized algorithms such as <a href="https://en.wikipedia.org/wiki/Large_Margin_Nearest_Neighbor" class="mw-redirect" title="Large Margin Nearest Neighbor">Large Margin Nearest Neighbor</a> or <a href="https://en.wikipedia.org/wiki/Neighbourhood_components_analysis" title="Neighbourhood components analysis">Neighbourhood components analysis</a>.</p>
<p>A drawback of the basic "majority voting" classification occurs when the class distribution is skewed. That is, examples of a more frequent class tend to dominate the prediction of the new example, because they tend to be common among the <i>k</i> nearest neighbors due to their large number.<sup id="cite_ref-Coomans_Massart1982_4-0" class="reference"><a href="#cite_note-Coomans_Massart1982-4">[4]</a></sup> One way to overcome this problem is to weight the classification, taking into account the distance from the test point to each of its <i>k</i> nearest neighbors. The class (or value, in regression problems) of each of the <i>k</i> nearest points is multiplied by a weight proportional to the inverse of the distance from that point to the test point. Another way to overcome skew is by abstraction in data representation. For example, in a <a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">self-organizing map</a> (SOM), each node is a representative (a center) of a cluster of similar points, regardless of their density in the original training data. <i>K</i>-NN can then be applied to the SOM.</p>
<h2><span class="mw-headline" id="Parameter_selection">Parameter selection</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=3" title="Edit section: Parameter selection">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The best choice of <i>k</i> depends upon the data; generally, larger values of <i>k</i> reduce the effect of noise on the classification,<sup id="cite_ref-5" class="reference"><a href="#cite_note-5">[5]</a></sup> but make boundaries between classes less distinct. A good <i>k</i> can be selected by various <a href="https://en.wikipedia.org/wiki/Heuristic_(computer_science)" title="Heuristic (computer science)">heuristic</a> techniques (see <a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization" title="Hyperparameter optimization">hyperparameter optimization</a>). The special case where the class is predicted to be the class of the closest training sample (i.e. when <i>k</i> = 1) is called the nearest neighbor algorithm.</p>
<p>The accuracy of the <i>k</i>-NN algorithm can be severely degraded by the presence of noisy or irrelevant features, or if the feature scales are not consistent with their importance. Much research effort has been put into <a href="https://en.wikipedia.org/wiki/Feature_selection" title="Feature selection">selecting</a> or <a href="https://en.wikipedia.org/wiki/Feature_scaling" title="Feature scaling">scaling</a> features to improve classification. A particularly popular<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (March 2013)">citation needed</span></a></i>]</sup> approach is the use of <a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm" title="Evolutionary algorithm">evolutionary algorithms</a> to optimize feature scaling.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">[6]</a></sup> Another popular approach is to scale features by the <a href="https://en.wikipedia.org/wiki/Mutual_information" title="Mutual information">mutual information</a> of the training data with the training classes.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (December 2008)">citation needed</span></a></i>]</sup></p>
<p>In binary (two class) classification problems, it is helpful to choose <i>k</i> to be an odd number as this avoids tied votes. One popular way of choosing the empirically optimal <i>k</i> in this setting is via bootstrap method.<sup id="cite_ref-HPS2008_7-0" class="reference"><a href="#cite_note-HPS2008-7">[7]</a></sup></p>
<h2><span class="mw-headline" id="The_1-nearest_neighbour_classifier">The <span class="texhtml">1</span>-nearest neighbour classifier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=4" title="Edit section: The 1-nearest neighbour classifier">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The most intuitive nearest neighbour type classifier is the one nearest neighbour classifier that assigns a point <span class="texhtml mvar" style="font-style:italic;">x</span> to the class of its closest neighbour in the feature space, that is <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
            <mi>n</mi>
            <mi>n</mi>
          </mrow>
        </msubsup>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <msub>
          <mi>Y</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C_{n}^{1nn}(x)=Y_{(1)}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f815904edbff2ce82502172ec0dce3311d57f2bb" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.171ex; width:14.853ex; height:3.343ex;" alt="C_{n}^{{1nn}}(x)=Y_{{(1)}}" /></span>.</p>
<p>As the size of training data set approaches infinity, the one nearest neighbour classifier guarantees an error rate of no worse than twice the <a href="https://en.wikipedia.org/wiki/Bayes_error_rate" title="Bayes error rate">Bayes error rate</a> (the minimum achievable error rate given the distribution of the data).</p>
<h2><span class="mw-headline" id="The_weighted_nearest_neighbour_classifier">The weighted nearest neighbour classifier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=5" title="Edit section: The weighted nearest neighbour classifier">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The <span class="texhtml mvar" style="font-style:italic;">k</span>-nearest neighbour classifier can be viewed as assigning the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbours a weight <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mn>1</mn>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi>k</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 1/k}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a7e9fedad8c70c6331b2640b56c23cef8c884e1f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:3.567ex; height:2.843ex;" alt="1/k" /></span> and all others <span class="texhtml mvar" style="font-style:italic;">0</span> weight. This can be generalised to weighted nearest neighbour classifiers. That is, where the <span class="texhtml mvar" style="font-style:italic;">i</span>th nearest neighbour is assigned a weight <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ni}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f6d76326293e410139d081d073068b9eb32a0777" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:3.475ex; height:2.009ex;" alt="w_{{ni}}" /></span>, with <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sum _{i=1}^{n}w_{ni}=1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/167dcdfe9d31faea6f1e6c157cc23fe6e3b39fb7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:11.51ex; height:7.009ex;" alt="\sum _{{i=1}}^{n}w_{{ni}}=1" /></span>. An analogous result on the strong consistency of weighted nearest neighbour classifiers also holds.<sup id="cite_ref-Stone_8-0" class="reference"><a href="#cite_note-Stone-8">[8]</a></sup></p>
<p>Let <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>w</mi>
            <mi>n</mi>
            <mi>n</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C_{n}^{wnn}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e88b657ee88d912408396f8c9ef6af3483bfdf01" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:5.212ex; height:2.509ex;" alt="C_{n}^{{wnn}}" /></span> denote the weighted nearest classifier with weights <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">{</mo>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
        </msub>
        <msubsup>
          <mo fence="false" stretchy="false">}</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \{w_{ni}\}_{i=1}^{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/efaf258e02ccae2b27c279885d6fb898adaf331d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:8.743ex; height:3.009ex;" alt="\{w_{{ni}}\}_{{i=1}}^{n}" /></span>. Subject to regularity conditions on to class distributions the excess risk has the following asymptotic expansion<sup id="cite_ref-Samworth12_9-0" class="reference"><a href="#cite_note-Samworth12-9">[9]</a></sup></p>
<dl>
<dd><span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msubsup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>w</mi>
            <mi>n</mi>
            <mi>n</mi>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>B</mi>
            <mi>a</mi>
            <mi>y</mi>
            <mi>e</mi>
            <mi>s</mi>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow>
          <mo>(</mo>
          <msub>
            <mi>B</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>1</mn>
            </mrow>
          </msub>
          <msubsup>
            <mi>s</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>n</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>2</mn>
            </mrow>
          </msubsup>
          <mo>+</mo>
          <msub>
            <mi>B</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>2</mn>
            </mrow>
          </msub>
          <msubsup>
            <mi>t</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mi>n</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>2</mn>
            </mrow>
          </msubsup>
          <mo>)</mo>
        </mrow>
        <mo fence="false" stretchy="false">{</mo>
        <mn>1</mn>
        <mo>+</mo>
        <mi>o</mi>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mo fence="false" stretchy="false">}</mo>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{wnn})-{\mathcal {R}}_{\mathcal {R}}(C^{Bayes})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/02f064c830dbca4fcd695427bcc45c7aeb1b1196" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.171ex; width:55.241ex; height:3.509ex;" alt="{\mathcal  {R}}_{{\mathcal  {R}}}(C_{{n}}^{{wnn}})-{\mathcal  {R}}_{{{\mathcal  {R}}}}(C^{{Bayes}})=\left(B_{1}s_{n}^{2}+B_{2}t_{n}^{2}\right)\{1+o(1)\}," /></span></dd>
</dl>
<p>for constants <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>B</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle B_{1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fa091eb428443c9c5c5fcf32a69d3665c89e00c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.836ex; height:2.509ex;" alt="B_{1}" /></span> and <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>B</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle B_{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/199944d59dcc18842dfd1deab6000a1d1dadcbae" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.836ex; height:2.509ex;" alt="B_{2}" /></span> where <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>s</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle s_{n}^{2}=\sum _{i=1}^{n}w_{ni}^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ed6dbd702b3141f1649ce10ccff3bac0acd55299" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:12.663ex; height:7.009ex;" alt="s_{n}^{2}=\sum _{{i=1}}^{n}w_{{ni}}^{2}" /></span> and <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>t</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mo>=</mo>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2212;<!-- − --></mo>
            <mn>2</mn>
            <mrow class="MJX-TeXAtom-ORD">
              <mo>/</mo>
            </mrow>
            <mi>d</mi>
          </mrow>
        </msup>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </munderover>
        <msub>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
        </msub>
        <mo fence="false" stretchy="false">{</mo>
        <msup>
          <mi>i</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
            <mo>+</mo>
            <mn>2</mn>
            <mrow class="MJX-TeXAtom-ORD">
              <mo>/</mo>
            </mrow>
            <mi>d</mi>
          </mrow>
        </msup>
        <mo>&#x2212;<!-- − --></mo>
        <mo stretchy="false">(</mo>
        <mi>i</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mn>1</mn>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
            <mo>+</mo>
            <mn>2</mn>
            <mrow class="MJX-TeXAtom-ORD">
              <mo>/</mo>
            </mrow>
            <mi>d</mi>
          </mrow>
        </msup>
        <mo fence="false" stretchy="false">}</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle t_{n}=n^{-2/d}\sum _{i=1}^{n}w_{ni}\{i^{1+2/d}-(i-1)^{1+2/d}\}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cd32f71ab3cd0784e73324108ecb05be734cd7de" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.005ex; width:40.672ex; height:7.009ex;" alt="t_{n}=n^{{-2/d}}\sum _{{i=1}}^{n}w_{{ni}}\{i^{{1+2/d}}-(i-1)^{{1+2/d}}\}" /></span>.</p>
<p>The optimal weighting scheme <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">{</mo>
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msubsup>
        <msubsup>
          <mo fence="false" stretchy="false">}</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \{w_{ni}^{*}\}_{i=1}^{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f97b387c9e937fac91f0644ac895c5c95d9a4921" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:8.743ex; height:3.009ex;" alt="\{w_{{ni}}^{*}\}_{{i=1}}^{n}" /></span>, that balances the two terms in the display above, is given as follows: set <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mo fence="false" stretchy="false">&#x230A;<!-- ⌊ --></mo>
        <mi>B</mi>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mfrac>
              <mn>4</mn>
              <mrow>
                <mi>d</mi>
                <mo>+</mo>
                <mn>4</mn>
              </mrow>
            </mfrac>
          </mrow>
        </msup>
        <mo fence="false" stretchy="false">&#x230B;<!-- ⌋ --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5cbecc881f1b8637b3d4d4527fd1671f5be252fa" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:14.147ex; height:4.176ex;" alt="k^{*}=\lfloor Bn^{{{\frac  4{d+4}}}}\rfloor " /></span>,</p>
<dl>
<dd><span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <msup>
              <mi>k</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mo>&#x2217;<!-- ∗ --></mo>
              </mrow>
            </msup>
          </mfrac>
        </mrow>
        <mrow>
          <mo>[</mo>
          <mn>1</mn>
          <mo>+</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mfrac>
              <mi>d</mi>
              <mn>2</mn>
            </mfrac>
          </mrow>
          <mo>&#x2212;<!-- − --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mfrac>
              <mi>d</mi>
              <mrow>
                <mn>2</mn>
                <msup>
                  <mrow class="MJX-TeXAtom-ORD">
                    <msup>
                      <mi>k</mi>
                      <mrow class="MJX-TeXAtom-ORD">
                        <mo>&#x2217;<!-- ∗ --></mo>
                      </mrow>
                    </msup>
                  </mrow>
                  <mrow class="MJX-TeXAtom-ORD">
                    <mn>2</mn>
                    <mrow class="MJX-TeXAtom-ORD">
                      <mo>/</mo>
                    </mrow>
                    <mi>d</mi>
                  </mrow>
                </msup>
              </mrow>
            </mfrac>
          </mrow>
          <mo fence="false" stretchy="false">{</mo>
          <msup>
            <mi>i</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>1</mn>
              <mo>+</mo>
              <mn>2</mn>
              <mrow class="MJX-TeXAtom-ORD">
                <mo>/</mo>
              </mrow>
              <mi>d</mi>
            </mrow>
          </msup>
          <mo>&#x2212;<!-- − --></mo>
          <mo stretchy="false">(</mo>
          <mi>i</mi>
          <mo>&#x2212;<!-- − --></mo>
          <mn>1</mn>
          <msup>
            <mo stretchy="false">)</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>1</mn>
              <mo>+</mo>
              <mn>2</mn>
              <mrow class="MJX-TeXAtom-ORD">
                <mo>/</mo>
              </mrow>
              <mi>d</mi>
            </mrow>
          </msup>
          <mo fence="false" stretchy="false">}</mo>
          <mo>]</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ni}^{*}={\frac {1}{k^{*}}}\left[1+{\frac {d}{2}}-{\frac {d}{2{k^{*}}^{2/d}}}\{i^{1+2/d}-(i-1)^{1+2/d}\}\right]}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fbfa4058134234385c31544db3e657c4b242ab34" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -2.671ex; width:50.977ex; height:6.343ex;" alt="w_{{ni}}^{*}={\frac  1{k^{*}}}\left[1+{\frac  d2}-{\frac  d{2{k^{*}}^{{2/d}}}}\{i^{{1+2/d}}-(i-1)^{{1+2/d}}\}\right]" /></span> for <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
        <mo>=</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mn>2</mn>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i=1,2,\dots ,k^{*}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4ea7e974f7466c9dedfe409ea013bc719264872b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:14.805ex; height:2.676ex;" alt="i=1,2,\dots ,k^{*}" /></span> and</dd>
<dd><span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>w</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mn>0</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle w_{ni}^{*}=0}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6bd3d5b77d7fef0dabd4326ee85b04fa244fa988" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:7.757ex; height:2.843ex;" alt="w_{{ni}}^{*}=0" /></span> for <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>i</mi>
        <mo>=</mo>
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mo>+</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle i=k^{*}+1,\dots ,n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b7c2f2e5c12f3febcefe0aae189d44031daf8e79" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:16.844ex; height:2.676ex;" alt="i=k^{*}+1,\dots ,n" /></span>.</dd>
</dl>
<p>With optimal weights the dominant term in the asymptotic expansion of the excess risk is <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2212;<!-- − --></mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mn>4</mn>
                <mrow>
                  <mi>d</mi>
                  <mo>+</mo>
                  <mn>4</mn>
                </mrow>
              </mfrac>
            </mrow>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07280735a8852d609ffd3942647d7e3255697f05" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:9.871ex; height:4.176ex;" alt="{\mathcal  {O}}(n^{{-{\frac  4{d+4}}}})" /></span>. Similar results are true when using a <a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">bagged nearest neighbour classifier</a>.</p>
<h2><span class="mw-headline" id="Properties">Properties</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=6" title="Edit section: Properties">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><i>k</i>-NN is a special case of a <a href="https://en.wikipedia.org/wiki/Variable_kernel_density_estimation" title="Variable kernel density estimation">variable-bandwidth, kernel density "balloon" estimator</a> with a uniform <a href="https://en.wikipedia.org/wiki/Kernel_(statistics)" title="Kernel (statistics)">kernel</a>.<sup id="cite_ref-Terrell_Scott1992_10-0" class="reference"><a href="#cite_note-Terrell_Scott1992-10">[10]</a></sup> <sup id="cite_ref-Mills2010_11-0" class="reference"><a href="#cite_note-Mills2010-11">[11]</a></sup></p>
<p>The naive version of the algorithm is easy to implement by computing the distances from the test example to all stored examples, but it is computationally intensive for large training sets. Using an appropriate <a href="https://en.wikipedia.org/wiki/Nearest_neighbor_search" title="Nearest neighbor search">nearest neighbor search</a> algorithm makes <i>k-</i>NN computationally tractable even for large data sets. Many nearest neighbor search algorithms have been proposed over the years; these generally seek to reduce the number of distance evaluations actually performed.</p>
<p><i>k-</i>NN has some strong <a href="https://en.wikipedia.org/wiki/Consistency_(statistics)" title="Consistency (statistics)">consistency</a> results. As the amount of data approaches infinity, the two-class <i>k-</i>NN algorithm is guaranteed to yield an error rate no worse than twice the <a href="https://en.wikipedia.org/wiki/Bayes_error_rate" title="Bayes error rate">Bayes error rate</a> (the minimum achievable error rate given the distribution of the data).<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">[12]</a></sup> Various improvements to the <i>k</i>-NN speed are possible by using proximity graphs.<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">[13]</a></sup></p>
<p>For multi-class <i>k-</i>NN classification, Cover and Hart (1967) prove an upper bound error rate of</p>
<p><span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mtext>&#xA0;</mtext>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mtext>&#xA0;</mtext>
        <msub>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
            <mi>N</mi>
            <mi>N</mi>
          </mrow>
        </msub>
        <mtext>&#xA0;</mtext>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mtext>&#xA0;</mtext>
        <msup>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mo stretchy="false">(</mo>
        <mn>2</mn>
        <mo>&#x2212;<!-- − --></mo>
        <mi>M</mi>
        <msup>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mo stretchy="false">(</mo>
        <mi>M</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle R^{*}\ \leq \ R_{kNN}\ \leq \ R^{*}(2-MR^{*}/(M-1))}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ac235b4caca9ddafadfa8634aae8b12b5a1297a0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:40.639ex; height:2.843ex;" alt="{\displaystyle R^{*}\ \leq \ R_{kNN}\ \leq \ R^{*}(2-MR^{*}/(M-1))}" /></span></p>
<p>where <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle R^{*}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a74b1bc0fa98794b6460254044f8e7b75e6d84f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.836ex; height:2.343ex;" alt="R^{*}" /></span>is the Bayes error rate (which is the minimal error rate possible), <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
            <mi>N</mi>
            <mi>N</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle R_{kNN}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1f3666637c81f02c1bfeb4e1969828745062aeba" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:5.804ex; height:2.509ex;" alt="{\displaystyle R_{kNN}}" /></span> is the <i>k-</i>NN error rate, and <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>M</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle M}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.453ex; height:2.176ex;" alt="M" /></span> is the number of classes in the problem. For <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>M</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle M}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/f82cade9898ced02fdd08712e5f0c0151758a0dd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.453ex; height:2.176ex;" alt="M" /></span>=2 and as the Bayesian error rate <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>R</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle R^{*}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a74b1bc0fa98794b6460254044f8e7b75e6d84f" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.836ex; height:2.343ex;" alt="R^{*}" /></span> approaches zero, this limit reduces to "not more than twice the Bayesian error rate".</p>
<h2><span class="mw-headline" id="Error_rates">Error rates</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=7" title="Edit section: Error rates">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>There are many results on the error rate of the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbour classifiers.<sup id="cite_ref-PTPR_14-0" class="reference"><a href="#cite_note-PTPR-14">[14]</a></sup> The <span class="texhtml mvar" style="font-style:italic;">k</span>-nearest neighbour classifier is strongly (that is for any joint distribution on <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mi>X</mi>
        <mo>,</mo>
        <mi>Y</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (X,Y)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/41f29b9537685f499713112d6802e811cbf51bba" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:6.649ex; height:2.843ex;" alt="(X,Y)" /></span>) <a href="https://en.wikipedia.org/wiki/Bayes_classifier" title="Bayes classifier">consistent</a> provided <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>:=</mo>
        <msub>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k:=k_{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0fccbcda1dd871e437cca392a4294e6affdb5692" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:7.435ex; height:2.509ex;" alt="k:=k_{n}" /></span> diverges and <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msub>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k_{n}/n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4e9a4dfc54979e5190d6baa3cc552824357edca1" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.026ex; height:2.843ex;" alt="k_{n}/n" /></span> converges to zero as <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>n</mi>
        <mo stretchy="false">&#x2192;<!-- → --></mo>
        <mi mathvariant="normal">&#x221E;<!-- ∞ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle n\to \infty }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a0d55d9b32f6fa8fab6a84ea444a6b5a24bb45e1" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:7.364ex; height:1.843ex;" alt="n\to \infty " /></span>.</p>
<p>Let <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
            <mi>n</mi>
            <mi>n</mi>
          </mrow>
        </msubsup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle C_{n}^{knn}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/3f2588b76da9c7429b56cec54535a18a8cae6386" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:4.892ex; height:2.843ex;" alt="C_{n}^{{knn}}" /></span> denote the <span class="texhtml mvar" style="font-style:italic;">k</span> nearest neighbour classifier based on a training set of size <span class="texhtml mvar" style="font-style:italic;">n</span>. Under certain regularity conditions, the <a href="https://en.wikipedia.org/wiki/Bayes_classifier" title="Bayes classifier">excess risk</a> yields the following asymptotic expansion<sup id="cite_ref-Samworth12_9-1" class="reference"><a href="#cite_note-Samworth12-9">[9]</a></sup></p>
<dl>
<dd>
<dl>
<dd><span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msubsup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
            <mi>n</mi>
            <mi>n</mi>
          </mrow>
        </msubsup>
        <mo stretchy="false">)</mo>
        <mo>&#x2212;<!-- − --></mo>
        <msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi class="MJX-tex-caligraphic" mathvariant="script">R</mi>
            </mrow>
          </mrow>
        </msub>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>C</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>B</mi>
            <mi>a</mi>
            <mi>y</mi>
            <mi>e</mi>
            <mi>s</mi>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow>
          <mo>{</mo>
          <msub>
            <mi>B</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>1</mn>
            </mrow>
          </msub>
          <mrow class="MJX-TeXAtom-ORD">
            <mfrac>
              <mn>1</mn>
              <mi>k</mi>
            </mfrac>
          </mrow>
          <mo>+</mo>
          <msub>
            <mi>B</mi>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>2</mn>
            </mrow>
          </msub>
          <msup>
            <mrow>
              <mo>(</mo>
              <mrow class="MJX-TeXAtom-ORD">
                <mfrac>
                  <mi>k</mi>
                  <mi>n</mi>
                </mfrac>
              </mrow>
              <mo>)</mo>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mn>4</mn>
              <mrow class="MJX-TeXAtom-ORD">
                <mo>/</mo>
              </mrow>
              <mi>d</mi>
            </mrow>
          </msup>
          <mo>}</mo>
        </mrow>
        <mo fence="false" stretchy="false">{</mo>
        <mn>1</mn>
        <mo>+</mo>
        <mi>o</mi>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mo fence="false" stretchy="false">}</mo>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {R}}_{\mathcal {R}}(C_{n}^{knn})-{\mathcal {R}}_{\mathcal {R}}(C^{Bayes})=\left\{B_{1}{\frac {1}{k}}+B_{2}\left({\frac {k}{n}}\right)^{4/d}\right\}\{1+o(1)\},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0d37bdd6d06251f082b0fe52c3f7da579b11cc85" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.171ex; width:62.634ex; height:7.509ex;" alt="{\mathcal  {R}}_{{\mathcal  {R}}}(C_{{n}}^{{knn}})-{\mathcal  {R}}_{{{\mathcal  {R}}}}(C^{{Bayes}})=\left\{B_{1}{\frac  1k}+B_{2}\left({\frac  kn}\right)^{{4/d}}\right\}\{1+o(1)\}," /></span></dd>
</dl>
</dd>
</dl>
<p>for some constants <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>B</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle B_{1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1fa091eb428443c9c5c5fcf32a69d3665c89e00c" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.836ex; height:2.509ex;" alt="B_{1}" /></span> and <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>B</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle B_{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/199944d59dcc18842dfd1deab6000a1d1dadcbae" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.836ex; height:2.509ex;" alt="B_{2}" /></span>.</p>
<p>The choice <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
        <mo>=</mo>
        <mo fence="false" stretchy="false">&#x230A;<!-- ⌊ --></mo>
        <mi>B</mi>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mfrac>
              <mn>4</mn>
              <mrow>
                <mi>d</mi>
                <mo>+</mo>
                <mn>4</mn>
              </mrow>
            </mfrac>
          </mrow>
        </msup>
        <mo fence="false" stretchy="false">&#x230B;<!-- ⌋ --></mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k^{*}=\lfloor Bn^{\frac {4}{d+4}}\rfloor }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5cbecc881f1b8637b3d4d4527fd1671f5be252fa" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:14.147ex; height:4.176ex;" alt="k^{*}=\lfloor Bn^{{{\frac  4{d+4}}}}\rfloor " /></span> offers a trade off between the two terms in the above display, for which the <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2217;<!-- ∗ --></mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k^{*}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/cc02328d1105a031ca024abcb86629ea4edb3cc8" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.283ex; height:2.343ex;" alt="k^{*}" /></span>-nearest neighbour error converges to the Bayes error at the optimal (<a href="https://en.wikipedia.org/wiki/Minimax" title="Minimax">minimax</a>) rate <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mi class="MJX-tex-caligraphic" mathvariant="script">O</mi>
          </mrow>
        </mrow>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2212;<!-- − --></mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mfrac>
                <mn>4</mn>
                <mrow>
                  <mi>d</mi>
                  <mo>+</mo>
                  <mn>4</mn>
                </mrow>
              </mfrac>
            </mrow>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\mathcal {O}}(n^{-{\frac {4}{d+4}}})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/07280735a8852d609ffd3942647d7e3255697f05" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:9.871ex; height:4.176ex;" alt="{\mathcal  {O}}(n^{{-{\frac  4{d+4}}}})" /></span>.</p>
<h2><span class="mw-headline" id="Metric_learning">Metric learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=8" title="Edit section: Metric learning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The K-nearest neighbor classification performance can often be significantly improved through (supervised) metric learning. Popular algorithms are <a href="https://en.wikipedia.org/wiki/Neighbourhood_components_analysis" title="Neighbourhood components analysis">neighbourhood components analysis</a> and <a href="https://en.wikipedia.org/wiki/Large_margin_nearest_neighbor" title="Large margin nearest neighbor">large margin nearest neighbor</a>. Supervised metric learning algorithms use the label information to learn a new <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)" title="Metric (mathematics)">metric</a> or <a href="https://en.wikipedia.org/wiki/Pseudometric_space" title="Pseudometric space">pseudo-metric</a>.</p>
<h2><span class="mw-headline" id="Feature_extraction">Feature extraction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=9" title="Edit section: Feature extraction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters) then the input data will be transformed into a reduced representation set of features (also named features vector). Transforming the input data into the set of features is called <a href="https://en.wikipedia.org/wiki/Feature_extraction" title="Feature extraction">feature extraction</a>. If the features extracted are carefully chosen it is expected that the features set will extract the relevant information from the input data in order to perform the desired task using this reduced representation instead of the full size input. Feature extraction is performed on raw data prior to applying <i>k</i>-NN algorithm on the transformed data in <a href="https://en.wikipedia.org/wiki/Feature_space" class="mw-redirect" title="Feature space">feature space</a>.</p>
<p>An example of a typical <a href="https://en.wikipedia.org/wiki/Computer_vision" title="Computer vision">computer vision</a> computation pipeline for <a href="https://en.wikipedia.org/wiki/Facial_recognition_system" title="Facial recognition system">face recognition</a> using <i>k</i>-NN including feature extraction and dimension reduction pre-processing steps (usually implemented with <a href="https://en.wikipedia.org/wiki/OpenCV" title="OpenCV">OpenCV</a>):</p>
<ol>
<li><a href="https://en.wikipedia.org/wiki/Haar_wavelet" title="Haar wavelet">Haar</a> face detection</li>
<li><a href="https://en.wikipedia.org/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a> tracking analysis</li>
<li><a href="https://en.wikipedia.org/wiki/Principal_Component_Analysis" class="mw-redirect" title="Principal Component Analysis">PCA</a> or <a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">Fisher LDA</a> projection into feature space, followed by <i>k</i>-NN classification</li>
</ol>
<h2><span class="mw-headline" id="Dimension_reduction">Dimension reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=10" title="Edit section: Dimension reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>For high-dimensional data (e.g., with number of dimensions more than 10) <a href="https://en.wikipedia.org/wiki/Dimension_reduction" class="mw-redirect" title="Dimension reduction">dimension reduction</a> is usually performed prior to applying the <i>k</i>-NN algorithm in order to avoid the effects of the <a href="https://en.wikipedia.org/wiki/Curse_of_Dimensionality" class="mw-redirect" title="Curse of Dimensionality">curse of dimensionality</a>. <sup id="cite_ref-15" class="reference"><a href="#cite_note-15">[15]</a></sup></p>
<p>The curse of dimensionality in the <i>k</i>-NN context basically means that <a href="https://en.wikipedia.org/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector (imagine multiple points lying more or less on a circle with the query point at the center; the distance from the query to all data points in the search space is almost the same).</p>
<p><a href="https://en.wikipedia.org/wiki/Feature_extraction" title="Feature extraction">Feature extraction</a> and dimension reduction can be combined in one step using <a href="https://en.wikipedia.org/wiki/Principal_Component_Analysis" class="mw-redirect" title="Principal Component Analysis">principal component analysis</a> (PCA), <a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">linear discriminant analysis</a> (LDA), or <a href="https://en.wikipedia.org/wiki/Canonical_correlation" title="Canonical correlation">canonical correlation analysis</a> (CCA) techniques as a pre-processing step, followed by clustering by <i>k</i>-NN on <a href="https://en.wikipedia.org/wiki/Feature_(machine_learning)" title="Feature (machine learning)">feature vectors</a> in reduced-dimension space. In <a href="Machine_learning" title="Machine learning">machine learning</a> this process is also called low-dimensional <a href="https://en.wikipedia.org/wiki/Embedding" title="Embedding">embedding</a>.<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">[16]</a></sup></p>
<p>For very-high-dimensional datasets (e.g. when performing a similarity search on live video streams, DNA data or high-dimensional <a href="https://en.wikipedia.org/wiki/Time_series" title="Time series">time series</a>) running a fast <b>approximate</b> <i>k</i>-NN search using <a href="https://en.wikipedia.org/wiki/Locality_Sensitive_Hashing" class="mw-redirect" title="Locality Sensitive Hashing">locality sensitive hashing</a>, "random projections",<sup id="cite_ref-17" class="reference"><a href="#cite_note-17">[17]</a></sup> "sketches" <sup id="cite_ref-18" class="reference"><a href="#cite_note-18">[18]</a></sup> or other high-dimensional similarity search techniques from the <a href="https://en.wikipedia.org/wiki/VLDB" title="VLDB">VLDB</a> toolbox might be the only feasible option.</p>
<h2><span class="mw-headline" id="Decision_boundary">Decision boundary</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=11" title="Edit section: Decision boundary">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Nearest neighbor rules in effect implicitly compute the <a href="https://en.wikipedia.org/wiki/Decision_boundary" title="Decision boundary">decision boundary</a>. It is also possible to compute the decision boundary explicitly, and to do so efficiently, so that the computational complexity is a function of the boundary complexity.<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">[19]</a></sup></p>
<h2><span class="mw-headline" id="Extension_of_k-NN_.28ENN.29_for_classification">Extension of <i>k-NN</i> (ENN) for classification</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=12" title="Edit section: Extension of k-NN (ENN) for classification">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Unlike the classic <i>k-NN</i> methods in which only the nearest neighbors of an object are used to estimate its group membership, an extended <i>k-NN</i> method, termed ENN,<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">[20]</a></sup> makes use of a <i>two-way communication</i> for classification: it considers not only who are the nearest neighbors of the test sample, but also who consider the test sample as their nearest neighbors. The idea of ENN method is to assign a group membership to an object by maximizing the <i>intra-class coherence</i>, which is a statistic measuring the coherence among all classes. Empirical studies have shown that ENN can significantly improve the classification accuracy in comparison with the <i>k-NN</i> method.</p>
<h2><span class="mw-headline" id="Data_reduction">Data reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=13" title="Edit section: Data reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Data reduction is one of the most important problems for work with huge data sets. Usually, only some of the data points are needed for accurate classification. Those data are called the <i>prototypes</i> and can be found as follows:</p>
<ol>
<li>Select the <i>class-outliers</i>, that is, training data that are classified incorrectly by <i>k</i>-NN (for a given <i>k</i>)</li>
<li>Separate the rest of the data into two sets: (i) the prototypes that are used for the classification decisions and (ii) the <i>absorbed points</i> that can be correctly classified by <i>k</i>-NN using prototypes. The absorbed points can then be removed from the training set.</li>
</ol>
<h3><span class="mw-headline" id="Selection_of_class-outliers">Selection of class-outliers</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=14" title="Edit section: Selection of class-outliers">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A training example surrounded by examples of other classes is called a class outlier. Causes of class outliers include:</p>
<ul>
<li>random error</li>
<li>insufficient training examples of this class (an isolated example appears instead of a cluster)</li>
<li>missing important features (the classes are separated in other dimensions which we do not know)</li>
<li>too many training examples of other classes (unbalanced classes) that create a "hostile" background for the given small class</li>
</ul>
<p>Class outliers with <i>k</i>-NN produce noise. They can be detected and separated for future analysis. Given two natural numbers, <i>k&gt;r&gt;0</i>, a training example is called a <i>(k,r)</i>NN class-outlier if its <i>k</i> nearest neighbors include more than <i>r</i> examples of other classes.</p>
<h3><span class="mw-headline" id="CNN_for_data_reduction">CNN for data reduction</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=15" title="Edit section: CNN for data reduction">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Condensed nearest neighbor (CNN, the <i><a href="https://en.wikipedia.org/wiki/Peter_E._Hart" title="Peter E. Hart">Hart</a> algorithm</i>) is an algorithm designed to reduce the data set for <i>k</i>-NN classification.<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">[21]</a></sup> It selects the set of prototypes <i>U</i> from the training data, such that 1NN with <i>U</i> can classify the examples almost as accurately as 1NN does with the whole data set.</p>
<div class="thumb tright">
<div class="thumbinner" style="width:132px;"><a href="https://en.wikipedia.org/wiki/File:BorderRAtio.PNG" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/e/e6/BorderRAtio.PNG/130px-BorderRAtio.PNG" width="130" height="104" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/e/e6/BorderRAtio.PNG 1.5x" data-file-width="159" data-file-height="127" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="https://en.wikipedia.org/wiki/File:BorderRAtio.PNG" class="internal" title="Enlarge"></a></div>
Calculation of the border ratio.</div>
</div>
</div>
<div class="thumb tright">
<div class="thumbinner" style="width:132px;"><a href="https://en.wikipedia.org/wiki/File:PointsTypes.png" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/e/e7/PointsTypes.png" width="130" height="59" class="thumbimage" data-file-width="130" data-file-height="59" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="https://en.wikipedia.org/wiki/File:PointsTypes.png" class="internal" title="Enlarge"></a></div>
Three types of points: prototypes, class-outliers, and absorbed points.</div>
</div>
</div>
<p>Given a training set <i>X</i>, CNN works iteratively:</p>
<ol>
<li>Scan all elements of <i>X</i>, looking for an element <i>x</i> whose nearest prototype from <i>U</i> has a different label than <i>x</i>.</li>
<li>Remove <i>x</i> from <i>X</i> and add it to <i>U</i></li>
<li>Repeat the scan until no more prototypes are added to <i>U</i>.</li>
</ol>
<p>Use <i>U</i> instead of <i>X</i> for classification. The examples that are not prototypes are called "absorbed" points.</p>
<p>It is efficient to scan the training examples in order of decreasing border ratio.<sup id="cite_ref-MirkesKnn_22-0" class="reference"><a href="#cite_note-MirkesKnn-22">[22]</a></sup> The border ratio of a training example <i>x</i> is defined as</p>
<dl>
<dd><span class="texhtml"><i>a</i>(<i>x</i>) = <span class="sfrac nowrap" style="display:inline-block; vertical-align:-0.5em; font-size:85%; text-align:center;"><span style="display:block; line-height:1em; margin:0 0.1em;">||<i>x'-y</i>||</span><span class="visualhide">/</span><span style="display:block; line-height:1em; margin:0 0.1em; border-top:1px solid;">||<i>x-y</i>||</span></span></span></dd>
</dl>
<p>where <span class="texhtml">||<i>x-y</i>||</span> is the distance to the closest example <i>y</i> having a different color than <i>x</i>, and <span class="texhtml">||<i>x'-y</i>||</span> is the distance from <i>y</i> to its closest example <i>x'</i> with the same label as <i>x</i>.</p>
<p>The border ratio is in the interval [0,1] because <span class="texhtml">||<i>x'-y</i>||</span>never exceeds <span class="texhtml">||<i>x-y</i>||</span>. This ordering gives preference to the borders of the classes for inclusion in the set of prototypes<i>U</i>. A point of a different label than <i>x</i> is called external to <i>x</i>. The calculation of the border ratio is illustrated by the figure on the right. The data points are labeled by colors: the initial point is <i>x</i> and its label is red. External points are blue and green. The closest to <i>x</i> external point is <i>y</i>. The closest to <i>y</i> red point is <i>x'</i> . The border ratio <span class="texhtml"><i>a</i>(<i>x</i>) = ||<i>x'-y</i>|| / ||<i>x-y</i>||</span>is the attribute of the initial point <i>x</i>.</p>
<p>Below is an illustration of CNN in a series of figures. There are three classes (red, green and blue). Fig. 1: initially there are 60 points in each class. Fig. 2 shows the 1NN classification map: each pixel is classified by 1NN using all the data. Fig. 3 shows the 5NN classification map. White areas correspond to the unclassified regions, where 5NN voting is tied (for example, if there are two green, two red and one blue points among 5 nearest neighbors). Fig. 4 shows the reduced data set. The crosses are the class-outliers selected by the (3,2)NN rule (all the three nearest neighbors of these instances belong to other classes); the squares are the prototypes, and the empty circles are the absorbed points. The left bottom corner shows the numbers of the class-outliers, prototypes and absorbed points for all three classes. The number of prototypes varies from 15% to 20% for different classes in this example. Fig. 5 shows that the 1NN classification map with the prototypes is very similar to that with the initial data set. The figures were produced using the Mirkes applet.<sup id="cite_ref-MirkesKnn_22-1" class="reference"><a href="#cite_note-MirkesKnn-22">[22]</a></sup></p>
<ul class="gallery mw-gallery-traditional">
<li class='gallerycaption'>CNN model reduction for k-NN classifiers</li>
<li class="gallerybox" style="width: 235px">
<div style="width: 235px">
<div class="thumb" style="width: 230px;">
<div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:Data3classes.png" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/182px-Data3classes.png" width="182" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/273px-Data3classes.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Data3classes.png/364px-Data3classes.png 2x" data-file-width="602" data-file-height="397" /></a></div>
</div>
<div class="gallerytext">
<p>Fig. 1. The dataset.</p>
</div>
</div>
</li>
<li class="gallerybox" style="width: 235px">
<div style="width: 235px">
<div class="thumb" style="width: 230px;">
<div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:Map1NN.png" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/183px-Map1NN.png" width="183" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/274px-Map1NN.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/52/Map1NN.png/365px-Map1NN.png 2x" data-file-width="603" data-file-height="397" /></a></div>
</div>
<div class="gallerytext">
<p>Fig. 2. The 1NN classification map.</p>
</div>
</div>
</li>
<li class="gallerybox" style="width: 235px">
<div style="width: 235px">
<div class="thumb" style="width: 230px;">
<div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:Map5NN.png" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/183px-Map5NN.png" width="183" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/274px-Map5NN.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Map5NN.png/365px-Map5NN.png 2x" data-file-width="602" data-file-height="396" /></a></div>
</div>
<div class="gallerytext">
<p>Fig. 3. The 5NN classification map.</p>
</div>
</div>
</li>
<li class="gallerybox" style="width: 235px">
<div style="width: 235px">
<div class="thumb" style="width: 230px;">
<div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:ReducedDataSet.png" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/182px-ReducedDataSet.png" width="182" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/272px-ReducedDataSet.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b3/ReducedDataSet.png/363px-ReducedDataSet.png 2x" data-file-width="608" data-file-height="402" /></a></div>
</div>
<div class="gallerytext">
<p>Fig. 4. The CNN reduced dataset.</p>
</div>
</div>
</li>
<li class="gallerybox" style="width: 235px">
<div style="width: 235px">
<div class="thumb" style="width: 230px;">
<div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:Map1NNReducedDataSet.png" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/183px-Map1NNReducedDataSet.png" width="183" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/275px-Map1NNReducedDataSet.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Map1NNReducedDataSet.png/366px-Map1NNReducedDataSet.png 2x" data-file-width="611" data-file-height="401" /></a></div>
</div>
<div class="gallerytext">
<p>Fig. 5. The 1NN classification map based on the CNN extracted prototypes.</p>
</div>
</div>
</li>
</ul>
<h2><span class="mw-headline" id="k-NN_regression"><i>k</i>-NN regression</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=16" title="Edit section: k-NN regression">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>In <i>k</i>-NN regression, the <i>k</i>-NN algorithm is used for estimating continuous variables. One such algorithm uses a weighted average of the <i>k</i> nearest neighbors, weighted by the inverse of their distance. This algorithm works as follows:</p>
<ol>
<li>Compute the Euclidean or <a href="https://en.wikipedia.org/wiki/Mahalanobis_distance" title="Mahalanobis distance">Mahalanobis distance</a> from the query example to the labeled examples.</li>
<li>Order the labeled examples by increasing distance.</li>
<li>Find a heuristically optimal number <i>k</i> of nearest neighbors, based on <a href="https://en.wikipedia.org/wiki/RMSE" class="mw-redirect" title="RMSE">RMSE</a>. This is done using cross validation.</li>
<li>Calculate an inverse distance weighted average with the <i>k</i>-nearest multivariate neighbors.</li>
</ol>
<h2><span class="mw-headline" id="k-NN_outlier"><i>k</i>-NN outlier</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=17" title="Edit section: k-NN outlier">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The distance to the <i>k</i>th nearest neighbor can also be seen as a local density estimate and thus is also a popular outlier score in <a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">anomaly detection</a>. The larger the distance to the <i>k</i>-NN, the lower the local density, the more likely the query point is an outlier.<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">[23]</a></sup> Although quite simple, this outlier model, along with another classic data mining method, <a href="https://en.wikipedia.org/wiki/Local_outlier_factor" title="Local outlier factor">local outlier factor</a>, works quite well also in comparison to more recent and more complex approaches, according to a large scale experimental analysis.<sup id="cite_ref-CamposZimek2016_24-0" class="reference"><a href="#cite_note-CamposZimek2016-24">[24]</a></sup></p>
<h2><span class="mw-headline" id="Validation_of_results">Validation of results</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=18" title="Edit section: Validation of results">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A <a href="https://en.wikipedia.org/wiki/Confusion_matrix" title="Confusion matrix">confusion matrix</a> or "matching matrix" is often used as a tool to validate the accuracy of <i>k</i>-NN classification. More robust statistical methods such as <a href="https://en.wikipedia.org/wiki/Likelihood-ratio_test" title="Likelihood-ratio test">likelihood-ratio test</a> can also be applied.</p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=19" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="navigation" aria-label="Portals" class="noprint portal plainlist tright" style="margin:0.5em 0 0.5em 1em;border:solid #aaa 1px">
<ul style="display:table;box-sizing:border-box;padding:0.1em;max-width:175px;background:#f9f9f9;font-size:85%;line-height:110%;font-style:italic;font-weight:bold">
<li style="display:table-row"><span style="display:table-cell;padding:0.2em;vertical-align:middle;text-align:center"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png" width="32" height="28" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/48px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/64px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28" /></span><span style="display:table-cell;padding:0.2em 0.2em 0.2em 0.3em;vertical-align:middle"><a href="https://en.wikipedia.org/wiki/Portal:Machine_learning" title="Portal:Machine learning">Machine learning portal</a></span></li>
<li style="display:table-row"><span style="display:table-cell;padding:0.2em;vertical-align:middle;text-align:center"><a href="https://en.wikipedia.org/wiki/File:Internet_map_1024.jpg" class="image"><img alt="icon" src="http://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/28px-Internet_map_1024.jpg" width="28" height="28" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/42px-Internet_map_1024.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d2/Internet_map_1024.jpg/56px-Internet_map_1024.jpg 2x" data-file-width="1280" data-file-height="1280" /></a></span><span style="display:table-cell;padding:0.2em 0.2em 0.2em 0.3em;vertical-align:middle"><a href="https://en.wikipedia.org/wiki/Portal:Computer_science" title="Portal:Computer science">Computer science portal</a></span></li>
<li style="display:table-row"><span style="display:table-cell;padding:0.2em;vertical-align:middle;text-align:center"><a href="https://en.wikipedia.org/wiki/File:Fisher_iris_versicolor_sepalwidth.svg" class="image"><img alt="icon" src="http://upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/32px-Fisher_iris_versicolor_sepalwidth.svg.png" width="32" height="22" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/48px-Fisher_iris_versicolor_sepalwidth.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/40/Fisher_iris_versicolor_sepalwidth.svg/64px-Fisher_iris_versicolor_sepalwidth.svg.png 2x" data-file-width="822" data-file-height="567" /></a></span><span style="display:table-cell;padding:0.2em 0.2em 0.2em 0.3em;vertical-align:middle"><a href="https://en.wikipedia.org/wiki/Portal:Statistics" title="Portal:Statistics">Statistics portal</a></span></li>
</ul>
</div>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Nearest_centroid_classifier" title="Nearest centroid classifier">Nearest centroid classifier</a></li>
<li><a href="https://en.wikipedia.org/wiki/Closest_pair_of_points_problem" title="Closest pair of points problem">Closest pair of points problem</a></li>
</ul>
<div style="clear:right;"></div>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=20" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-1">^</a></b></span> <span class="reference-text"><cite class="citation journal">Altman, N. S. (1992). "An introduction to kernel and nearest-neighbor nonparametric regression". <i>The American Statistician</i>. <b>46</b> (3): 175–185. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1080%2F00031305.1992.10475879">10.1080/00031305.1992.10475879</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=An+introduction+to+kernel+and+nearest-neighbor+nonparametric+regression&amp;rft.aufirst=N.+S.&amp;rft.aulast=Altman&amp;rft.date=1992&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1080%2F00031305.1992.10475879&amp;rft.issue=3&amp;rft.jtitle=The+American+Statistician&amp;rft.pages=175-185&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=46" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text">This scheme is a generalization of linear interpolation.</span></li>
<li id="cite_note-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-3">^</a></b></span> <span class="reference-text"><cite class="citation web">Jaskowiak, P. A.; Campello, R. J. G. B. <a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.208.993">"Comparing Correlation Coefficients as Dissimilarity Measures for Cancer Classification in Gene Expression Data"</a>. <i><a rel="nofollow" class="external free" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.208.993">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.208.993</a></i>. Brazilian Symposium on Bioinformatics (BSB 2011). pp.&#160;1–8<span class="reference-accessdate">. Retrieved <span class="nowrap">16 October</span> 2014</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Comparing+Correlation+Coefficients+as+Dissimilarity+Measures+for+Cancer+Classification+in+Gene+Expression+Data&amp;rft.au=Campello%2C+R.+J.+G.+B.&amp;rft.aufirst=P.+A.&amp;rft.aulast=Jaskowiak&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.208.993&amp;rft.jtitle=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.208.993&amp;rft.pages=1-8&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span> <span style="font-size:100%" class="error citation-comment">External link in <code style="color:inherit; border:inherit; padding:inherit;">|website=</code> (<a href="https://en.wikipedia.org/wiki/Help:CS1_errors#param_has_ext_link" title="Help:CS1 errors">help</a>)</span></span></li>
<li id="cite_note-Coomans_Massart1982-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-Coomans_Massart1982_4-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">D. Coomans; D.L. Massart (1982). "Alternative k-nearest neighbour rules in supervised pattern recognition&#160;: Part 1. k-Nearest neighbour classification by using alternative voting rules". <i><a href="https://en.wikipedia.org/wiki/Analytica_Chimica_Acta" title="Analytica Chimica Acta">Analytica Chimica Acta</a></i>. <b>136</b>: 15–27. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1016%2FS0003-2670(01)95359-0">10.1016/S0003-2670(01)95359-0</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Alternative+k-nearest+neighbour+rules+in+supervised+pattern+recognition+%3A+Part+1.+k-Nearest+neighbour+classification+by+using+alternative+voting+rules&amp;rft.au=D.+Coomans&amp;rft.au=D.L.+Massart&amp;rft.date=1982&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1016%2FS0003-2670%2801%2995359-0&amp;rft.jtitle=Analytica+Chimica+Acta&amp;rft.pages=15-27&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=136" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text">Everitt, B. S., Landau, S., Leese, M. and Stahl, D. (2011) Miscellaneous Clustering Methods, in Cluster Analysis, 5th Edition, John Wiley &amp; Sons, Ltd, Chichester, UK.</span></li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation journal">Nigsch F, Bender A, van Buuren B, Tissen J, Nigsch E, Mitchell JB (2006). "Melting point prediction employing k-nearest neighbor algorithms and genetic parameter optimization". <i>Journal of Chemical Information and Modeling</i>. <b>46</b> (6): 2412–2422. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1021%2Fci060149f">10.1021/ci060149f</a>. <a href="https://en.wikipedia.org/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="http://www.ncbi.nlm.nih.gov/pubmed/17125183">17125183</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Melting+point+prediction+employing+k-nearest+neighbor+algorithms+and+genetic+parameter+optimization&amp;rft.au=Bender%2C+A&amp;rft.aufirst=F&amp;rft.aulast=Nigsch&amp;rft.au=Mitchell%2C+JB&amp;rft.au=Nigsch%2C+E&amp;rft.au=Tissen%2C+J&amp;rft.au=van+Buuren%2C+B&amp;rft.date=2006&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1021%2Fci060149f&amp;rft_id=info%3Apmid%2F17125183&amp;rft.issue=6&amp;rft.jtitle=Journal+of+Chemical+Information+and+Modeling&amp;rft.pages=2412-2422&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=46" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-HPS2008-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-HPS2008_7-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hall P, Park BU, Samworth RJ (2008). "Choice of neighbor order in nearest-neighbor classification". <i><a href="https://en.wikipedia.org/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>36</b> (5): 2135–2152. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1214%2F07-AOS537">10.1214/07-AOS537</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Choice+of+neighbor+order+in+nearest-neighbor+classification&amp;rft.aufirst=P&amp;rft.aulast=Hall&amp;rft.au=Park%2C+BU&amp;rft.au=Samworth%2C+RJ&amp;rft.date=2008&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1214%2F07-AOS537&amp;rft.issue=5&amp;rft.jtitle=Annals+of+Statistics&amp;rft.pages=2135-2152&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=36" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Stone-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-Stone_8-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Stone C. J. (1977). "Consistent nonparametric regression". <i><a href="https://en.wikipedia.org/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>5</b> (4): 595–620. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1214%2Faos%2F1176343886">10.1214/aos/1176343886</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Consistent+nonparametric+regression&amp;rft.au=Stone+C.+J.&amp;rft.date=1977&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1214%2Faos%2F1176343886&amp;rft.issue=4&amp;rft.jtitle=Annals+of+Statistics&amp;rft.pages=595-620&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=5" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Samworth12-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-Samworth12_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Samworth12_9-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Samworth R. J. (2012). "Optimal weighted nearest neighbour classifiers". <i><a href="https://en.wikipedia.org/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>40</b> (5): 2733–2763. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1214%2F12-AOS1049">10.1214/12-AOS1049</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Optimal+weighted+nearest+neighbour+classifiers&amp;rft.au=Samworth+R.+J.&amp;rft.date=2012&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1214%2F12-AOS1049&amp;rft.issue=5&amp;rft.jtitle=Annals+of+Statistics&amp;rft.pages=2733-2763&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=40" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Terrell_Scott1992-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-Terrell_Scott1992_10-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">D. G. Terrell; D. W. Scott (1992). "Variable kernel density estimation". <i><a href="https://en.wikipedia.org/wiki/Annals_of_Statistics" title="Annals of Statistics">Annals of Statistics</a></i>. <b>20</b> (3): 1236–1265. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1214%2Faos%2F1176348768">10.1214/aos/1176348768</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Variable+kernel+density+estimation&amp;rft.au=D.+G.+Terrell&amp;rft.au=D.+W.+Scott&amp;rft.date=1992&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1214%2Faos%2F1176348768&amp;rft.issue=3&amp;rft.jtitle=Annals+of+Statistics&amp;rft.pages=1236-1265&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=20" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Mills2010-11"><span class="mw-cite-backlink"><b><a href="#cite_ref-Mills2010_11-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Mills, Peter. "Efficient statistical classification of satellite measurements". <i>International Journal of Remote Sensing</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Efficient+statistical+classification+of+satellite+measurements&amp;rft.aufirst=Peter&amp;rft.aulast=Mills&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Remote+Sensing&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="https://en.wikipedia.org/wiki/Thomas_M._Cover" title="Thomas M. Cover">Cover TM</a>, <a href="https://en.wikipedia.org/wiki/Peter_E._Hart" title="Peter E. Hart">Hart PE</a> (1967). "Nearest neighbor pattern classification". <i>IEEE Transactions on Information Theory</i>. <b>13</b> (1): 21–27. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109%2FTIT.1967.1053964">10.1109/TIT.1967.1053964</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Nearest+neighbor+pattern+classification&amp;rft.au=Cover+TM%2C+Hart+PE&amp;rft.date=1967&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1967.1053964&amp;rft.issue=1&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.pages=21-27&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=13" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal">Toussaint GT (April 2005). "Geometric proximity graphs for improving nearest neighbor methods in instance-based learning and data mining". <i>International Journal of Computational Geometry and Applications</i>. <b>15</b> (2): 101–150. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1142%2FS0218195905001622">10.1142/S0218195905001622</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Geometric+proximity+graphs+for+improving+nearest+neighbor+methods+in+instance-based+learning+and+data+mining&amp;rft.au=Toussaint+GT&amp;rft.date=2005-04&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1142%2FS0218195905001622&amp;rft.issue=2&amp;rft.jtitle=International+Journal+of+Computational+Geometry+and+Applications&amp;rft.pages=101-150&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=15" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-PTPR-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-PTPR_14-0">^</a></b></span> <span class="reference-text"><cite class="citation book">Devroye, L., Gyorfi, L. &amp; Lugosi, G. (1996). <i>A probabilistic theory of pattern recognition</i>. Springer. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-3879-4618-7" title="Special:BookSources/0-3879-4618-7">0-3879-4618-7</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.au=Devroye%2C+L.%2C+Gyorfi%2C+L.+%26+Lugosi%2C+G.&amp;rft.btitle=A+probabilistic+theory+of+pattern+recognition&amp;rft.date=1996&amp;rft.genre=book&amp;rft.isbn=0-3879-4618-7&amp;rft.pub=Springer&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span> <span class="citation-comment" style="display:none; color:#33aa33">CS1 maint: Multiple names: authors list (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Multiple_names:_authors_list" title="Category:CS1 maint: Multiple names: authors list">link</a>)</span></span></li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text">Beyer, Kevin, et al.. 'When is “nearest neighbor” meaningful?. Database Theory—ICDT’99, 217-235|year 1999</span></li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text">Shaw, Blake, and Tony Jebara. 'Structure preserving embedding. Proceedings of the 26th Annual International Conference on Machine Learning. ACM,2009</span></li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text">Bingham, Ella, and Heikki Mannila. Random projection in dimensionality reduction: applications to image and text data. Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining. ACM | year 2001</span></li>
<li id="cite_note-18"><span class="mw-cite-backlink"><b><a href="#cite_ref-18">^</a></b></span> <span class="reference-text">Shasha, D High Performance Discovery in Time Series.Berlin: Springer, 2004, <a href="https://en.wikipedia.org/wiki/Special:BookSources/0387008578" class="internal mw-magiclink-isbn">ISBN 0-387-00857-8</a></span></li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bremner D, Demaine E, Erickson J, Iacono J, Langerman S, Morin P, Toussaint G (2005). "Output-sensitive algorithms for computing nearest-neighbor decision boundaries". <i>Discrete and Computational Geometry</i>. <b>33</b> (4): 593–604. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2Fs00454-004-1152-0">10.1007/s00454-004-1152-0</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Output-sensitive+algorithms+for+computing+nearest-neighbor+decision+boundaries&amp;rft.au=Demaine%2C+E&amp;rft.au=Erickson%2C+J&amp;rft.aufirst=D&amp;rft.au=Iacono%2C+J&amp;rft.au=Langerman%2C+S&amp;rft.aulast=Bremner&amp;rft.au=Morin%2C+P&amp;rft.au=Toussaint%2C+G&amp;rft.date=2005&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2Fs00454-004-1152-0&amp;rft.issue=4&amp;rft.jtitle=Discrete+and+Computational+Geometry&amp;rft.pages=593-604&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=33" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation journal">Tang B, He H (2015). "ENN: Extended Nearest Neighbor Method for Pattern Recognition [Research Frontier]". <i>IEEE Computational Intelligence Magazine</i>. <b>10</b> (3): 52–60. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109%2FMCI.2015.2437512">10.1109/MCI.2015.2437512</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=ENN%3A+Extended+Nearest+Neighbor+Method+for+Pattern+Recognition+%5BResearch+Frontier%5D&amp;rft.aufirst=B&amp;rft.au=He%2C+H&amp;rft.aulast=Tang&amp;rft.date=2015&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1109%2FMCI.2015.2437512&amp;rft.issue=3&amp;rft.jtitle=IEEE+Computational+Intelligence+Magazine&amp;rft.pages=52-60&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=10" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><a href="https://en.wikipedia.org/wiki/Peter_E._Hart" title="Peter E. Hart">P. E. Hart</a>, The Condensed Nearest Neighbor Rule. IEEE Transactions on Information Theory 18 (1968) 515–516. doi: 10.1109/TIT.1968.1054155</span></li>
<li id="cite_note-MirkesKnn-22"><span class="mw-cite-backlink">^ <a href="#cite_ref-MirkesKnn_22-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-MirkesKnn_22-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text">E. M. Mirkes, <a rel="nofollow" class="external text" href="http://www.math.le.ac.uk/people/ag153/homepage/KNN/KNN3.html">KNN and Potential Energy: applet.</a> University of Leicester, 2011.</span></li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation conference">Ramaswamy, S.; Rastogi, R.; Shim, K. (2000). <i>Efficient algorithms for mining outliers from large data sets</i>. Proceedings of the 2000 ACM SIGMOD international conference on Management of data – SIGMOD '00. p.&#160;427. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1145%2F342009.335437">10.1145/342009.335437</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="https://en.wikipedia.org/wiki/Special:BookSources/1-58113-217-4" title="Special:BookSources/1-58113-217-4">1-58113-217-4</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.aufirst=S.&amp;rft.aulast=Ramaswamy&amp;rft.au=Rastogi%2C+R.&amp;rft.au=Shim%2C+K.&amp;rft.btitle=Efficient+algorithms+for+mining+outliers+from+large+data+sets&amp;rft.date=2000&amp;rft.genre=conference&amp;rft_id=info%3Adoi%2F10.1145%2F342009.335437&amp;rft.isbn=1-58113-217-4&amp;rft.pages=427&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-CamposZimek2016-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-CamposZimek2016_24-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Campos, Guilherme O.; Zimek, Arthur; Sander, Jörg; Campello, Ricardo J. G. B.; Micenková, Barbora; Schubert, Erich; Assent, Ira; Houle, Michael E. (2016). "On the evaluation of unsupervised outlier detection: measures, datasets, and an empirical study". <i>Data Mining and Knowledge Discovery</i>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2Fs10618-015-0444-8">10.1007/s10618-015-0444-8</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="http://www.worldcat.org/issn/1384-5810">1384-5810</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=On+the+evaluation+of+unsupervised+outlier+detection%3A+measures%2C+datasets%2C+and+an+empirical+study&amp;rft.au=Assent%2C+Ira&amp;rft.au=Campello%2C+Ricardo+J.+G.+B.&amp;rft.aufirst=Guilherme+O.&amp;rft.au=Houle%2C+Michael+E.&amp;rft.aulast=Campos&amp;rft.au=Micenkov%C3%A1%2C+Barbora&amp;rft.au=Sander%2C+J%C3%B6rg&amp;rft.au=Schubert%2C+Erich&amp;rft.au=Zimek%2C+Arthur&amp;rft.date=2016&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2Fs10618-015-0444-8&amp;rft.issn=1384-5810&amp;rft.jtitle=Data+Mining+and+Knowledge+Discovery&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
</ol>
</div>
<h2><span class="mw-headline" id="Further_reading">Further reading</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit&amp;section=21" title="Edit section: Further reading">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.31.1422">When Is "Nearest Neighbor" Meaningful?</a></li>
<li><cite class="citation book"><a href="https://en.wikipedia.org/wiki/Belur_V._Dasarathy" title="Belur V. Dasarathy">Belur V. Dasarathy</a>, ed. (1991). <i>Nearest Neighbor (NN) Norms: NN Pattern Classification Techniques</i>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-8186-8930-7" title="Special:BookSources/0-8186-8930-7">0-8186-8930-7</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.btitle=Nearest+Neighbor+%28NN%29+Norms%3A+NN+Pattern+Classification+Techniques&amp;rft.date=1991&amp;rft.genre=book&amp;rft.isbn=0-8186-8930-7&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></li>
<li><cite class="citation book">Shakhnarovish, Darrell, and Indyk, eds. (2005). <i>Nearest-Neighbor Methods in Learning and Vision</i>. <a href="https://en.wikipedia.org/wiki/MIT_Press" title="MIT Press">MIT Press</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-262-19547-X" title="Special:BookSources/0-262-19547-X">0-262-19547-X</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.btitle=Nearest-Neighbor+Methods+in+Learning+and+Vision&amp;rft.date=2005&amp;rft.genre=book&amp;rft.isbn=0-262-19547-X&amp;rft.pub=MIT+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span> <span class="citation-comment" style="display:none; color:#33aa33">CS1 maint: Uses editors parameter (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_editors_parameter" title="Category:CS1 maint: Uses editors parameter">link</a>)</span></li>
<li><cite class="citation journal">Mäkelä H Pekkarinen A (2004-07-26). "Estimation of forest stand volumes by Landsat TM imagery and stand-level field-inventory data". <i><a href="https://en.wikipedia.org/wiki/Forest_Ecology_and_Management" title="Forest Ecology and Management">Forest Ecology and Management</a></i>. <b>196</b> (2–3): 245–255. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1016%2Fj.foreco.2004.02.049">10.1016/j.foreco.2004.02.049</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-nearest+neighbors+algorithm&amp;rft.atitle=Estimation+of+forest+stand+volumes+by+Landsat+TM+imagery+and+stand-level+field-inventory+data&amp;rft.au=M%C3%A4kel%C3%A4+H+Pekkarinen+A&amp;rft.date=2004-07-26&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1016%2Fj.foreco.2004.02.049&amp;rft.issue=2%933&amp;rft.jtitle=Forest+Ecology+and+Management&amp;rft.pages=245-255&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=196" class="Z3988"><span style="display:none;">&#160;</span></span></li>
<li>Fast k nearest neighbor search using <a href="https://en.wikipedia.org/wiki/GPU" class="mw-redirect" title="GPU">GPU</a>. In Proceedings of the CVPR Workshop on Computer Vision on GPU, Anchorage, Alaska, USA, June 2008. V. Garcia and E. Debreuve and M. Barlaud.</li>
<li><a rel="nofollow" class="external text" href="http://www.scholarpedia.org/article/K-nearest_neighbor">Scholarpedia article on <i>k</i>-NN</a></li>
<li><a rel="nofollow" class="external text" href="https://code.google.com/p/google-all-pairs-similarity-search/">google-all-pairs-similarity-search</a></li>
</ul>


<!-- 
NewPP limit report
Parsed by mw1181
Cached time: 20161231010013
Cache expiry: 2592000
Dynamic content: false
CPU time usage: 0.392 seconds
Real time usage: 0.841 seconds
Preprocessor visited node count: 2003/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 68577/2097152 bytes
Template argument size: 2716/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 3/500
Lua time usage: 0.146/10.000 seconds
Lua memory usage: 5.27 MB/50 MB
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%  286.984      1 - -total
 43.98%  126.227      1 - Template:Reflist
 31.51%   90.430     14 - Template:Cite_journal
 18.63%   53.475      3 - Template:Fix
 13.14%   37.712      1 - Template:Machine_learning_bar
 12.38%   35.533      1 - Template:Sidebar_with_collapsible_lists
 12.35%   35.433      1 - Template:Fact
  8.84%   25.355      3 - Template:Delink
  8.63%   24.759      6 - Template:Category_handler
  8.47%   24.298      2 - Template:Citation_needed
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1775388-0!*!0!!en!4!*!math=5 and timestamp 20161231010012 and revision id 751879613
 -->
<noscript><img src="http://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=751879613">https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=751879613</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="https://en.wikipedia.org/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Classification_algorithms" title="Category:Classification algorithms">Classification algorithms</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Search_algorithms" title="Category:Search algorithms">Search algorithms</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Machine_learning_algorithms" title="Category:Machine learning algorithms">Machine learning algorithms</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Statistical_classification" title="Category:Statistical classification">Statistical classification</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Nonparametric_statistics" title="Category:Nonparametric statistics">Nonparametric statistics</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="https://en.wikipedia.org/wiki/Category:CS1_errors:_external_links" title="Category:CS1 errors: external links">CS1 errors: external links</a></li><li><a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Multiple_names:_authors_list" title="Category:CS1 maint: Multiple names: authors list">CS1 maint: Multiple names: authors list</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Pages_using_ISBN_magic_links" title="Category:Pages using ISBN magic links">Pages using ISBN magic links</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_September_2016" title="Category:Articles with unsourced statements from September 2016">Articles with unsourced statements from September 2016</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_March_2013" title="Category:Articles with unsourced statements from March 2013">Articles with unsourced statements from March 2013</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_December_2008" title="Category:Articles with unsourced statements from December 2008">Articles with unsourced statements from December 2008</a></li><li><a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_editors_parameter" title="Category:CS1 maint: Uses editors parameter">CS1 maint: Uses editors parameter</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>

			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="https://en.wikipedia.org/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="https://en.wikipedia.org/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&amp;returnto=K-nearest+neighbors+algorithm" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="https://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=K-nearest+neighbors+algorithm" title="You're encouraged to log in; however, it's not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
															<li  id="ca-nstab-main" class="selected"><span><a href="K-nearest_neighbors_algorithm.html"  title="View the content page [c]" accesskey="c">Article</a></span></li>
															<li  id="ca-talk"><span><a href="https://en.wikipedia.org/wiki/Talk:K-nearest_neighbors_algorithm"  title="Discussion about the content page [t]" accesskey="t" rel="discussion">Talk</a></span></li>
													</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<h3 id="p-variants-label">
							<span>Variants</span><a href="#"></a>
						</h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
															<li id="ca-view" class="selected"><span><a href="K-nearest_neighbors_algorithm.html" >Read</a></span></li>
															<li id="ca-edit"><span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=edit"  title="Edit this page [e]" accesskey="e">Edit</a></span></li>
															<li id="ca-history" class="collapsible"><span><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=history"  title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>
													</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<h3 id="p-cactions-label"><span>More</span><a href="#"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>

						<form action="https://en.wikipedia.org/w/index.php" id="searchform">
							<div id="simpleSearch">
							<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="https://en.wikipedia.org/wiki/Main_Page"  title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id='p-navigation' aria-labelledby='p-navigation-label'>
			<h3 id='p-navigation-label'>Navigation</h3>

			<div class="body">
									<ul>
						<li id="n-mainpage-description"><a href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="https://en.wikipedia.org/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="https://en.wikipedia.org/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="https://en.wikipedia.org/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="https://en.wikipedia.org/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="http://shop.wikimedia.org/" title="Visit the Wikipedia store">Wikipedia store</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-interaction' aria-labelledby='p-interaction-label'>
			<h3 id='p-interaction-label'>Interaction</h3>

			<div class="body">
									<ul>
						<li id="n-help"><a href="https://en.wikipedia.org/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="https://en.wikipedia.org/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="https://en.wikipedia.org/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="http://en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-tb' aria-labelledby='p-tb-label'>
			<h3 id='p-tb-label'>Tools</h3>

			<div class="body">
									<ul>
						<li id="t-whatlinkshere"><a href="https://en.wikipedia.org/wiki/Special:WhatLinksHere/K-nearest_neighbors_algorithm" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="https://en.wikipedia.org/wiki/Special:RecentChangesLinked/K-nearest_neighbors_algorithm" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="https://en.wikipedia.org/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;oldid=751879613" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Q1071612" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=K-nearest_neighbors_algorithm&amp;id=751879613" title="Information on how to cite this page">Cite this page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-coll-print_export' aria-labelledby='p-coll-print_export-label'>
			<h3 id='p-coll-print_export-label'>Print/export</h3>

			<div class="body">
									<ul>
						<li id="coll-create_a_book"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=K-nearest+neighbors+algorithm">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=render_article&amp;arttitle=K-nearest+neighbors+algorithm&amp;returnto=K-nearest+neighbors+algorithm&amp;oldid=751879613&amp;writer=rdf2latex">Download as PDF</a></li><li id="t-print"><a href="https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-lang' aria-labelledby='p-lang-label'>
			<h3 id='p-lang-label'>Languages</h3>

			<div class="body">
									<ul>
						<li class="interlanguage-link interwiki-ca"><a href="https://ca.wikipedia.org/wiki/Knn" title="Knn – Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target">Català</a></li><li class="interlanguage-link interwiki-cs"><a href="https://cs.wikipedia.org/wiki/Algoritmus_k-nejbližších_sousedů" title="Algoritmus k-nejbližších sousedů – Czech" lang="cs" hreflang="cs" class="interlanguage-link-target">Čeština</a></li><li class="interlanguage-link interwiki-da"><a href="https://da.wikipedia.org/wiki/K-nærmeste_naboer" title="K-nærmeste naboer – Danish" lang="da" hreflang="da" class="interlanguage-link-target">Dansk</a></li><li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Nächste-Nachbarn-Klassifikation" title="Nächste-Nachbarn-Klassifikation – German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/K-vecinos_más_cercanos" title="K-vecinos más cercanos – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/Méthode_des_k_plus_proches_voisins" title="Méthode des k plus proches voisins – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li><li class="interlanguage-link interwiki-ko"><a href="https://ko.wikipedia.org/wiki/K-최근접_이웃_알고리즘" title="K-최근접 이웃 알고리즘 – Korean" lang="ko" hreflang="ko" class="interlanguage-link-target">한국어</a></li><li class="interlanguage-link interwiki-id"><a href="https://id.wikipedia.org/wiki/KNN" title="KNN – Indonesian" lang="id" hreflang="id" class="interlanguage-link-target">Bahasa Indonesia</a></li><li class="interlanguage-link interwiki-it"><a href="https://it.wikipedia.org/wiki/K-nearest_neighbors" title="K-nearest neighbors – Italian" lang="it" hreflang="it" class="interlanguage-link-target">Italiano</a></li><li class="interlanguage-link interwiki-he"><a href="https://he.wikipedia.org/wiki/אלגוריתם_שכן_קרוב" title="אלגוריתם שכן קרוב – Hebrew" lang="he" hreflang="he" class="interlanguage-link-target">עברית</a></li><li class="interlanguage-link interwiki-ja"><a href="https://ja.wikipedia.org/wiki/K近傍法" title="K近傍法 – Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target">日本語</a></li><li class="interlanguage-link interwiki-no"><a href="https://no.wikipedia.org/wiki/K-NN" title="K-NN – Norwegian" lang="no" hreflang="no" class="interlanguage-link-target">Norsk bokmål</a></li><li class="interlanguage-link interwiki-pl"><a href="https://pl.wikipedia.org/wiki/K_najbliższych_sąsiadów" title="K najbliższych sąsiadów – Polish" lang="pl" hreflang="pl" class="interlanguage-link-target">Polski</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/Метод_k_ближайших_соседей" title="Метод k ближайших соседей – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">Русский</a></li><li class="interlanguage-link interwiki-sr"><a href="https://sr.wikipedia.org/wiki/Алгоритам_к_најближих_суседа" title="Алгоритам к најближих суседа – Serbian" lang="sr" hreflang="sr" class="interlanguage-link-target">Српски / srpski</a></li><li class="interlanguage-link interwiki-th"><a href="https://th.wikipedia.org/wiki/ขั้นตอนวิธีการค้นหาเพื่อนบ้านใกล้สุด_k_ตัว" title="ขั้นตอนวิธีการค้นหาเพื่อนบ้านใกล้สุด k ตัว – Thai" lang="th" hreflang="th" class="interlanguage-link-target">ไทย</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/Метод_найближчих_k-сусідів" title="Метод найближчих k-сусідів – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target">Українська</a></li><li class="interlanguage-link interwiki-vi"><a href="https://vi.wikipedia.org/wiki/Giải_thuật_k_hàng_xóm_gần_nhất" title="Giải thuật k hàng xóm gần nhất – Vietnamese" lang="vi" hreflang="vi" class="interlanguage-link-target">Tiếng Việt</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/最近鄰居法" title="最近鄰居法 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li><li class="uls-p-lang-dummy"><a href="#"></a></li>					</ul>
				<div class='after-portlet after-portlet-lang'><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Q1071612#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last modified on 28 November 2016, at 08:29.</li>
											<li id="footer-info-copyright">Text is available under the <a rel="license" href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="http://wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="http://wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="http://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
									</ul>
							<ul id="footer-places">
											<li id="footer-places-privacy"><a href="https://wikimediafoundation.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
											<li id="footer-places-about"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
											<li id="footer-places-disclaimer"><a href="https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
											<li id="footer-places-contact"><a href="http://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
											<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
											<li id="footer-places-cookiestatement"><a href="https://wikimediafoundation.org/wiki/Cookie_statement">Cookie statement</a></li>
											<li id="footer-places-mobileview"><a href="http://en.m.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
									</ul>
										<ul id="footer-icons" class="noprint">
											<li id="footer-copyrightico">
							<a href="https://wikimediafoundation.org/"><img src="https://en.wikipedia.org/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>						</li>
											<li id="footer-poweredbyico">
							<a href="http://www.mediawiki.org/"><img src="https://en.wikipedia.org/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>						</li>
									</ul>
						<div style="clear:both"></div>
		</div>
		<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":931,"wgHostname":"mw1181"});});</script>
	</body>

<!-- Mirrored from en.wikipedia.org/wiki/K-nearest_neighbors_algorithm by HTTrack Website Copier/3.x [XR&CO'2014], Sat, 31 Dec 2016 05:04:01 GMT -->
</html>
