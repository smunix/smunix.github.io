<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">

Providence Salumu
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=UTF-8" /><!-- /Added by HTTrack -->
<head>
<meta charset="UTF-8"/>
<title>k-means clustering - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"K-means_clustering","wgTitle":"K-means clustering","wgCurRevisionId":756148216,"wgRevisionId":756148216,"wgArticleId":1860407,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 French-language sources (fr)","CS1 maint: Multiple names: authors list","CS1 maint: Uses editors parameter","CS1 maint: Uses authors parameter","All articles with unsourced statements","Articles with unsourced statements from November 2016","Articles with unsourced statements from March 2014","Wikipedia articles needing clarification from February 2016","All Wikipedia articles needing clarification","Articles using small message boxes","Data clustering algorithms","Statistical algorithms"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"K-means_clustering","wgRelevantArticleId":1860407,"wgRequestId":"WGcECwpAIC8AAAf0NZkAAABJ","wgIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgWikiEditorEnabledModules":{"toolbar":true,"dialogs":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","usePageImages":true,"usePageDescriptions":true},"wgPreferredVariant":"en","wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikibaseItemId":"Q310401","wgCentralAuthMobileDomain":false,"wgVisualEditorToolbarScrollOffset":0,"wgEditSubmitButtonLabelPublish":false});mw.loader.state({"ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","user":"ready","user.options":"loading","user.tokens":"loading","ext.math.styles":"ready","ext.cite.styles":"ready","mediawiki.page.gallery.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready"});mw.loader.implement("user.options@0j3lz3q",function($,jQuery,require,module){mw.user.options.set({"variant":"en"});});mw.loader.implement("user.tokens@1dqfd7l",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/;

});mw.loader.load(["ext.math.scripts","ext.cite.a11y","mediawiki.toc","mediawiki.action.view.postEdit","site","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.legacy.wikibits","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.gadget.featured-articles-links","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.interface","ext.quicksurveys.init","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});</script>
<link rel="stylesheet" href="https://en.wikipedia.org/w/load.php?debug=false&amp;lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.page.gallery.styles%7Cmediawiki.sectionAnchor%7Cmediawiki.skinning.interface%7Cskins.vector.styles%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="https://en.wikipedia.org/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="https://en.wikipedia.org/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.29.0-wmf.6"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<link rel="alternate" href="https://en.wikipedia.org/wiki/android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/K-means_clustering"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit"/>
<link rel="apple-touch-icon" href="https://en.wikipedia.org/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="https://en.wikipedia.org/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="https://en.wikipedia.org/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="http://en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="copyright" href="http://creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="K-means_clustering.html"/>
<link rel="dns-prefetch" href="http://login.wikimedia.org/"/>
<link rel="dns-prefetch" href="http://meta.wikimedia.org/" />
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-K-means_clustering rootpage-K-means_clustering skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>

							<div id="siteNotice"><!-- CentralNotice --></div>
						<div class="mw-indicators">
</div>
			<h1 id="firstHeading" class="firstHeading" lang="en"><i>k</i>-means clustering</h1>
									<div id="bodyContent" class="mw-body-content">
									<div id="siteSub">From Wikipedia, the free encyclopedia</div>
								<div id="contentSub"></div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="#mw-head">navigation</a>, 					<a href="#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><script>function mfTempOpenSection(id){var block=document.getElementById("mf-section-"+id);block.className+=" open-block";block.previousSibling.className+=" open-block";}</script><table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%">
<tr>
<th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="Machine_learning.html" title="Machine learning">Machine learning</a> and<br />
<a href="https://en.wikipedia.org/wiki/Data_mining" title="Data mining">data mining</a></th>
</tr>
<tr>
<td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="https://en.wikipedia.org/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="232" /></a></td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="https://en.wikipedia.org/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="https://en.wikipedia.org/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="https://en.wikipedia.org/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
<div style="padding:0.1em 0;line-height:1.2em;"><a href="https://en.wikipedia.org/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br />
<span style="font-weight:normal;"><small>(<b><a href="https://en.wikipedia.org/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;• <b><a href="https://en.wikipedia.org/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</small></span></div>
</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a> (<a href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a>, <a href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a>, <a href="https://en.wikipedia.org/wiki/Random_forest" title="Random forest">Random forest</a>)</li>
<li><a href="K-nearest_neighbors_algorithm.html" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">Neural networks</a></li>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="https://en.wikipedia.org/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="https://en.wikipedia.org/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="https://en.wikipedia.org/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><strong class="selflink"><i>k</i>-means</strong></li>
<li><a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">Expectation-maximization (EM)</a></li>
<li><br />
<a href="https://en.wikipedia.org/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="https://en.wikipedia.org/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Canonical_correlation_analysis" class="mw-redirect" title="Canonical correlation analysis">CCA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Graphical_model" title="Graphical model">Graphical models</a> (<a href="https://en.wikipedia.org/wiki/Bayesian_network" title="Bayesian network">Bayes net</a>, <a href="https://en.wikipedia.org/wiki/Conditional_random_field" title="Conditional random field">CRF</a>, <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" title="Hidden Markov model">HMM</a>)</li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_classification" class="mw-redirect" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Artificial_neural_network" title="Artificial neural network">Neural nets</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="https://en.wikipedia.org/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a></li>
<li><a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network" title="Convolutional neural network">Convolutional neural network</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="https://en.wikipedia.org/wiki/Reinforcement_Learning" class="mw-redirect" title="Reinforcement Learning">Reinforcement Learning</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Q-Learning" class="mw-redirect" title="Q-Learning">Q-Learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/State-Action-Reward-State-Action" title="State-Action-Reward-State-Action">SARSA</a></li>
<li><a href="https://en.wikipedia.org/wiki/Temporal_Difference_Learning" class="mw-redirect" title="Temporal Difference Learning">Temporal Difference (TD)</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Bias-variance_dilemma" class="mw-redirect" title="Bias-variance dilemma">Bias-variance dilemma</a></li>
<li><a href="https://en.wikipedia.org/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="https://en.wikipedia.org/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Vapnik–Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine learning venues</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="https://en.wikipedia.org/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a></li>
<li><a href="https://en.wikipedia.org/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="https://en.wikipedia.org/w/index.php?title=International_Journal_of_Machine_Learning_and_Cybernetics&amp;action=edit&amp;redlink=1" class="new" title="International Journal of Machine Learning and Cybernetics (page does not exist)">IJMLC</a></li>
<li><a href="https://en.wikipedia.org/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="https://en.wikipedia.org/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a rel="nofollow" class="external text" href="http://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td class="plainlist" style="padding:0.3em 0.4em 0.3em;font-weight:bold;border-top: 1px solid #aaa; border-bottom: 1px solid #aaa;border-top:1px solid #aaa;border-bottom:1px solid #aaa;">
<ul>
<li><a href="https://en.wikipedia.org/wiki/File:Portal-puzzle.svg" class="image"><img alt="Portal-puzzle.svg" src="http://upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png" width="16" height="14" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28" /></a> <a href="https://en.wikipedia.org/wiki/Portal:Machine_learning" title="Portal:Machine learning">Machine learning portal</a></li>
</ul>
</td>
</tr>
<tr>
<td style="text-align:right;font-size:115%;padding-top: 0.6em;">
<div class="plainlinks hlist navbar mini">
<ul>
<li class="nv-view"><a href="https://en.wikipedia.org/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li>
<li class="nv-talk"><a href="https://en.wikipedia.org/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li>
<li class="nv-edit"><a class="external text" href="http://en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li>
</ul>
</div>
</td>
</tr>
</table>
<p><b><i>k</i>-means clustering</b> is a method of <a href="https://en.wikipedia.org/wiki/Vector_quantization" title="Vector quantization">vector quantization</a>, originally from <a href="https://en.wikipedia.org/wiki/Signal_processing" title="Signal processing">signal processing</a>, that is popular for <a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">cluster analysis</a> in <a href="https://en.wikipedia.org/wiki/Data_mining" title="Data mining">data mining</a>. <i>k</i>-means clustering aims to <a href="https://en.wikipedia.org/wiki/Partition_of_a_set" title="Partition of a set">partition</a> <i>n</i> observations into <i>k</i> clusters in which each observation belongs to the <a href="https://en.wikipedia.org/wiki/Cluster_(statistics)" class="mw-redirect" title="Cluster (statistics)">cluster</a> with the nearest <a href="https://en.wikipedia.org/wiki/Mean" title="Mean">mean</a>, serving as a <a href="https://en.wikipedia.org/wiki/Prototype" title="Prototype">prototype</a> of the cluster. This results in a partitioning of the data space into <a href="https://en.wikipedia.org/wiki/Voronoi_cell" class="mw-redirect" title="Voronoi cell">Voronoi cells</a>.</p>
<p>The problem is computationally difficult (<a href="https://en.wikipedia.org/wiki/NP-hard" class="mw-redirect" title="NP-hard">NP-hard</a>); however, there are efficient <a href="https://en.wikipedia.org/wiki/Heuristic_algorithm" class="mw-redirect" title="Heuristic algorithm">heuristic algorithms</a> that are commonly employed and converge quickly to a <a href="https://en.wikipedia.org/wiki/Local_optimum" title="Local optimum">local optimum</a>. These are usually similar to the <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a> for <a href="https://en.wikipedia.org/wiki/Mixture_model" title="Mixture model">mixtures</a> of <a href="https://en.wikipedia.org/wiki/Gaussian_distribution" class="mw-redirect" title="Gaussian distribution">Gaussian distributions</a> via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, <i>k</i>-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.</p>
<p>The algorithm has a loose relationship to the <a href="https://en.wikipedia.org/wiki/K-nearest_neighbor" class="mw-redirect" title="K-nearest neighbor"><i>k</i>-nearest neighbor classifier</a>, a popular <a href="Machine_learning.html" title="Machine learning">machine learning</a> technique for classification that is often confused with <i>k</i>-means because of the <i>k</i> in the name. One can apply the 1-nearest neighbor classifier on the cluster centers obtained by <i>k</i>-means to classify new data into the existing clusters. This is known as <a href="https://en.wikipedia.org/wiki/Nearest_centroid_classifier" title="Nearest centroid classifier">nearest centroid classifier</a> or Rocchio algorithm<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (November 2016)">citation needed</span></a></i>]</sup>.</p>
<p></p>
<div id="toc" class="toc">
<div id="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Description"><span class="tocnumber">1</span> <span class="toctext">Description</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#History"><span class="tocnumber">2</span> <span class="toctext">History</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Algorithms"><span class="tocnumber">3</span> <span class="toctext">Algorithms</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#Standard_algorithm"><span class="tocnumber">3.1</span> <span class="toctext">Standard algorithm</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="#Initialization_methods"><span class="tocnumber">3.1.1</span> <span class="toctext">Initialization methods</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-6"><a href="#Complexity"><span class="tocnumber">3.2</span> <span class="toctext">Complexity</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Variations"><span class="tocnumber">3.3</span> <span class="toctext">Variations</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-8"><a href="#Discussion"><span class="tocnumber">4</span> <span class="toctext">Discussion</span></a></li>
<li class="toclevel-1 tocsection-9"><a href="#Applications"><span class="tocnumber">5</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2 tocsection-10"><a href="#Vector_quantization"><span class="tocnumber">5.1</span> <span class="toctext">Vector quantization</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Cluster_analysis"><span class="tocnumber">5.2</span> <span class="toctext">Cluster analysis</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Feature_learning"><span class="tocnumber">5.3</span> <span class="toctext">Feature learning</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-13"><a href="#Relation_to_other_statistical_machine_learning_algorithms"><span class="tocnumber">6</span> <span class="toctext">Relation to other statistical machine learning algorithms</span></a>
<ul>
<li class="toclevel-2 tocsection-14"><a href="#Gaussian_Mixture_Model"><span class="tocnumber">6.1</span> <span class="toctext">Gaussian Mixture Model</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Principal_component_analysis_.28PCA.29"><span class="tocnumber">6.2</span> <span class="toctext">Principal component analysis (PCA)</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Mean_shift_clustering"><span class="tocnumber">6.3</span> <span class="toctext">Mean shift clustering</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Independent_component_analysis_.28ICA.29"><span class="tocnumber">6.4</span> <span class="toctext">Independent component analysis (ICA)</span></a></li>
<li class="toclevel-2 tocsection-18"><a href="#Bilateral_filtering"><span class="tocnumber">6.5</span> <span class="toctext">Bilateral filtering</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-19"><a href="#Similar_problems"><span class="tocnumber">7</span> <span class="toctext">Similar problems</span></a></li>
<li class="toclevel-1 tocsection-20"><a href="#Software_implementations"><span class="tocnumber">8</span> <span class="toctext">Software implementations</span></a>
<ul>
<li class="toclevel-2 tocsection-21"><a href="#Free_Software.2FOpen_Source"><span class="tocnumber">8.1</span> <span class="toctext">Free Software/Open Source</span></a></li>
<li class="toclevel-2 tocsection-22"><a href="#Proprietary"><span class="tocnumber">8.2</span> <span class="toctext">Proprietary</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-23"><a href="#See_also"><span class="tocnumber">9</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-24"><a href="#References"><span class="tocnumber">10</span> <span class="toctext">References</span></a></li>
</ul>
</div>
<p></p>
<h2><span class="mw-headline" id="Description">Description</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=1" title="Edit section: Description">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Given a set of observations (<b>x</b><sub>1</sub>, <b>x</b><sub>2</sub>, …, <b>x</b><sub><i>n</i></sub>), where each observation is a <i>d</i>-dimensional real vector, <i>k</i>-means clustering aims to partition the <i>n</i> observations into <i>k</i> (≤ <i>n</i>) sets <b>S</b>&#160;=&#160;{<i>S</i><sub>1</sub>,&#160;<i>S</i><sub>2</sub>,&#160;…,&#160;<i>S</i><sub><i>k</i></sub>} so as to minimize the within-cluster sum of squares (WCSS) (sum of distance functions of each point in the cluster to the K center). In other words, its objective is to find:</p>
<center><span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <munder>
            <mrow class="MJX-TeXAtom-OP">
              <mi mathvariant="normal">a</mi>
              <mi mathvariant="normal">r</mi>
              <mi mathvariant="normal">g</mi>
              <mspace width="thinmathspace" />
              <mi mathvariant="normal">m</mi>
              <mi mathvariant="normal">i</mi>
              <mi mathvariant="normal">n</mi>
            </mrow>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">S</mi>
            </mrow>
          </munder>
        </mrow>
        <munderover>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
            <mo>=</mo>
            <mn>1</mn>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>k</mi>
          </mrow>
        </munderover>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mo>&#x2208;<!-- ∈ --></mo>
            <msub>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
          </mrow>
        </munder>
        <msup>
          <mrow>
            <mo>&#x2225;</mo>
            <mrow class="MJX-TeXAtom-ORD">
              <mi mathvariant="bold">x</mi>
            </mrow>
            <mo>&#x2212;<!-- − --></mo>
            <msub>
              <mrow class="MJX-TeXAtom-ORD">
                <mi mathvariant="bold-italic">&#x03BC;<!-- μ --></mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
            </msub>
            <mo>&#x2225;</mo>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/debd28209802c22a6e6a1d74d099f728e6bd17a4" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -3.505ex; width:25.986ex; height:7.843ex;" alt="{\underset {\mathbf {S} }{\operatorname {arg\,min} }}\sum _{i=1}^{k}\sum _{\mathbf {x} \in S_{i}}\left\|\mathbf {x} -{\boldsymbol {\mu }}_{i}\right\|^{2}" /></span></center>
<p>where <i><b>μ</b></i><sub><i>i</i></sub> is the mean of points in <i>S</i><sub><i>i</i></sub>.</p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=2" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The term "<i>k</i>-means" was first used by James MacQueen in 1967,<sup id="cite_ref-macqueen1967_1-0" class="reference"><a href="#cite_note-macqueen1967-1">[1]</a></sup> though the idea goes back to <a href="https://en.wikipedia.org/wiki/Hugo_Steinhaus" title="Hugo Steinhaus">Hugo Steinhaus</a> in 1957.<sup id="cite_ref-2" class="reference"><a href="#cite_note-2">[2]</a></sup> The <a href="#Standard_algorithm">standard algorithm</a> was first proposed by Stuart Lloyd in 1957 as a technique for <a href="https://en.wikipedia.org/wiki/Pulse-code_modulation" title="Pulse-code modulation">pulse-code modulation</a>, though it wasn't published outside of <a href="https://en.wikipedia.org/wiki/Bell_Labs" title="Bell Labs">Bell Labs</a> until 1982.<sup id="cite_ref-lloyd1957_3-0" class="reference"><a href="#cite_note-lloyd1957-3">[3]</a></sup> In 1965, E. W. Forgy published essentially the same method, which is why it is sometimes referred to as Lloyd-Forgy.<sup id="cite_ref-forgy65_4-0" class="reference"><a href="#cite_note-forgy65-4">[4]</a></sup> A more efficient version was later proposed and published in <a href="https://en.wikipedia.org/wiki/FORTRAN" class="mw-redirect" title="FORTRAN">FORTRAN</a> by Hartigan and Wong.<sup id="cite_ref-hartigan1975_5-0" class="reference"><a href="#cite_note-hartigan1975-5">[5]</a></sup><sup id="cite_ref-hartigan1979_6-0" class="reference"><a href="#cite_note-hartigan1979-6">[6]</a></sup></p>
<h2><span class="mw-headline" id="Algorithms">Algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=3" title="Edit section: Algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Standard_algorithm">Standard algorithm</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=4" title="Edit section: Standard algorithm">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The most common algorithm uses an iterative refinement technique. Due to its ubiquity it is often called the <b><i>k</i>-means algorithm</b>; it is also referred to as <b><a href="https://en.wikipedia.org/wiki/Lloyd%27s_algorithm" title="Lloyd's algorithm">Lloyd's algorithm</a></b>, particularly in the computer science community.</p>
<p>Given an initial set of <i>k</i> means <i>m</i><sub>1</sub><sup>(1)</sup>,…,<i>m</i><sub><i>k</i></sub><sup>(1)</sup> (see below), the algorithm proceeds by alternating between two steps:<sup id="cite_ref-7" class="reference"><a href="#cite_note-7">[7]</a></sup></p>
<dl>
<dd><b>Assignment step</b>: Assign each observation to the cluster whose mean yields the least within-cluster sum of squares (WCSS). Since the sum of squares is the squared <a href="https://en.wikipedia.org/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a>, this is intuitively the "nearest" mean.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">[8]</a></sup> (Mathematically, this means partitioning the observations according to the <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" title="Voronoi diagram">Voronoi diagram</a> generated by the means).
<dl>
<dd><span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo maxsize="1.2em" minsize="1.2em">{</mo>
          </mrow>
        </mrow>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>p</mi>
          </mrow>
        </msub>
        <mo>:</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo fence="true" stretchy="true" symmetric="true" maxsize="1.2em" minsize="1.2em">&#x2225;</mo>
          </mrow>
        </mrow>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>p</mi>
          </mrow>
        </msub>
        <mo>&#x2212;<!-- − --></mo>
        <msubsup>
          <mi>m</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mo fence="true" stretchy="true" symmetric="true" maxsize="1.2em" minsize="1.2em">&#x2225;</mo>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo fence="true" stretchy="true" symmetric="true" maxsize="1.2em" minsize="1.2em">&#x2225;</mo>
          </mrow>
        </mrow>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>p</mi>
          </mrow>
        </msub>
        <mo>&#x2212;<!-- − --></mo>
        <msubsup>
          <mi>m</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <msup>
          <mrow class="MJX-TeXAtom-ORD">
            <mrow class="MJX-TeXAtom-ORD">
              <mo fence="true" stretchy="true" symmetric="true" maxsize="1.2em" minsize="1.2em">&#x2225;</mo>
            </mrow>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mtext>&#xA0;</mtext>
        <mi mathvariant="normal">&#x2200;<!-- ∀ --></mi>
        <mi>j</mi>
        <mo>,</mo>
        <mn>1</mn>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mi>j</mi>
        <mo>&#x2264;<!-- ≤ --></mo>
        <mi>k</mi>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mo maxsize="1.2em" minsize="1.2em">}</mo>
          </mrow>
        </mrow>
        <mo>,</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S_{i}^{(t)}={\big \{}x_{p}:{\big \|}x_{p}-m_{i}^{(t)}{\big \|}^{2}\leq {\big \|}x_{p}-m_{j}^{(t)}{\big \|}^{2}\ \forall j,1\leq j\leq k{\big \}},}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/145a262c93066470be0e062683d64340a1b20121" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.338ex; width:56.785ex; height:4.009ex;" alt="S_{i}^{(t)}={\big \{}x_{p}:{\big \|}x_{p}-m_{i}^{(t)}{\big \|}^{2}\leq {\big \|}x_{p}-m_{j}^{(t)}{\big \|}^{2}\ \forall j,1\leq j\leq k{\big \}}," /></span></dd>
<dd>where each <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>p</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle x_{p}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0bec554743fa797409a83ad8d00b4d35e110a50a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -1.005ex; width:2.407ex; height:2.343ex;" alt="x_{p}" /></span> is assigned to exactly one <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>S</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo stretchy="false">)</mo>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S^{(t)}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4cc468cbde601ba30acbde7fb11bd9bf8b04b8c8" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:3.66ex; height:2.843ex;" alt="S^{(t)}" /></span>, even if it could be assigned to two or more of them.</dd>
</dl>
</dd>
<dd><b>Update step</b>: Calculate the new means to be the <a href="https://en.wikipedia.org/wiki/Centroids" class="mw-redirect" title="Centroids">centroids</a> of the observations in the new clusters.
<dl>
<dd><span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msubsup>
          <mi>m</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>i</mi>
          </mrow>
          <mrow class="MJX-TeXAtom-ORD">
            <mo stretchy="false">(</mo>
            <mi>t</mi>
            <mo>+</mo>
            <mn>1</mn>
            <mo stretchy="false">)</mo>
          </mrow>
        </msubsup>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mfrac>
            <mn>1</mn>
            <mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">|</mo>
              </mrow>
              <msubsup>
                <mi>S</mi>
                <mrow class="MJX-TeXAtom-ORD">
                  <mi>i</mi>
                </mrow>
                <mrow class="MJX-TeXAtom-ORD">
                  <mo stretchy="false">(</mo>
                  <mi>t</mi>
                  <mo stretchy="false">)</mo>
                </mrow>
              </msubsup>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">|</mo>
              </mrow>
            </mrow>
          </mfrac>
        </mrow>
        <munder>
          <mo>&#x2211;<!-- ∑ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <msub>
              <mi>x</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>j</mi>
              </mrow>
            </msub>
            <mo>&#x2208;<!-- ∈ --></mo>
            <msubsup>
              <mi>S</mi>
              <mrow class="MJX-TeXAtom-ORD">
                <mi>i</mi>
              </mrow>
              <mrow class="MJX-TeXAtom-ORD">
                <mo stretchy="false">(</mo>
                <mi>t</mi>
                <mo stretchy="false">)</mo>
              </mrow>
            </msubsup>
          </mrow>
        </munder>
        <msub>
          <mi>x</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>j</mi>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle m_{i}^{(t+1)}={\frac {1}{|S_{i}^{(t)}|}}\sum _{x_{j}\in S_{i}^{(t)}}x_{j}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/740f4271e822c6400120cb7020ed9cb8439207da" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -4.505ex; width:23.802ex; height:7.843ex;" alt="m_{i}^{(t+1)}={\frac {1}{|S_{i}^{(t)}|}}\sum _{x_{j}\in S_{i}^{(t)}}x_{j}" /></span></dd>
<dd>Since the arithmetic mean is a <a href="https://en.wikipedia.org/wiki/Least-squares_estimation" class="mw-redirect" title="Least-squares estimation">least-squares estimator</a>, this also minimizes the within-cluster sum of squares (WCSS) objective.</dd>
</dl>
</dd>
</dl>
<p>The algorithm has converged when the assignments no longer change. Since both steps optimize the WCSS objective, and there only exists a finite number of such partitionings, the algorithm must converge to a (local) optimum. There is no guarantee that the global optimum is found using this algorithm.</p>
<p>The algorithm is often presented as assigning objects to the nearest cluster by distance. The standard algorithm aims at minimizing the WCSS objective, and thus assigns by "least sum of squares", which is exactly equivalent to assigning by the smallest Euclidean distance. Using a different distance function other than (squared) Euclidean distance may stop the algorithm from converging.<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="https://en.wikipedia.org/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (March 2014)">citation needed</span></a></i>]</sup> Various modifications of k-means such as spherical k-means and <a href="https://en.wikipedia.org/wiki/K-medoids" title="K-medoids">k-medoids</a> have been proposed to allow using other distance measures.</p>
<h4><span class="mw-headline" id="Initialization_methods">Initialization methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=5" title="Edit section: Initialization methods">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Commonly used initialization methods are Forgy and Random Partition.<sup id="cite_ref-hamerly_9-0" class="reference"><a href="#cite_note-hamerly-9">[9]</a></sup> The Forgy method randomly chooses <i>k</i> observations from the data set and uses these as the initial means. The Random Partition method first randomly assigns a cluster to each observation and then proceeds to the update step, thus computing the initial mean to be the centroid of the cluster's randomly assigned points. The Forgy method tends to spread the initial means out, while Random Partition places all of them close to the center of the data set. According to Hamerly et al.,<sup id="cite_ref-hamerly_9-1" class="reference"><a href="#cite_note-hamerly-9">[9]</a></sup> the Random Partition method is generally preferable for algorithms such as the <i>k</i>-harmonic means and fuzzy <i>k</i>-means. For expectation maximization and standard <i>k</i>-means algorithms, the Forgy method of initialization is preferable.</p>
<ul class="gallery mw-gallery-traditional">
<li class='gallerycaption'>Demonstration of the standard algorithm</li>
<li class="gallerybox" style="width: 185px">
<div style="width: 185px">
<div class="thumb" style="width: 180px;">
<div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:K_Means_Example_Step_1.svg" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/K_Means_Example_Step_1.svg/124px-K_Means_Example_Step_1.svg.png" width="124" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/5/5e/K_Means_Example_Step_1.svg/187px-K_Means_Example_Step_1.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/5/5e/K_Means_Example_Step_1.svg/249px-K_Means_Example_Step_1.svg.png 2x" data-file-width="197" data-file-height="190" /></a></div>
</div>
<div class="gallerytext">
<p>1. <i>k</i> initial "means" (in this case <i>k</i>=3) are randomly generated within the data domain (shown in color).</p>
</div>
</div>
</li>
<li class="gallerybox" style="width: 185px">
<div style="width: 185px">
<div class="thumb" style="width: 180px;">
<div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:K_Means_Example_Step_2.svg" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/K_Means_Example_Step_2.svg/139px-K_Means_Example_Step_2.svg.png" width="139" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/a/a5/K_Means_Example_Step_2.svg/209px-K_Means_Example_Step_2.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a5/K_Means_Example_Step_2.svg/278px-K_Means_Example_Step_2.svg.png 2x" data-file-width="197" data-file-height="170" /></a></div>
</div>
<div class="gallerytext">
<p>2. <i>k</i> clusters are created by associating every observation with the nearest mean. The partitions here represent the <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" title="Voronoi diagram">Voronoi diagram</a> generated by the means.</p>
</div>
</div>
</li>
<li class="gallerybox" style="width: 185px">
<div style="width: 185px">
<div class="thumb" style="width: 180px;">
<div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:K_Means_Example_Step_3.svg" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/K_Means_Example_Step_3.svg/139px-K_Means_Example_Step_3.svg.png" width="139" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3e/K_Means_Example_Step_3.svg/209px-K_Means_Example_Step_3.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3e/K_Means_Example_Step_3.svg/278px-K_Means_Example_Step_3.svg.png 2x" data-file-width="197" data-file-height="170" /></a></div>
</div>
<div class="gallerytext">
<p>3. The <a href="https://en.wikipedia.org/wiki/Centroid" title="Centroid">centroid</a> of each of the <i>k</i> clusters becomes the new mean.</p>
</div>
</div>
</li>
<li class="gallerybox" style="width: 185px">
<div style="width: 185px">
<div class="thumb" style="width: 180px;">
<div style="margin:15px auto;"><a href="https://en.wikipedia.org/wiki/File:K_Means_Example_Step_4.svg" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/K_Means_Example_Step_4.svg/139px-K_Means_Example_Step_4.svg.png" width="139" height="120" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/d2/K_Means_Example_Step_4.svg/209px-K_Means_Example_Step_4.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/d/d2/K_Means_Example_Step_4.svg/278px-K_Means_Example_Step_4.svg.png 2x" data-file-width="197" data-file-height="170" /></a></div>
</div>
<div class="gallerytext">
<p>4. Steps 2 and 3 are repeated until convergence has been reached.</p>
</div>
</div>
</li>
</ul>
<p>As it is a heuristic algorithm, there is no guarantee that it will converge to the global optimum, and the result may depend on the initial clusters. As the algorithm is usually very fast, it is common to run it multiple times with different starting conditions. However, in the worst case, <i>k</i>-means can be very slow to converge: in particular it has been shown that there exist certain point sets, even in 2 dimensions, on which <i>k</i>-means takes exponential time, that is <span class="texhtml">2<sup>Ω(<var>n</var>)</sup></span>, to converge.<sup id="cite_ref-10" class="reference"><a href="#cite_note-10">[10]</a></sup> These point sets do not seem to arise in practice: this is corroborated by the fact that the <a href="https://en.wikipedia.org/wiki/Smoothed_analysis" title="Smoothed analysis">smoothed</a> running time of <i>k</i>-means is polynomial.<sup id="cite_ref-Arthur.2C_D..3B_Manthey.2C_B..3B_Roeglin.2C_H._2009_11-0" class="reference"><a href="#cite_note-Arthur.2C_D..3B_Manthey.2C_B..3B_Roeglin.2C_H._2009-11">[11]</a></sup></p>
<p>The "assignment" step is also referred to as <b>expectation step</b>, the "update step" as <b>maximization step</b>, making this algorithm a variant of the <i>generalized</i> <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a>.</p>
<h3><span class="mw-headline" id="Complexity">Complexity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=6" title="Edit section: Complexity">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Regarding computational complexity, finding the optimal solution to the <i>k</i>-means clustering problem for observations in <i>d</i> dimensions is:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/NP-hard" class="mw-redirect" title="NP-hard">NP-hard</a> in general Euclidean space <i>d</i> even for 2 clusters<sup id="cite_ref-12" class="reference"><a href="#cite_note-12">[12]</a></sup><sup id="cite_ref-13" class="reference"><a href="#cite_note-13">[13]</a></sup></li>
<li><a href="https://en.wikipedia.org/wiki/NP-hard" class="mw-redirect" title="NP-hard">NP-hard</a> for a general number of clusters <i>k</i> even in the plane<sup id="cite_ref-14" class="reference"><a href="#cite_note-14">[14]</a></sup></li>
<li>If <i>k</i> and <i>d</i> (the dimension) are fixed, the problem can be exactly solved in time <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>O</mi>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
            <mi>k</mi>
            <mo>+</mo>
            <mn>1</mn>
          </mrow>
        </msup>
        <mi>log</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mi>n</mi>
        </mrow>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle O(n^{dk+1}\log {n})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/ce4420f2f37226f6a8f606891e74a9efce36cfbe" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:14.28ex; height:3.176ex;" alt="O(n^{dk+1}\log {n})" /></span>, where <i>n</i> is the number of entities to be clustered<sup id="cite_ref-15" class="reference"><a href="#cite_note-15">[15]</a></sup></li>
</ul>
<p>Thus, a variety of <a href="https://en.wikipedia.org/wiki/Heuristic_algorithm" class="mw-redirect" title="Heuristic algorithm">heuristic algorithms</a> such as Lloyd's algorithm given above are generally used.</p>
<p>The running time of Lloyd's algorithm is often given as <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>O</mi>
        <mo stretchy="false">(</mo>
        <mi>n</mi>
        <mi>k</mi>
        <mi>d</mi>
        <mi>i</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle O(nkdi)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/7a1d9d1a265e27b3a21e050f1730aea5eb28e577" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:8.28ex; height:2.843ex;" alt="O(nkdi)" /></span>, where <i>n</i> is the number of <i>d</i>-dimensional vectors, <i>k</i> the number of clusters and <i>i</i> the number of iterations needed until convergence. On data that does have a clustering structure, the number of iterations until convergence is often small, and results only improve slightly after the first dozen iterations. Lloyd's algorithm is therefore often considered to be of "linear" complexity in practice.</p>
<p>Following are some recent insights into this algorithm complexity behavior.</p>
<ul>
<li>Lloyd's <i>k</i>-means algorithm has polynomial smoothed running time. It is shown that<sup id="cite_ref-Arthur.2C_D..3B_Manthey.2C_B..3B_Roeglin.2C_H._2009_11-1" class="reference"><a href="#cite_note-Arthur.2C_D..3B_Manthey.2C_B..3B_Roeglin.2C_H._2009-11">[11]</a></sup> for arbitrary set of <i>n</i> points in <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">[</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <msup>
          <mo stretchy="false">]</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle [0,1]^{d}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/e13ae4917276744b214714a20b3cb8ee305e309d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:5.804ex; height:3.176ex;" alt="[0,1]^{d}" /></span>, if each point is independently perturbed by a normal distribution with mean <span class="texhtml">0</span> and variance <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mi>&#x03C3;<!-- σ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \sigma ^{2}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/53a5c55e536acf250c1d3e0f754be5692b843ef5" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.403ex; height:2.676ex;" alt="\sigma ^{2}" /></span>, then the expected running time of <span class="texhtml mvar" style="font-style:italic;">k</span>-means algorithm is bounded by <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>O</mi>
        <mo stretchy="false">(</mo>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>34</mn>
          </mrow>
        </msup>
        <msup>
          <mi>k</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>34</mn>
          </mrow>
        </msup>
        <msup>
          <mi>d</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>8</mn>
          </mrow>
        </msup>
        <msup>
          <mi>log</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>4</mn>
          </mrow>
        </msup>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>n</mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <msup>
          <mi>&#x03C3;<!-- σ --></mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>6</mn>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle O(n^{34}k^{34}d^{8}\log ^{4}(n)/\sigma ^{6})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/245c9d99b5cc1788e4e67fdf449d1cfcb160bf31" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:23.576ex; height:3.176ex;" alt="{\displaystyle O(n^{34}k^{34}d^{8}\log ^{4}(n)/\sigma ^{6})}" /></span>, which is a polynomial in <span class="texhtml mvar" style="font-style:italic;">n</span>, <span class="texhtml mvar" style="font-style:italic;">k</span>, <span class="texhtml mvar" style="font-style:italic;">d</span> and <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mn>1</mn>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi>&#x03C3;<!-- σ --></mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 1/\sigma }</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/00d5187486468042e9692b18c216b60679aafef3" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:3.686ex; height:2.843ex;" alt="1/\sigma " /></span>.</li>
<li>Better bounds are proved for simple cases. For example,<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">[16]</a></sup> showed that the running time of <i>k</i>-means algorithm is bounded by <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>O</mi>
        <mo stretchy="false">(</mo>
        <mi>d</mi>
        <msup>
          <mi>n</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>4</mn>
          </mrow>
        </msup>
        <msup>
          <mi>M</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msup>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle O(dn^{4}M^{2})}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8892c255d524bfb2ef96555e56746c603945dddd" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:10.878ex; height:3.176ex;" alt="O(dn^{4}M^{2})" /></span> for <span class="texhtml mvar" style="font-style:italic;">n</span> points in an <a href="https://en.wikipedia.org/wiki/Integer_lattice" title="Integer lattice">integer lattice</a> <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">{</mo>
        <mn>1</mn>
        <mo>,</mo>
        <mo>&#x2026;<!-- … --></mo>
        <mo>,</mo>
        <mi>M</mi>
        <msup>
          <mo fence="false" stretchy="false">}</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>d</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \{1,\dots ,M\}^{d}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b357149a0b365e24b8636c26bec1e899fe5e64e7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:12.28ex; height:3.176ex;" alt="\{1,\dots ,M\}^{d}" /></span>.</li>
</ul>
<p>Lloyd's algorithm is the standard approach for this problem, However, it spends a lot of processing time computing the distances between each of the k cluster centers and the n data points. Since points usually stay in the same clusters after a few iterations, much of this work is unnecessary, making the naive implementation very inefficient. Some implementations use the triangle inequality in order to create bounds and accelerate Lloyd's algorithm.<sup id="cite_ref-phillips_17-0" class="reference"><a href="#cite_note-phillips-17">[17]</a></sup><sup id="cite_ref-elkan_18-0" class="reference"><a href="#cite_note-elkan-18">[18]</a></sup><sup id="cite_ref-hamerly2_19-0" class="reference"><a href="#cite_note-hamerly2-19">[19]</a></sup></p>
<h3><span class="mw-headline" id="Variations">Variations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=7" title="Edit section: Variations">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization" title="Jenks natural breaks optimization">Jenks natural breaks optimization</a>: <i>k</i>-means applied to univariate data</li>
<li><a href="https://en.wikipedia.org/wiki/K-medians_clustering" title="K-medians clustering">k-medians clustering</a> uses the median in each dimension instead of the mean, and this way minimizes <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msub>
          <mi>L</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>1</mn>
          </mrow>
        </msub>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle L_{1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0e79dc1b001f8b923df475ed14de023cbc456013" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:2.655ex; height:2.509ex;" alt="L_{1}" /></span> norm (<a href="https://en.wikipedia.org/wiki/Taxicab_geometry" title="Taxicab geometry">Taxicab geometry</a>).</li>
<li><a href="https://en.wikipedia.org/wiki/K-medoids" title="K-medoids">k-medoids</a> (also: Partitioning Around Medoids, PAM) uses the medoid instead of the mean, and this way minimizes the sum of distances for <i>arbitrary</i> distance functions.</li>
<li><a href="https://en.wikipedia.org/wiki/Fuzzy_clustering#Fuzzy_c-means_clustering" title="Fuzzy clustering">Fuzzy C-Means Clustering</a> is a soft version of K-means, where each data point has a fuzzy degree of belonging to each cluster.</li>
<li><a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model" title="Mixture model">Gaussian mixture</a> models trained with <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">expectation-maximization algorithm</a> (EM algorithm) maintains probabilistic assignments to clusters, instead of deterministic assignments, and multivariate Gaussian distributions instead of means.</li>
<li><a href="https://en.wikipedia.org/wiki/K-means++" title="K-means++">k-means++</a> chooses initial centers in a way that gives a provable upper bound on the WCSS objective.</li>
<li>The filtering algorithm uses <a href="https://en.wikipedia.org/wiki/Kd-tree" class="mw-redirect" title="Kd-tree">kd-trees</a> to speed up each k-means step.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">[20]</a></sup></li>
<li>Some methods attempt to speed up each k-means step using the <a href="https://en.wikipedia.org/wiki/Triangle_inequality" title="Triangle inequality">triangle inequality</a>.<sup id="cite_ref-phillips_17-1" class="reference"><a href="#cite_note-phillips-17">[17]</a></sup><sup id="cite_ref-elkan_18-1" class="reference"><a href="#cite_note-elkan-18">[18]</a></sup><sup id="cite_ref-hamerly2_19-1" class="reference"><a href="#cite_note-hamerly2-19">[19]</a></sup><sup id="cite_ref-21" class="reference"><a href="#cite_note-21">[21]</a></sup></li>
<li>Escape local optima by swapping points between clusters.<sup id="cite_ref-hartigan1979_6-1" class="reference"><a href="#cite_note-hartigan1979-6">[6]</a></sup></li>
<li>The <a href="https://en.wikipedia.org/w/index.php?title=Spherical_k-means&amp;action=edit&amp;redlink=1" class="new" title="Spherical k-means (page does not exist)">Spherical k-means</a> clustering algorithm is suitable for textual data.<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">[22]</a></sup></li>
<li>Hierarchical variants such as Bisecting k-means,<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">[23]</a></sup> <a href="https://en.wikipedia.org/wiki/X-means_clustering" class="mw-redirect" title="X-means clustering">X-means clustering</a><sup id="cite_ref-24" class="reference"><a href="#cite_note-24">[24]</a></sup> and G-means clustering<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">[25]</a></sup> <a href="https://en.wikipedia.org/wiki/Hierarchical_clustering#Divisive_clustering" title="Hierarchical clustering">repeatedly split clusters to build a hierarchy</a>, and can also try to automatically determine the optimal number of clusters in a dataset.</li>
<li><a href="https://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation" title="Cluster analysis">Internal cluster evaluation</a> measures such as <a href="https://en.wikipedia.org/wiki/Silhouette_(clustering)" title="Silhouette (clustering)">cluster silhouette</a> can be helpful at <a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set" title="Determining the number of clusters in a data set">determining the number of clusters</a>.</li>
<li><a href="https://en.wikipedia.org/w/index.php?title=Minkowski_weighted_k-means&amp;action=edit&amp;redlink=1" class="new" title="Minkowski weighted k-means (page does not exist)">Minkowski weighted k-means</a> automatically calculates cluster specific feature weights, supporting the intuitive idea that a feature may have different degrees of relevance at different features.<sup id="cite_ref-26" class="reference"><a href="#cite_note-26">[26]</a></sup> These weights can also be used to re-scale a given data set, increasing the likelihood of a cluster validity index to be optimized at the expected number of clusters.<sup id="cite_ref-27" class="reference"><a href="#cite_note-27">[27]</a></sup></li>
<li>Mini-batch K-means: K-means variation using "mini batch" samples for data sets that do not fit into memory.<sup id="cite_ref-28" class="reference"><a href="#cite_note-28">[28]</a></sup></li>
</ul>
<h2><span class="mw-headline" id="Discussion">Discussion</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=8" title="Edit section: Discussion">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="thumb tright">
<div class="thumbinner" style="width:652px;"><a href="https://en.wikipedia.org/wiki/File:K-means_convergence_to_a_local_minimum.png" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/K-means_convergence_to_a_local_minimum.png/650px-K-means_convergence_to_a_local_minimum.png" width="650" height="78" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/7/7c/K-means_convergence_to_a_local_minimum.png/975px-K-means_convergence_to_a_local_minimum.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/7c/K-means_convergence_to_a_local_minimum.png/1300px-K-means_convergence_to_a_local_minimum.png 2x" data-file-width="2914" data-file-height="349" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="https://en.wikipedia.org/wiki/File:K-means_convergence_to_a_local_minimum.png" class="internal" title="Enlarge"></a></div>
A typical example of the k-means convergence to a local minimum. In this example, the result of k-means clustering (the right figure) contradicts the obvious cluster structure of the data set. The small circles are the data points, the four ray stars are the centroids (means). The initial configuration is on the left figure. The algorithm converges after five iterations presented on the figures, from the left to the right. The illustration was prepared with the Mirkes Java applet.<sup id="cite_ref-Mirkes2011_29-0" class="reference"><a href="#cite_note-Mirkes2011-29">[29]</a></sup></div>
</div>
</div>
<div class="thumb tright">
<div class="thumbinner" style="width:452px;"><a href="https://en.wikipedia.org/wiki/File:Iris_Flowers_Clustering_kMeans.svg" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/1/10/Iris_Flowers_Clustering_kMeans.svg/450px-Iris_Flowers_Clustering_kMeans.svg.png" width="450" height="211" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/1/10/Iris_Flowers_Clustering_kMeans.svg/675px-Iris_Flowers_Clustering_kMeans.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/10/Iris_Flowers_Clustering_kMeans.svg/900px-Iris_Flowers_Clustering_kMeans.svg.png 2x" data-file-width="660" data-file-height="309" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="https://en.wikipedia.org/wiki/File:Iris_Flowers_Clustering_kMeans.svg" class="internal" title="Enlarge"></a></div>
<i>k</i>-means clustering result for the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" title="Iris flower data set">Iris flower data set</a> and actual species visualized using <a href="https://en.wikipedia.org/wiki/Environment_for_DeveLoping_KDD-Applications_Supported_by_Index-Structures" class="mw-redirect" title="Environment for DeveLoping KDD-Applications Supported by Index-Structures">ELKI</a>. Cluster means are marked using larger, semi-transparent symbols.</div>
</div>
</div>
<div class="thumb tright">
<div class="thumbinner" style="width:452px;"><a href="https://en.wikipedia.org/wiki/File:ClusterAnalysis_Mouse.svg" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/450px-ClusterAnalysis_Mouse.svg.png" width="450" height="182" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/675px-ClusterAnalysis_Mouse.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/0/09/ClusterAnalysis_Mouse.svg/900px-ClusterAnalysis_Mouse.svg.png 2x" data-file-width="1355" data-file-height="547" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="https://en.wikipedia.org/wiki/File:ClusterAnalysis_Mouse.svg" class="internal" title="Enlarge"></a></div>
<i>k</i>-means clustering and <a href="https://en.wikipedia.org/wiki/EM_clustering" class="mw-redirect" title="EM clustering">EM clustering</a> on an artificial dataset ("mouse"). The tendency of <i>k</i>-means to produce equi-sized clusters leads to bad results, while EM benefits from the Gaussian distribution present in the data set
<table class="plainlinks metadata ambox mbox-small-left ambox-style ambox-confusing" role="presentation">
<tr>
<td class="mbox-image"><img alt="" src="http://upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/20px-Edit-clear.svg.png" width="20" height="20" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/30px-Edit-clear.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/40px-Edit-clear.svg.png 2x" data-file-width="48" data-file-height="48" /></td>
<td class="mbox-text"><span class="mbox-text-span">This section <b>may be <a href="https://en.wikipedia.org/wiki/Wikipedia:Vagueness" title="Wikipedia:Vagueness">confusing or unclear</a> to readers</b>. In particular, K-means is by itself EM method. Is it Gaussian mixture?. <small><i>(February 2016)</i></small> <small class="hide-when-compact"><i>(<a href="https://en.wikipedia.org/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></span></td>
</tr>
</table>
</div>
</div>
</div>
<p>Three key features of <i>k</i>-means which make it efficient are often regarded as its biggest drawbacks:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Euclidean_distance" title="Euclidean distance">Euclidean distance</a> is used as a <a href="https://en.wikipedia.org/wiki/Metric_(mathematics)" title="Metric (mathematics)">metric</a> and <a href="https://en.wikipedia.org/wiki/Variance" title="Variance">variance</a> is used as a measure of cluster scatter.</li>
<li>The number of clusters <i>k</i> is an input parameter: an inappropriate choice of <i>k</i> may yield poor results. That is why, when performing k-means, it is important to run diagnostic checks for <a href="https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set" title="Determining the number of clusters in a data set">determining the number of clusters in the data set</a>.</li>
<li>Convergence to a local minimum may produce counterintuitive ("wrong") results (see example in Fig.).</li>
</ul>
<p>A key limitation of <i>k</i>-means is its cluster model. The concept is based on spherical clusters that are separable in a way so that the mean value converges towards the cluster center. The clusters are expected to be of similar size, so that the assignment to the nearest cluster center is the correct assignment. When for example applying <i>k</i>-means with a value of <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>=</mo>
        <mn>3</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k=3}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/662e06a2436f8a44fec791f5c794621f10dc8f30" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.504ex; height:2.176ex;" alt="k=3" /></span> onto the well-known <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set" title="Iris flower data set">Iris flower data set</a>, the result often fails to separate the three <a href="https://en.wikipedia.org/wiki/Iris_(plant)" title="Iris (plant)">Iris</a> species contained in the data set. With <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>=</mo>
        <mn>2</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k=2}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0bd301789e1f25a3da4be297ff637754ebee5f5d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.504ex; height:2.176ex;" alt="k=2" /></span>, the two visible clusters (one containing two species) will be discovered, whereas with <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>=</mo>
        <mn>3</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k=3}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/662e06a2436f8a44fec791f5c794621f10dc8f30" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.504ex; height:2.176ex;" alt="k=3" /></span> one of the two clusters will be split into two even parts. In fact, <span><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>k</mi>
        <mo>=</mo>
        <mn>2</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle k=2}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0bd301789e1f25a3da4be297ff637754ebee5f5d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.504ex; height:2.176ex;" alt="k=2" /></span> is more appropriate for this data set, despite the data set containing 3 <i>classes</i>. As with any other clustering algorithm, the <i>k</i>-means result relies on the data set to satisfy the assumptions made by the clustering algorithms. It works well on some data sets, while failing on others.</p>
<p>The result of <i>k</i>-means can also be seen as the <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" title="Voronoi diagram">Voronoi cells</a> of the cluster means. Since data is split halfway between cluster means, this can lead to suboptimal splits as can be seen in the "mouse" example. The Gaussian models used by the <a href="https://en.wikipedia.org/wiki/Expectation-maximization_algorithm" class="mw-redirect" title="Expectation-maximization algorithm">Expectation-maximization algorithm</a> (which can be seen as a generalization of <i>k</i>-means) are more flexible here by having both variances and covariances. The EM result is thus able to accommodate clusters of variable size much better than <i>k</i>-means as well as correlated clusters (not in this example).</p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=9" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p><i>k</i>-means clustering, in particular when using heuristics such as Lloyd's algorithm, is rather easy to implement and apply even on large data sets. As such, it has been successfully used in various topics, including <a href="https://en.wikipedia.org/wiki/Market_segmentation" title="Market segmentation">market segmentation</a>, <a href="https://en.wikipedia.org/wiki/Computer_vision" title="Computer vision">computer vision</a>, <a href="https://en.wikipedia.org/wiki/Geostatistics" title="Geostatistics">geostatistics</a>,<sup id="cite_ref-30" class="reference"><a href="#cite_note-30">[30]</a></sup> <a href="https://en.wikipedia.org/wiki/Astronomy" title="Astronomy">astronomy</a> and <a href="https://en.wikipedia.org/wiki/Data_Mining_in_Agriculture" class="mw-redirect" title="Data Mining in Agriculture">agriculture</a>. It often is used as a preprocessing step for other algorithms, for example to find a starting configuration.</p>
<h3><span class="mw-headline" id="Vector_quantization">Vector quantization</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=10" title="Edit section: Vector quantization">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote">Main article: <a href="https://en.wikipedia.org/wiki/Vector_quantization" title="Vector quantization">Vector quantization</a></div>
<div class="thumb tright">
<div class="thumbinner" style="width:202px;"><a href="https://en.wikipedia.org/wiki/File:Rosa_Gold_Glow_2_small_noblue.png" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/8/82/Rosa_Gold_Glow_2_small_noblue.png" width="200" height="196" class="thumbimage" data-file-width="200" data-file-height="196" /></a>
<div class="thumbcaption">Two-channel (for illustration purposes -- red and green only) color image.</div>
</div>
</div>
<div class="thumb tright">
<div class="thumbinner" style="width:252px;"><a href="https://en.wikipedia.org/wiki/File:Rosa_Gold_Glow_2_small_noblue_color_space.png" class="image"><img alt="" src="http://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Rosa_Gold_Glow_2_small_noblue_color_space.png/250px-Rosa_Gold_Glow_2_small_noblue_color_space.png" width="250" height="247" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Rosa_Gold_Glow_2_small_noblue_color_space.png/375px-Rosa_Gold_Glow_2_small_noblue_color_space.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Rosa_Gold_Glow_2_small_noblue_color_space.png/500px-Rosa_Gold_Glow_2_small_noblue_color_space.png 2x" data-file-width="803" data-file-height="792" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="https://en.wikipedia.org/wiki/File:Rosa_Gold_Glow_2_small_noblue_color_space.png" class="internal" title="Enlarge"></a></div>
Vector quantization of colors present in the image above into Voronoi cells using <i>k</i>-means.</div>
</div>
</div>
<p><i>k</i>-means originates from signal processing, and still finds use in this domain. For example, in <a href="https://en.wikipedia.org/wiki/Computer_graphics" title="Computer graphics">computer graphics</a>, <a href="https://en.wikipedia.org/wiki/Color_quantization" title="Color quantization">color quantization</a> is the task of reducing the <a href="https://en.wikipedia.org/wiki/Color_palette" class="mw-redirect" title="Color palette">color palette</a> of an image to a fixed number of colors <i>k</i>. The <i>k</i>-means algorithm can easily be used for this task and produces competitive results. A use case for this approach is <a href="https://en.wikipedia.org/wiki/Image_segmentation" title="Image segmentation">image segmentation</a>. Other uses of vector quantization include <a href="https://en.wikipedia.org/wiki/Sampling_(statistics)" title="Sampling (statistics)">non-random sampling</a>, as <i>k</i>-means can easily be used to choose <i>k</i> different but prototypical objects from a large data set for further analysis.</p>
<h3><span class="mw-headline" id="Cluster_analysis">Cluster analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=11" title="Edit section: Cluster analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote">Main article: <a href="https://en.wikipedia.org/wiki/Cluster_analysis" title="Cluster analysis">Cluster analysis</a></div>
<p>In cluster analysis, the <i>k</i>-means algorithm can be used to partition the input data set into <i>k</i> partitions (clusters).</p>
<p>However, the pure <i>k</i>-means algorithm is not very flexible, and as such is of limited use (except for when vector quantization as above is actually the desired use case!). In particular, the parameter <i>k</i> is known to be hard to choose (as discussed above) when not given by external constraints. Another limitation of the algorithm is that it cannot be used with arbitrary distance functions or on non-numerical data. For these use cases, many other algorithms have been developed since.</p>
<h3><span class="mw-headline" id="Feature_learning">Feature learning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=12" title="Edit section: Feature learning">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><i>k</i>-means clustering has been used as a <a href="https://en.wikipedia.org/wiki/Feature_learning" title="Feature learning">feature learning</a> (or <a href="https://en.wikipedia.org/wiki/Dictionary_learning" class="mw-redirect" title="Dictionary learning">dictionary learning</a>) step, in either (<a href="https://en.wikipedia.org/wiki/Semi-supervised_learning" title="Semi-supervised learning">semi-</a>)<a href="https://en.wikipedia.org/wiki/Supervised_learning" title="Supervised learning">supervised learning</a> or <a href="https://en.wikipedia.org/wiki/Unsupervised_learning" title="Unsupervised learning">unsupervised learning</a>.<sup id="cite_ref-Coates2012_31-0" class="reference"><a href="#cite_note-Coates2012-31">[31]</a></sup> The basic approach is first to train a <i>k</i>-means clustering representation, using the input training data (which need not be labelled). Then, to project any input datum into the new feature space, we have a choice of "encoding" functions, but we can use for example the thresholded matrix-product of the datum with the centroid locations, the distance from the datum to each centroid, or simply an indicator function for the nearest centroid,<sup id="cite_ref-Coates2012_31-1" class="reference"><a href="#cite_note-Coates2012-31">[31]</a></sup><sup id="cite_ref-32" class="reference"><a href="#cite_note-32">[32]</a></sup> or some smooth transformation of the distance.<sup id="cite_ref-coates2011_33-0" class="reference"><a href="#cite_note-coates2011-33">[33]</a></sup> Alternatively, by transforming the sample-cluster distance through a <a href="https://en.wikipedia.org/wiki/Radial_basis_function" title="Radial basis function">Gaussian RBF</a>, one effectively obtains the hidden layer of a <a href="https://en.wikipedia.org/wiki/Radial_basis_function_network" title="Radial basis function network">radial basis function network</a>.<sup id="cite_ref-schwenker_34-0" class="reference"><a href="#cite_note-schwenker-34">[34]</a></sup></p>
<p>This use of <i>k</i>-means has been successfully combined with simple, <a href="https://en.wikipedia.org/wiki/Linear_classifier" title="Linear classifier">linear classifiers</a> for semi-supervised learning in <a href="https://en.wikipedia.org/wiki/Natural_language_processing" title="Natural language processing">NLP</a> (specifically for <a href="https://en.wikipedia.org/wiki/Named_entity_recognition" class="mw-redirect" title="Named entity recognition">named entity recognition</a>)<sup id="cite_ref-35" class="reference"><a href="#cite_note-35">[35]</a></sup> and in <a href="https://en.wikipedia.org/wiki/Computer_vision" title="Computer vision">computer vision</a>. On an object recognition task, it was found to exhibit comparable performance with more sophisticated feature learning approaches such as <a href="https://en.wikipedia.org/wiki/Autoencoder" title="Autoencoder">autoencoders</a> and <a href="https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">restricted Boltzmann machines</a>.<sup id="cite_ref-coates2011_33-1" class="reference"><a href="#cite_note-coates2011-33">[33]</a></sup> However, it generally requires more data than the sophisticated methods, for equivalent performance, because each data point only contributes to one "feature" rather than multiple.<sup id="cite_ref-Coates2012_31-2" class="reference"><a href="#cite_note-Coates2012-31">[31]</a></sup></p>
<h2><span class="mw-headline" id="Relation_to_other_statistical_machine_learning_algorithms">Relation to other statistical machine learning algorithms</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=13" title="Edit section: Relation to other statistical machine learning algorithms">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Gaussian_Mixture_Model">Gaussian Mixture Model</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=14" title="Edit section: Gaussian Mixture Model">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><i>k</i>-means clustering, and its associated <a href="https://en.wikipedia.org/wiki/Expectation–maximization_algorithm" title="Expectation–maximization algorithm">expectation-maximization algorithm</a>, is a special case of a <a href="https://en.wikipedia.org/wiki/Mixture_model" title="Mixture model">Gaussian mixture model</a>, specifically, the limit of taking all covariances as diagonal, equal, and small. It is often easy to generalize a <i>k</i>-means problem into a Gaussian mixture model.<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">[36]</a></sup> Another generalization of the k-means algorithm is the <a href="https://en.wikipedia.org/wiki/K-SVD" title="K-SVD">K-SVD</a> algorithm, which estimates data points as a sparse linear combination of "codebook vectors". K-means corresponds to the special case of using a single codebook vector, with a weight of 1.<sup id="cite_ref-37" class="reference"><a href="#cite_note-37">[37]</a></sup></p>
<h3><span class="mw-headline" id="Principal_component_analysis_.28PCA.29">Principal component analysis (PCA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=15" title="Edit section: Principal component analysis (PCA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>It was proved <sup id="cite_ref-38" class="reference"><a href="#cite_note-38">[38]</a></sup> <sup id="cite_ref-39" class="reference"><a href="#cite_note-39">[39]</a></sup> that the relaxed solution of <span class="texhtml"><var>k</var></span>-means clustering, specified by the cluster indicators, is given by <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" title="Principal component analysis">principal component analysis</a> (PCA), and the PCA subspace spanned by the principal directions is identical to the cluster centroid subspace. The intuition is that k-means describe spherically shaped (ball-like) clusters. If the data have 2 clusters, the line connecting the two centroids is the best 1-dimensional projection direction, which is also the 1st PCA direction. Cutting the line at the center of mass separate the clusters (this is the continuous relaxation of the discreet cluster indicator). If the data have 3 clusters, the 2-dimensional plane spanned by 3 cluster centroids is the best 2-D projection. This plane is also the first 2 PCA dimensions. Well-separated clusters are effectively modeled by ball-shape clusters and thus discovered by K-means. Non-ball-shaped clusters are hard to separate when they are close-by. For example, two half-moon shaped clusters intertwined in space does not separate well when projected to PCA subspace. But neither is k-means supposed to do well on this data. However, that PCA is a useful relaxation of k-means clustering was not a new result,<sup id="cite_ref-40" class="reference"><a href="#cite_note-40">[40]</a></sup> and it is straightforward to uncover counterexamples to the statement that the cluster centroid subspace is spanned by the principal directions.<sup id="cite_ref-41" class="reference"><a href="#cite_note-41">[41]</a></sup></p>
<h3><span class="mw-headline" id="Mean_shift_clustering">Mean shift clustering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=16" title="Edit section: Mean shift clustering">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Basic <a href="https://en.wikipedia.org/wiki/Mean_shift" title="Mean shift">mean shift</a> clustering algorithms maintain a set of data points the same size as the input data set. Initially, this set is copied from the input set. Then this set is iteratively replaced by the mean of those points in the set that are within a given distance of that point. By contrast, <i>k</i>-means restricts this updated set to <i>k</i> points usually much less than the number of points in the input data set, and replaces each point in this set by the mean of all points in the <i>input set</i> that are closer to that point than any other (e.g. within the Voronoi partition of each updating point). A mean shift algorithm that is similar then to <i>k</i>-means, called <i>likelihood mean shift</i>, replaces the set of points undergoing replacement by the mean of all points in the input set that are within a given distance of the changing set.<sup id="cite_ref-Little2011_42-0" class="reference"><a href="#cite_note-Little2011-42">[42]</a></sup> One of the advantages of mean shift over <i>k</i>-means is that there is no need to choose the number of clusters, because mean shift is likely to find only a few clusters if indeed only a small number exist. However, mean shift can be much slower than <i>k</i>-means, and still requires selection of a bandwidth parameter. Mean shift has soft variants much as <i>k</i>-means does.</p>
<h3><span class="mw-headline" id="Independent_component_analysis_.28ICA.29">Independent component analysis (ICA)</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=17" title="Edit section: Independent component analysis (ICA)">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>It has been shown in <sup id="cite_ref-43" class="reference"><a href="#cite_note-43">[43]</a></sup> that under sparsity assumptions and when input data is pre-processed with the <a href="https://en.wikipedia.org/wiki/Whitening_transformation" title="Whitening transformation">whitening transformation</a> <i>k</i>-means produces the solution to the linear <a href="https://en.wikipedia.org/wiki/Independent_component_analysis" title="Independent component analysis">Independent component analysis</a> task. This aids in explaining the successful application of <i>k</i>-means to <a href="K-means_clustering.html#Feature_learning" title="K-means clustering">feature learning</a>.</p>
<h3><span class="mw-headline" id="Bilateral_filtering">Bilateral filtering</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=18" title="Edit section: Bilateral filtering">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p><i>k</i>-means implicitly assumes that the ordering of the input data set does not matter. The <a href="https://en.wikipedia.org/wiki/Bilateral_filter" title="Bilateral filter">bilateral filter</a> is similar to K-means and <a href="https://en.wikipedia.org/wiki/Mean_shift" title="Mean shift">mean shift</a> in that it maintains a set of data points that are iteratively replaced by means. However, the bilateral filter restricts the calculation of the (kernel weighted) mean to include only points that are close in the ordering of the input data.<sup id="cite_ref-Little2011_42-1" class="reference"><a href="#cite_note-Little2011-42">[42]</a></sup> This makes it applicable to problems such as image denoising, where the spatial arrangement of pixels in an image is of critical importance.</p>
<h2><span class="mw-headline" id="Similar_problems">Similar problems</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=19" title="Edit section: Similar problems">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>The set of squared error minimizing cluster functions also includes the <a href="https://en.wikipedia.org/wiki/K-medoids" title="K-medoids"><span class="texhtml"><var>k</var></span>-medoids</a> algorithm, an approach which forces the center point of each cluster to be one of the actual points, i.e., it uses <a href="https://en.wikipedia.org/wiki/Medoids" class="mw-redirect" title="Medoids">medoids</a> in place of <a href="https://en.wikipedia.org/wiki/Centroids" class="mw-redirect" title="Centroids">centroids</a>.</p>
<h2><span class="mw-headline" id="Software_implementations">Software implementations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=20" title="Edit section: Software implementations">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Different implementations of the same algorithm were found to exhibit enormous performance differences, with the fastest on a test data set finishing in 10 seconds, the slowest taking 25988 seconds.<sup id="cite_ref-44" class="reference"><a href="#cite_note-44">[44]</a></sup> The differences can be attributed to implementation quality, language and compiler differences, and the use of indexes for acceleration.</p>
<h3><span class="mw-headline" id="Free_Software.2FOpen_Source">Free Software/Open Source</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=21" title="Edit section: Free Software/Open Source">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>the following implementations are available under <a href="https://en.wikipedia.org/wiki/Free_and_open-source_software" title="Free and open-source software">Free/Open Source Software</a> licenses, with publicly available source code.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Accord.NET" title="Accord.NET">Accord.NET</a> contains C# implementations for <i>k</i>-means, <i>k</i>-means++ and <i>k</i>-modes.</li>
<li><a href="https://en.wikipedia.org/wiki/CrimeStat" title="CrimeStat">CrimeStat</a> implements two spatial <i>k</i>-means algorithms, one of which allows the user to define the starting locations.</li>
<li><a href="https://en.wikipedia.org/wiki/ELKI" title="ELKI">ELKI</a> contains <i>k</i>-means (with Lloyd and MacQueen iteration, along with different initializations such as <i>k</i>-means++ initialization) and various more advanced clustering algorithms.</li>
<li><a href="https://en.wikipedia.org/wiki/Julia_language" class="mw-redirect" title="Julia language">Julia</a> contains a <i>k</i>-means implementation in the JuliaStats Clustering package.</li>
<li><a href="https://en.wikipedia.org/wiki/Apache_Mahout" title="Apache Mahout">Mahout</a> contains a <a href="https://en.wikipedia.org/wiki/MapReduce" title="MapReduce">MapReduce</a> based <i>k</i>-means.</li>
<li><a href="https://en.wikipedia.org/wiki/MLPACK_(C++_library)" title="MLPACK (C++ library)">MLPACK</a> contains a C++ implementation of <i>k</i>-means.</li>
<li><a href="https://en.wikipedia.org/wiki/GNU_Octave" title="GNU Octave">Octave</a> contains <i>k</i>-means.</li>
<li><a href="https://en.wikipedia.org/wiki/OpenCV" title="OpenCV">OpenCV</a> contains a <i>k</i>-means implementation.</li>
<li><a href="https://en.wikipedia.org/wiki/PSPP" title="PSPP">PSPP</a> contains <i>k</i>-means, The QUICK CLUSTER command performs k-means clustering on the dataset.</li>
<li><a href="https://en.wikipedia.org/wiki/R_(programming_language)" title="R (programming language)">R</a> contains three <i>k</i>-means variations.</li>
<li><a href="https://en.wikipedia.org/wiki/SciPy" title="SciPy">SciPy</a> and <a href="https://en.wikipedia.org/wiki/Scikit-learn" title="Scikit-learn">scikit-learn</a> contain multiple <i>k</i>-means implementations.</li>
<li><a href="https://en.wikipedia.org/wiki/Apache_Spark" title="Apache Spark">Spark</a> MLlib implements a distributed <i>k</i>-means algorithm.</li>
<li><a href="https://en.wikipedia.org/wiki/Torch_(machine_learning)" title="Torch (machine learning)">Torch</a> contains an <i>unsup</i> package that provides <i>k</i>-means clustering.</li>
<li><a href="https://en.wikipedia.org/wiki/Weka_(machine_learning)" title="Weka (machine learning)">Weka</a> contains <i>k</i>-means and <i>x</i>-means.</li>
</ul>
<h3><span class="mw-headline" id="Proprietary">Proprietary</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=22" title="Edit section: Proprietary">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The following implementations are available under <a href="https://en.wikipedia.org/wiki/Proprietary_software" title="Proprietary software">proprietary</a> license terms, and may not have publicly available source code.</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Ayasdi" title="Ayasdi">Ayasdi</a></li>
<li><a href="https://en.wikipedia.org/wiki/MATLAB" title="MATLAB">MATLAB</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mathematica" class="mw-redirect" title="Mathematica">Mathematica</a></li>
<li><a href="https://en.wikipedia.org/wiki/RapidMiner" title="RapidMiner">RapidMiner</a></li>
<li><a href="https://en.wikipedia.org/wiki/SAP_HANA" title="SAP HANA">SAP HANA</a></li>
<li><a href="https://en.wikipedia.org/wiki/SAS_System" class="mw-redirect" title="SAS System">SAS</a></li>
<li><a href="https://en.wikipedia.org/wiki/SPSS" title="SPSS">SPSS</a></li>
<li><a href="https://en.wikipedia.org/wiki/Stata" title="Stata">Stata</a></li>
</ul>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=23" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/K-means++" title="K-means++">K-means++</a></li>
<li><a href="https://en.wikipedia.org/wiki/Centroidal_Voronoi_tessellation" title="Centroidal Voronoi tessellation">Centroidal Voronoi tessellation</a></li>
<li><a href="https://en.wikipedia.org/wiki/K_q-flats" title="K q-flats">k q-flats</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linde–Buzo–Gray_algorithm" title="Linde–Buzo–Gray algorithm">Linde–Buzo–Gray algorithm</a></li>
<li><a href="https://en.wikipedia.org/wiki/Self-organizing_map" title="Self-organizing map">Self-organizing map</a></li>
<li><a href="https://en.wikipedia.org/wiki/Head/tail_Breaks" title="Head/tail Breaks">Head/tail Breaks</a></li>
</ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit&amp;section=24" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-count references-column-count-2" style="-moz-column-count: 2; -webkit-column-count: 2; column-count: 2; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-macqueen1967-1"><span class="mw-cite-backlink"><b><a href="#cite_ref-macqueen1967_1-0">^</a></b></span> <span class="reference-text"><cite class="citation conference">MacQueen, J. B. (1967). <a rel="nofollow" class="external text" href="http://projecteuclid.org/euclid.bsmsp/1200512992"><i>Some Methods for classification and Analysis of Multivariate Observations</i></a>. Proceedings of 5th Berkeley Symposium on Mathematical Statistics and Probability. <b>1</b>. University of California Press. pp.&#160;281–297. <a href="https://en.wikipedia.org/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a>&#160;<a rel="nofollow" class="external text" href="http://www.ams.org/mathscinet-getitem?mr=0214227">0214227</a>. <a href="https://en.wikipedia.org/wiki/Zentralblatt_MATH" title="Zentralblatt MATH">Zbl</a>&#160;<a rel="nofollow" class="external text" href="http://zbmath.org/?format=complete&amp;q=an:0214.46201">0214.46201</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2009-04-07</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.aufirst=J.+B.&amp;rft.aulast=MacQueen&amp;rft.btitle=Some+Methods+for+classification+and+Analysis+of+Multivariate+Observations&amp;rft.date=1967&amp;rft.genre=conference&amp;rft_id=%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3D0214227&amp;rft_id=%2F%2Fzbmath.org%2F%3Fformat%3Dcomplete%26q%3Dan%3A0214.46201&amp;rft_id=http%3A%2F%2Fprojecteuclid.org%2Feuclid.bsmsp%2F1200512992&amp;rft.pages=281-297&amp;rft.pub=University+of+California+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-2"><span class="mw-cite-backlink"><b><a href="#cite_ref-2">^</a></b></span> <span class="reference-text"><cite class="citation journal"><a href="https://en.wikipedia.org/wiki/Hugo_Steinhaus" title="Hugo Steinhaus">Steinhaus, H.</a> (1957). "Sur la division des corps matériels en parties". <i>Bull. Acad. Polon. Sci.</i> (in French). <b>4</b> (12): 801–804. <a href="https://en.wikipedia.org/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a>&#160;<a rel="nofollow" class="external text" href="http://www.ams.org/mathscinet-getitem?mr=0090073">0090073</a>. <a href="https://en.wikipedia.org/wiki/Zentralblatt_MATH" title="Zentralblatt MATH">Zbl</a>&#160;<a rel="nofollow" class="external text" href="http://zbmath.org/?format=complete&amp;q=an:0079.16403">0079.16403</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Sur+la+division+des+corps+mat%C3%A9riels+en+parties&amp;rft.aufirst=H.&amp;rft.aulast=Steinhaus&amp;rft.date=1957&amp;rft.genre=article&amp;rft_id=%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3D0090073&amp;rft_id=%2F%2Fzbmath.org%2F%3Fformat%3Dcomplete%26q%3Dan%3A0079.16403&amp;rft.issue=12&amp;rft.jtitle=Bull.+Acad.+Polon.+Sci.&amp;rft.pages=801-804&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=4" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-lloyd1957-3"><span class="mw-cite-backlink"><b><a href="#cite_ref-lloyd1957_3-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Lloyd, S. P. (1957). "Least square quantization in PCM". <i>Bell Telephone Laboratories Paper</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Least+square+quantization+in+PCM&amp;rft.aufirst=S.+P.&amp;rft.aulast=Lloyd&amp;rft.date=1957&amp;rft.genre=article&amp;rft.jtitle=Bell+Telephone+Laboratories+Paper&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span> Published in journal much later: <cite class="citation journal">Lloyd., S. P. (1982). <a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~roweis/csc2515-2006/readings/lloyd57.pdf">"Least squares quantization in PCM"</a> <span style="font-size:85%;">(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/IEEE_Transactions_on_Information_Theory" title="IEEE Transactions on Information Theory">IEEE Transactions on Information Theory</a></i>. <b>28</b> (2): 129–137. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109%2FTIT.1982.1056489">10.1109/TIT.1982.1056489</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2009-04-15</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Least+squares+quantization+in+PCM&amp;rft.aufirst=S.+P.&amp;rft.aulast=Lloyd.&amp;rft.date=1982&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~roweis%2Fcsc2515-2006%2Freadings%2Flloyd57.pdf&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.1982.1056489&amp;rft.issue=2&amp;rft.jtitle=IEEE+Transactions+on+Information+Theory&amp;rft.pages=129-137&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=28" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-forgy65-4"><span class="mw-cite-backlink"><b><a href="#cite_ref-forgy65_4-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">E.W. Forgy (1965). "Cluster analysis of multivariate data: efficiency versus interpretability of classifications". <i>Biometrics</i>. <b>21</b>: 768–769. <a href="https://en.wikipedia.org/wiki/JSTOR" title="JSTOR">JSTOR</a>&#160;<a rel="nofollow" class="external text" href="http://www.jstor.org/stable/2528559">2528559</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Cluster+analysis+of+multivariate+data%3A+efficiency+versus+interpretability+of+classifications&amp;rft.au=E.W.+Forgy&amp;rft.date=1965&amp;rft.genre=article&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2528559&amp;rft.jtitle=Biometrics&amp;rft.pages=768-769&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=21" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-hartigan1975-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-hartigan1975_5-0">^</a></b></span> <span class="reference-text"><cite class="citation book">J.A. Hartigan (1975). <i>Clustering algorithms</i>. John Wiley &amp; Sons, Inc.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.au=J.A.+Hartigan&amp;rft.btitle=Clustering+algorithms&amp;rft.date=1975&amp;rft.genre=book&amp;rft.pub=John+Wiley+%26+Sons%2C+Inc.&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-hartigan1979-6"><span class="mw-cite-backlink">^ <a href="#cite_ref-hartigan1979_6-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-hartigan1979_6-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Hartigan, J. A.; Wong, M. A. (1979). "Algorithm AS 136: A K-Means Clustering Algorithm". <i><a href="https://en.wikipedia.org/wiki/Journal_of_the_Royal_Statistical_Society,_Series_C" class="mw-redirect" title="Journal of the Royal Statistical Society, Series C">Journal of the Royal Statistical Society, Series C</a></i>. <b>28</b> (1): 100–108. <a href="https://en.wikipedia.org/wiki/JSTOR" title="JSTOR">JSTOR</a>&#160;<a rel="nofollow" class="external text" href="http://www.jstor.org/stable/2346830">2346830</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Algorithm+AS+136%3A+A+K-Means+Clustering+Algorithm&amp;rft.aufirst=J.+A.&amp;rft.aulast=Hartigan&amp;rft.au=Wong%2C+M.+A.&amp;rft.date=1979&amp;rft.genre=article&amp;rft_id=%2F%2Fwww.jstor.org%2Fstable%2F2346830&amp;rft.issue=1&amp;rft.jtitle=Journal+of+the+Royal+Statistical+Society%2C+Series+C&amp;rft.pages=100-108&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=28" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-7">^</a></b></span> <span class="reference-text"><cite id="mackay2003" class="citation book"><a href="https://en.wikipedia.org/wiki/David_MacKay_(scientist)" class="mw-redirect" title="David MacKay (scientist)">MacKay, David</a> (2003). <a rel="nofollow" class="external text" href="http://www.inference.phy.cam.ac.uk/mackay/itprnn/ps/284.292.pdf">"Chapter 20. An Example Inference Task: Clustering"</a> <span style="font-size:85%;">(PDF)</span>. <a rel="nofollow" class="external text" href="http://www.inference.phy.cam.ac.uk/mackay/itila/book.html"><i>Information Theory, Inference and Learning Algorithms</i></a>. Cambridge University Press. pp.&#160;284–292. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="https://en.wikipedia.org/wiki/Special:BookSources/0-521-64298-1" title="Special:BookSources/0-521-64298-1">0-521-64298-1</a>. <a href="https://en.wikipedia.org/wiki/Mathematical_Reviews" title="Mathematical Reviews">MR</a>&#160;<a rel="nofollow" class="external text" href="http://www.ams.org/mathscinet-getitem?mr=2012999">2012999</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Chapter+20.+An+Example+Inference+Task%3A+Clustering&amp;rft.aufirst=David&amp;rft.aulast=MacKay&amp;rft.btitle=Information+Theory%2C+Inference+and+Learning+Algorithms&amp;rft.date=2003&amp;rft.genre=bookitem&amp;rft_id=%2F%2Fwww.ams.org%2Fmathscinet-getitem%3Fmr%3D2012999&amp;rft_id=http%3A%2F%2Fwww.inference.phy.cam.ac.uk%2Fmackay%2Fitprnn%2Fps%2F284.292.pdf&amp;rft.isbn=0-521-64298-1&amp;rft.pages=284-292&amp;rft.pub=Cambridge+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text">Since the square root is a monotone function, this also is the minimum Euclidean distance assignment.</span></li>
<li id="cite_note-hamerly-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-hamerly_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-hamerly_9-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Hamerly, G.; Elkan, C. (2002). <a rel="nofollow" class="external text" href="http://people.csail.mit.edu/tieu/notebook/kmeans/15_p600-hamerly.pdf">"Alternatives to the k-means algorithm that find better clusterings"</a> <span style="font-size:85%;">(PDF)</span>. <i>Proceedings of the eleventh international conference on Information and knowledge management (CIKM)</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Alternatives+to+the+k-means+algorithm+that+find+better+clusterings&amp;rft.au=Elkan%2C+C.&amp;rft.au=Hamerly%2C+G.&amp;rft.btitle=Proceedings+of+the+eleventh+international+conference+on+Information+and+knowledge+management+%28CIKM%29&amp;rft.date=2002&amp;rft.genre=conference&amp;rft_id=http%3A%2F%2Fpeople.csail.mit.edu%2Ftieu%2Fnotebook%2Fkmeans%2F15_p600-hamerly.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation journal">Vattani., A. (2011). <a rel="nofollow" class="external text" href="http://cseweb.ucsd.edu/users/avattani/papers/kmeans-journal.pdf">"k-means requires exponentially many iterations even in the plane"</a> <span style="font-size:85%;">(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Discrete_and_Computational_Geometry" title="Discrete and Computational Geometry">Discrete and Computational Geometry</a></i>. <b>45</b> (4): 596–616. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2Fs00454-011-9340-1">10.1007/s00454-011-9340-1</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=k-means+requires+exponentially+many+iterations+even+in+the+plane&amp;rft.aufirst=A.&amp;rft.aulast=Vattani.&amp;rft.date=2011&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fcseweb.ucsd.edu%2Fusers%2Favattani%2Fpapers%2Fkmeans-journal.pdf&amp;rft_id=info%3Adoi%2F10.1007%2Fs00454-011-9340-1&amp;rft.issue=4&amp;rft.jtitle=Discrete+and+Computational+Geometry&amp;rft.pages=596-616&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=45" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Arthur.2C_D..3B_Manthey.2C_B..3B_Roeglin.2C_H._2009-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-Arthur.2C_D..3B_Manthey.2C_B..3B_Roeglin.2C_H._2009_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Arthur.2C_D..3B_Manthey.2C_B..3B_Roeglin.2C_H._2009_11-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Arthur, D.; Manthey, B.; Roeglin, H. (2009). "k-means has polynomial smoothed complexity". <i>Proceedings of the 50th Symposium on Foundations of Computer Science (FOCS)</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=k-means+has+polynomial+smoothed+complexity&amp;rft.au=Arthur%2C+D.&amp;rft.au=Manthey%2C+B.&amp;rft.au=Roeglin%2C+H.&amp;rft.btitle=Proceedings+of+the+50th+Symposium+on+Foundations+of+Computer+Science+%28FOCS%29&amp;rft.date=2009&amp;rft.genre=conference&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-12"><span class="mw-cite-backlink"><b><a href="#cite_ref-12">^</a></b></span> <span class="reference-text"><cite class="citation journal">Aloise, D.; Deshpande, A.; Hansen, P.; Popat, P. (2009). "NP-hardness of Euclidean sum-of-squares clustering". <i><a href="https://en.wikipedia.org/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">Machine Learning</a></i>. <b>75</b>: 245–249. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2Fs10994-009-5103-0">10.1007/s10994-009-5103-0</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=NP-hardness+of+Euclidean+sum-of-squares+clustering&amp;rft.au=Aloise%2C+D.&amp;rft.au=Deshpande%2C+A.&amp;rft.au=Hansen%2C+P.&amp;rft.au=Popat%2C+P.&amp;rft.date=2009&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2Fs10994-009-5103-0&amp;rft.jtitle=Machine+Learning&amp;rft.pages=245-249&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=75" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal">Dasgupta, S.; Freund, Y. (July 2009). "Random Projection Trees for Vector Quantization". <i>Information Theory, IEEE Transactions on</i>. <b>55</b>: 3229–3242. <a href="https://en.wikipedia.org/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="http://arxiv.org/abs/0805.1390">0805.1390</a><span style="margin-left:0.1em"><img alt="Freely accessible" src="http://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109%2FTIT.2009.2021326">10.1109/TIT.2009.2021326</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Random+Projection+Trees+for+Vector+Quantization&amp;rft.au=Dasgupta%2C+S.&amp;rft.au=Freund%2C+Y.&amp;rft.date=2009-07&amp;rft.genre=article&amp;rft_id=info%3Aarxiv%2F0805.1390&amp;rft_id=info%3Adoi%2F10.1109%2FTIT.2009.2021326&amp;rft.jtitle=Information+Theory%2C+IEEE+Transactions+on&amp;rft.pages=3229-3242&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=55" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation journal">Mahajan, M.; Nimbhorkar, P.; Varadarajan, K. (2009). "The Planar k-Means Problem is NP-Hard". <i><a href="https://en.wikipedia.org/wiki/Lecture_Notes_in_Computer_Science" title="Lecture Notes in Computer Science">Lecture Notes in Computer Science</a></i>. <b>5431</b>: 274–285. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2F978-3-642-00202-1_24">10.1007/978-3-642-00202-1_24</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=The+Planar+k-Means+Problem+is+NP-Hard&amp;rft.au=Mahajan%2C+M.&amp;rft.au=Nimbhorkar%2C+P.&amp;rft.au=Varadarajan%2C+K.&amp;rft.date=2009&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-00202-1_24&amp;rft.jtitle=Lecture+Notes+in+Computer+Science&amp;rft.pages=274-285&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=5431" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-15">^</a></b></span> <span class="reference-text"><cite class="citation conference">Inaba, M.; Katoh, N.; Imai, H. (1994). <i>Applications of weighted Voronoi diagrams and randomization to variance-based</i> k<i>-clustering</i>. <a href="https://en.wikipedia.org/wiki/Symposium_on_Computational_Geometry" title="Symposium on Computational Geometry">Proceedings of 10th ACM Symposium on Computational Geometry</a>. pp.&#160;332–339. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1145%2F177424.178042">10.1145/177424.178042</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.au=Imai%2C+H.&amp;rft.au=Inaba%2C+M.&amp;rft.au=Katoh%2C+N.&amp;rft.btitle=Applications+of+weighted+Voronoi+diagrams+and+randomization+to+variance-based+k-clustering&amp;rft.date=1994&amp;rft.genre=conference&amp;rft_id=info%3Adoi%2F10.1145%2F177424.178042&amp;rft.pages=332-339&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation thesis">Arthur; Abhishek Bhowmick (2009). <a rel="nofollow" class="external text" href="https://gautam5.cse.iitk.ac.in/opencs/sites/default/files/final.pdf"><i>A theoretical analysis of Lloyd's algorithm for k-means clustering</i></a> <span style="font-size:85%;">(PDF)</span> (Thesis).</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.au=Abhishek+Bhowmick&amp;rft.au=Arthur&amp;rft.date=2009&amp;rft_id=https%3A%2F%2Fgautam5.cse.iitk.ac.in%2Fopencs%2Fsites%2Fdefault%2Ffiles%2Ffinal.pdf&amp;rft.title=A+theoretical+analysis+of+Lloyd%27s+algorithm+for+k-means+clustering&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-phillips-17"><span class="mw-cite-backlink">^ <a href="#cite_ref-phillips_17-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-phillips_17-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation book">Phillips, Steven J. (2002-01-04). Mount, David M.; Stein, Clifford, eds. <a rel="nofollow" class="external text" href="http://link.springer.com/chapter/10.1007/3-540-45643-0_13"><i>Acceleration of K-Means and Related Clustering Algorithms</i></a>. Lecture Notes in Computer Science. Springer Berlin Heidelberg. pp.&#160;166–177. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2F3-540-45643-0_13">10.1007/3-540-45643-0_13</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-3-540-43977-6" title="Special:BookSources/978-3-540-43977-6">978-3-540-43977-6</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.aufirst=Steven+J.&amp;rft.aulast=Phillips&amp;rft.btitle=Acceleration+of+K-Means+and+Related+Clustering+Algorithms&amp;rft.date=2002-01-04&amp;rft.genre=book&amp;rft_id=http%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F3-540-45643-0_13&amp;rft_id=info%3Adoi%2F10.1007%2F3-540-45643-0_13&amp;rft.isbn=978-3-540-43977-6&amp;rft.pages=166-177&amp;rft.pub=Springer+Berlin+Heidelberg&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-elkan-18"><span class="mw-cite-backlink">^ <a href="#cite_ref-elkan_18-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-elkan_18-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Elkan, C. (2003). <a rel="nofollow" class="external text" href="http://www-cse.ucsd.edu/~elkan/kmeansicml03.pdf">"Using the triangle inequality to accelerate k-means"</a> <span style="font-size:85%;">(PDF)</span>. <i>Proceedings of the Twentieth International Conference on Machine Learning (ICML)</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Using+the+triangle+inequality+to+accelerate+k-means&amp;rft.au=Elkan%2C+C.&amp;rft.btitle=Proceedings+of+the+Twentieth+International+Conference+on+Machine+Learning+%28ICML%29&amp;rft.date=2003&amp;rft.genre=conference&amp;rft_id=http%3A%2F%2Fwww-cse.ucsd.edu%2F~elkan%2Fkmeansicml03.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-hamerly2-19"><span class="mw-cite-backlink">^ <a href="#cite_ref-hamerly2_19-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-hamerly2_19-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web">Hamerly, Greg. <a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.3017&amp;rep=rep1&amp;type=pdf">"Making k-means even faster"</a>. <i>citeseerx.ist.psu.edu</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2015-12-10</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Making+k-means+even+faster&amp;rft.aufirst=Greg&amp;rft.aulast=Hamerly&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fdownload%3Fdoi%3D10.1.1.187.3017%26rep%3Drep1%26type%3Dpdf&amp;rft.jtitle=citeseerx.ist.psu.edu&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text"><cite class="citation journal">Kanungo, T.; <a href="https://en.wikipedia.org/wiki/David_Mount" title="David Mount">Mount, D. M.</a>; <a href="https://en.wikipedia.org/wiki/Nathan_Netanyahu" title="Nathan Netanyahu">Netanyahu, N. S.</a>; Piatko, C. D.; Silverman, R.; Wu, A. Y. (2002). <a rel="nofollow" class="external text" href="http://www.cs.umd.edu/~mount/Papers/pami02.pdf">"An efficient k-means clustering algorithm: Analysis and implementation"</a> <span style="font-size:85%;">(PDF)</span>. <i>IEEE Trans. Pattern Analysis and Machine Intelligence</i>. <b>24</b>: 881–892. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1109%2FTPAMI.2002.1017616">10.1109/TPAMI.2002.1017616</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2009-04-24</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=An+efficient+k-means+clustering+algorithm%3A+Analysis+and+implementation&amp;rft.au=Kanungo%2C+T.%3B+Mount%2C+D.+M.%3B+Netanyahu%2C+N.+S.%3B+Piatko%2C+C.+D.%3B+Silverman%2C+R.%3B+Wu%2C+A.+Y.&amp;rft.date=2002&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.cs.umd.edu%2F~mount%2FPapers%2Fpami02.pdf&amp;rft_id=info%3Adoi%2F10.1109%2FTPAMI.2002.1017616&amp;rft.jtitle=IEEE+Trans.+Pattern+Analysis+and+Machine+Intelligence&amp;rft.pages=881-892&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=24" class="Z3988"><span style="display:none;">&#160;</span></span> <span class="citation-comment" style="display:none; color:#33aa33">CS1 maint: Multiple names: authors list (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Multiple_names:_authors_list" title="Category:CS1 maint: Multiple names: authors list">link</a>)</span></span></li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation journal">Drake, Jonathan (2012). <a rel="nofollow" class="external text" href="http://opt.kyb.tuebingen.mpg.de/papers/opt2012_paper_13.pdf">"Accelerated k-means with adaptive distance bounds"</a> <span style="font-size:85%;">(PDF)</span>. <i>the 5th NIPS Workshop on Optimization for Machine Learning, OPT2012</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Accelerated+k-means+with+adaptive+distance+bounds&amp;rft.aufirst=Jonathan&amp;rft.aulast=Drake&amp;rft.date=2012&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fopt.kyb.tuebingen.mpg.de%2Fpapers%2Fopt2012_paper_13.pdf&amp;rft.jtitle=the+5th+NIPS+Workshop+on+Optimization+for+Machine+Learning%2C+OPT2012&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><cite class="citation journal">Dhillon, I. S.; Modha, D. M. (2001). "Concept decompositions for large sparse text data using clustering". <i>Machine Learning</i>. <b>42</b> (1): 143–175. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1023%2Fa%3A1007612920971">10.1023/a:1007612920971</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Concept+decompositions+for+large+sparse+text+data+using+clustering&amp;rft.aufirst=I.+S.&amp;rft.aulast=Dhillon&amp;rft.au=Modha%2C+D.+M.&amp;rft.date=2001&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1023%2Fa%3A1007612920971&amp;rft.issue=1&amp;rft.jtitle=Machine+Learning&amp;rft.pages=143-175&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=42" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text">Steinbach, M., Karypis, G., &amp; Kumar, V. (2000, August). A comparison of document clustering techniques. In KDD workshop on text mining (Vol. 400, No. 1, pp. 525-526).</span></li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text">Pelleg, D., &amp; Moore, A. W. (2000, June). X-means: Extending K-means with Efficient Estimation of the Number of Clusters. In ICML (Vol. 1).</span></li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text">Hamerly, G., &amp; Elkan, C. (2004). Learning the k in k-means. Advances in neural information processing systems, 16, 281.</span></li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text"><cite class="citation journal">Amorim, R.C.; Mirkin, B. (2012). "Minkowski Metric, Feature Weighting and Anomalous Cluster Initialisation in K-Means Clustering". <i>Pattern Recognition</i>. <b>45</b> (3): 1061–1075. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1016%2Fj.patcog.2011.08.012">10.1016/j.patcog.2011.08.012</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Minkowski+Metric%2C+Feature+Weighting+and+Anomalous+Cluster+Initialisation+in+K-Means+Clustering&amp;rft.aufirst=R.C.&amp;rft.aulast=Amorim&amp;rft.au=Mirkin%2C+B.&amp;rft.date=2012&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1016%2Fj.patcog.2011.08.012&amp;rft.issue=3&amp;rft.jtitle=Pattern+Recognition&amp;rft.pages=1061-1075&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=45" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><cite class="citation journal">Amorim, R.C.; Hennig, C. (2015). "Recovering the number of clusters in data sets with noise features using feature rescaling factors". <i>Information Sciences</i>. <b>324</b>: 126–145. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1016%2Fj.ins.2015.06.039">10.1016/j.ins.2015.06.039</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Recovering+the+number+of+clusters+in+data+sets+with+noise+features+using+feature+rescaling+factors&amp;rft.aufirst=R.C.&amp;rft.au=Hennig%2C+C.&amp;rft.aulast=Amorim&amp;rft.date=2015&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ins.2015.06.039&amp;rft.jtitle=Information+Sciences&amp;rft.pages=126-145&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=324" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><cite class="citation conference">Sculley, David (2010). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=1772862">"Web-scale k-means clustering"</a>. <i>Proceedings of the 19th international conference on World wide web</i>. ACM. pp.&#160;1177–1178<span class="reference-accessdate">. Retrieved <span class="nowrap">2016-12-21</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Web-scale+k-means+clustering&amp;rft.aufirst=David&amp;rft.aulast=Sculley&amp;rft.btitle=Proceedings+of+the+19th+international+conference+on+World+wide+web&amp;rft.date=2010&amp;rft.genre=conference&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1772862&amp;rft.pages=1177-1178&amp;rft.pub=ACM&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Mirkes2011-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-Mirkes2011_29-0">^</a></b></span> <span class="reference-text"><cite class="citation web">Mirkes, E.M. <a rel="nofollow" class="external text" href="http://www.math.le.ac.uk/people/ag153/homepage/KmeansKmedoids/Kmeans_Kmedoids.html">"K-means and K-medoids applet."</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.aufirst=E.M.&amp;rft.aulast=Mirkes&amp;rft.btitle=K-means+and+K-medoids+applet.&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Fwww.math.le.ac.uk%2Fpeople%2Fag153%2Fhomepage%2FKmeansKmedoids%2FKmeans_Kmedoids.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"><cite class="citation journal">Honarkhah, M; Caers, J (2010). "Stochastic Simulation of Patterns Using Distance-Based Pattern Modeling". <i>Mathematical Geosciences</i>. <b>42</b>: 487–517. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2Fs11004-010-9276-7">10.1007/s11004-010-9276-7</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Stochastic+Simulation+of+Patterns+Using+Distance-Based+Pattern+Modeling&amp;rft.au=Caers%2C+J&amp;rft.aufirst=M&amp;rft.aulast=Honarkhah&amp;rft.date=2010&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2Fs11004-010-9276-7&amp;rft.jtitle=Mathematical+Geosciences&amp;rft.pages=487-517&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=42" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Coates2012-31"><span class="mw-cite-backlink">^ <a href="#cite_ref-Coates2012_31-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Coates2012_31-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-Coates2012_31-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation encyclopaedia">Coates, Adam; Ng, Andrew Y. (2012). <a rel="nofollow" class="external text" href="https://cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf">"Learning feature representations with k-means"</a> <span style="font-size:85%;">(PDF)</span>. In G. Montavon, G. B. Orr, K.-R. Müller. <i>Neural Networks: Tricks of the Trade</i>. Springer.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Learning+feature+representations+with+k-means&amp;rft.aufirst=Adam&amp;rft.aulast=Coates&amp;rft.au=Ng%2C+Andrew+Y.&amp;rft.btitle=Neural+Networks%3A+Tricks+of+the+Trade&amp;rft.date=2012&amp;rft.genre=bookitem&amp;rft_id=https%3A%2F%2Fcs.stanford.edu%2F~acoates%2Fpapers%2Fcoatesng_nntot2012.pdf&amp;rft.pub=Springer&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span> <span class="citation-comment" style="display:none; color:#33aa33">CS1 maint: Uses editors parameter (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_editors_parameter" title="Category:CS1 maint: Uses editors parameter">link</a>)</span></span></li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><cite class="citation conference">Csurka, Gabriella; Dance, Christopher C.; Fan, Lixin; Willamowski, Jutta; Bray, Cédric (2004). <a rel="nofollow" class="external text" href="http://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/csurka-eccv-04.pdf"><i>Visual categorization with bags of keypoints</i></a> <span style="font-size:85%;">(PDF)</span>. ECCV Workshop on Statistical Learning in Computer Vision.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.au=Bray%2C+C%C3%A9dric&amp;rft.au=Dance%2C+Christopher+C.&amp;rft.au=Fan%2C+Lixin&amp;rft.aufirst=Gabriella&amp;rft.aulast=Csurka&amp;rft.au=Willamowski%2C+Jutta&amp;rft.btitle=Visual+categorization+with+bags+of+keypoints&amp;rft.date=2004&amp;rft.genre=conference&amp;rft_id=http%3A%2F%2Fwww.cs.cmu.edu%2F~efros%2Fcourses%2FLBMV07%2FPapers%2Fcsurka-eccv-04.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-coates2011-33"><span class="mw-cite-backlink">^ <a href="#cite_ref-coates2011_33-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-coates2011_33-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Coates, Adam; Lee, Honglak; Ng, Andrew Y. (2011). <a rel="nofollow" class="external text" href="http://www.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf"><i>An analysis of single-layer networks in unsupervised feature learning</i></a> <span style="font-size:85%;">(PDF)</span>. International Conference on Artificial Intelligence and Statistics (AISTATS).</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.aufirst=Adam&amp;rft.aulast=Coates&amp;rft.au=Lee%2C+Honglak&amp;rft.au=Ng%2C+Andrew+Y.&amp;rft.btitle=An+analysis+of+single-layer+networks+in+unsupervised+feature+learning&amp;rft.date=2011&amp;rft.genre=conference&amp;rft_id=http%3A%2F%2Fwww.stanford.edu%2F~acoates%2Fpapers%2Fcoatesleeng_aistats_2011.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-schwenker-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-schwenker_34-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Schwenker, Friedhelm; Kestler, Hans A.; Palm, Günther (2001). "Three learning phases for radial-basis-function networks". <i>Neural Networks</i>. <b>14</b>: 439–458. <a href="https://en.wikipedia.org/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="plainlinks"><a rel="nofollow" class="external text" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.109.312">10.1.1.109.312</a><span style="margin-left:0.1em"><img alt="Freely accessible" src="http://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span></span>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1016%2Fs0893-6080(01)00027-2">10.1016/s0893-6080(01)00027-2</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Three+learning+phases+for+radial-basis-function+networks&amp;rft.aufirst=Friedhelm&amp;rft.au=Kestler%2C+Hans+A.&amp;rft.aulast=Schwenker&amp;rft.au=Palm%2C+G%C3%BCnther&amp;rft.date=2001&amp;rft.genre=article&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.109.312&amp;rft_id=info%3Adoi%2F10.1016%2Fs0893-6080%2801%2900027-2&amp;rft.jtitle=Neural+Networks&amp;rft.pages=439-458&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=14" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-35"><span class="mw-cite-backlink"><b><a href="#cite_ref-35">^</a></b></span> <span class="reference-text"><cite class="citation conference">Lin, Dekang; Wu, Xiaoyun (2009). <a rel="nofollow" class="external text" href="http://www.aclweb.org/anthology/P/P09/P09-1116.pdf"><i>Phrase clustering for discriminative learning</i></a> <span style="font-size:85%;">(PDF)</span>. Annual Meeting of the <a href="https://en.wikipedia.org/wiki/Association_for_Computational_Linguistics" title="Association for Computational Linguistics">ACL</a> and IJCNLP. pp.&#160;1030–1038.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.aufirst=Dekang&amp;rft.aulast=Lin&amp;rft.au=Wu%2C+Xiaoyun&amp;rft.btitle=Phrase+clustering+for+discriminative+learning&amp;rft.date=2009&amp;rft.genre=conference&amp;rft_id=http%3A%2F%2Fwww.aclweb.org%2Fanthology%2FP%2FP09%2FP09-1116.pdf&amp;rft.pages=1030-1038&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><cite class="citation book">Press, WH; Teukolsky, SA; Vetterling, WT; Flannery, BP (2007). <a rel="nofollow" class="external text" href="http://apps.nrbook.com/empanel/index.html#pg=842">"Section 16.1. Gaussian Mixture Models and k-Means Clustering"</a>. <i>Numerical Recipes: The Art of Scientific Computing</i> (3rd ed.). New York: Cambridge University Press. <a href="https://en.wikipedia.org/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="https://en.wikipedia.org/wiki/Special:BookSources/978-0-521-88068-8" title="Special:BookSources/978-0-521-88068-8">978-0-521-88068-8</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Section+16.1.+Gaussian+Mixture+Models+and+k-Means+Clustering&amp;rft.aufirst=WH&amp;rft.au=Flannery%2C+BP&amp;rft.aulast=Press&amp;rft.au=Teukolsky%2C+SA&amp;rft.au=Vetterling%2C+WT&amp;rft.btitle=Numerical+Recipes%3A+The+Art+of+Scientific+Computing&amp;rft.date=2007&amp;rft.edition=3rd&amp;rft.genre=bookitem&amp;rft_id=http%3A%2F%2Fapps.nrbook.com%2Fempanel%2Findex.html%23pg%3D842&amp;rft.isbn=978-0-521-88068-8&amp;rft.place=New+York&amp;rft.pub=Cambridge+University+Press&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><cite class="citation journal">Aharon, Michal; Elad, Michael; Bruckstein, Alfred (2006). <a rel="nofollow" class="external text" href="http://intranet.daiict.ac.in/~ajit_r/IT530/KSVD_IEEETSP.pdf">"K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation"</a> <span style="font-size:85%;">(PDF)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=K-SVD%3A+An+Algorithm+for+Designing+Overcomplete+Dictionaries+for+Sparse+Representation&amp;rft.au=Bruckstein%2C+Alfred&amp;rft.au=Elad%2C+Michael&amp;rft.aufirst=Michal&amp;rft.aulast=Aharon&amp;rft.date=2006&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fintranet.daiict.ac.in%2F~ajit_r%2FIT530%2FKSVD_IEEETSP.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text"><cite class="citation journal">H. Zha, C. Ding, M. Gu, X. He and H.D. Simon (Dec 2001). <a rel="nofollow" class="external text" href="http://ranger.uta.edu/~chqding/papers/Zha-Kmeans.pdf">"Spectral Relaxation for K-means Clustering"</a> <span style="font-size:85%;">(PDF)</span>. <i>Neural Information Processing Systems vol.14 (NIPS 2001)</i>. Vancouver, Canada: 1057–1064.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Spectral+Relaxation+for+K-means+Clustering&amp;rft.date=2001-12&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Franger.uta.edu%2F~chqding%2Fpapers%2FZha-Kmeans.pdf&amp;rft.jtitle=Neural+Information+Processing+Systems+vol.14+%28NIPS+2001%29&amp;rft.pages=1057-1064&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span> <span class="citation-comment" style="display:none; color:#33aa33">CS1 maint: Uses authors parameter (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">link</a>)</span></span></li>
<li id="cite_note-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-39">^</a></b></span> <span class="reference-text"><cite class="citation journal">Chris Ding and Xiaofeng He (July 2004). <a rel="nofollow" class="external text" href="http://ranger.uta.edu/~chqding/papers/KmeansPCA1.pdf">"K-means Clustering via Principal Component Analysis"</a> <span style="font-size:85%;">(PDF)</span>. <i>Proc. of Int'l Conf. Machine Learning (ICML 2004)</i>: 225–232.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=K-means+Clustering+via+Principal+Component+Analysis&amp;rft.date=2004-07&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Franger.uta.edu%2F~chqding%2Fpapers%2FKmeansPCA1.pdf&amp;rft.jtitle=Proc.+of+Int%27l+Conf.+Machine+Learning+%28ICML+2004%29&amp;rft.pages=225-232&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span> <span class="citation-comment" style="display:none; color:#33aa33">CS1 maint: Uses authors parameter (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">link</a>)</span></span></li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><cite class="citation journal">Drineas, P.; A. Frieze; R. Kannan; S. Vempala; V. Vinay (2004). <a rel="nofollow" class="external text" href="http://www.cc.gatech.edu/~vempala/papers/dfkvv.pdf">"Clustering large graphs via the singular value decomposition"</a> <span style="font-size:85%;">(PDF)</span>. <i>Machine learning</i>. <b>56</b>: 9–33. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1023%2Fb%3Amach.0000033113.59016.96">10.1023/b:mach.0000033113.59016.96</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2012-08-02</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Clustering+large+graphs+via+the+singular+value+decomposition&amp;rft.au=A.+Frieze&amp;rft.aufirst=P.&amp;rft.aulast=Drineas&amp;rft.au=R.+Kannan&amp;rft.au=S.+Vempala&amp;rft.au=V.+Vinay&amp;rft.date=2004&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.cc.gatech.edu%2F~vempala%2Fpapers%2Fdfkvv.pdf&amp;rft_id=info%3Adoi%2F10.1023%2Fb%3Amach.0000033113.59016.96&amp;rft.jtitle=Machine+learning&amp;rft.pages=9-33&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=56" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Cohen, M.; S. Elder; C. Musco; C. Musco; M. Persu (2014). "Dimensionality reduction for k-means clustering and low rank approximation (Appendix B)". <a href="https://en.wikipedia.org/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="http://arxiv.org/abs/1410.6801">1410.6801</a><span style="margin-left:0.1em"><img alt="Freely accessible" src="http://upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Dimensionality+reduction+for+k-means+clustering+and+low+rank+approximation+%28Appendix+B%29&amp;rft.au=C.+Musco&amp;rft.au=C.+Musco&amp;rft.aufirst=M.&amp;rft.aulast=Cohen&amp;rft.au=M.+Persu&amp;rft.au=S.+Elder&amp;rft.date=2014&amp;rft.genre=preprint&amp;rft_id=info%3Aarxiv%2F1410.6801&amp;rft.jtitle=arXiv&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Little2011-42"><span class="mw-cite-backlink">^ <a href="#cite_ref-Little2011_42-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Little2011_42-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Little, M.A.; Jones, N.S. (2011). <a rel="nofollow" class="external text" href="http://www.maxlittle.net/publications/pwc_filtering_arxiv.pdf">"Generalized Methods and Solvers for Piecewise Constant Signals: Part I"</a> <span style="font-size:85%;">(PDF)</span>. <i><a href="https://en.wikipedia.org/wiki/Proceedings_of_the_Royal_Society_A" class="mw-redirect" title="Proceedings of the Royal Society A">Proceedings of the Royal Society A</a></i>. <b>467</b>: 3088–3114. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1098%2Frspa.2010.0671">10.1098/rspa.2010.0671</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=Generalized+Methods+and+Solvers+for+Piecewise+Constant+Signals%3A+Part+I&amp;rft.aufirst=M.A.&amp;rft.au=Jones%2C+N.S.&amp;rft.aulast=Little&amp;rft.date=2011&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.maxlittle.net%2Fpublications%2Fpwc_filtering_arxiv.pdf&amp;rft_id=info%3Adoi%2F10.1098%2Frspa.2010.0671&amp;rft.jtitle=Proceedings+of+the+Royal+Society+A&amp;rft.pages=3088-3114&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.volume=467" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text"><cite class="citation journal">Alon Vinnikov and Shai Shalev-Shwartz (2014). <a rel="nofollow" class="external text" href="http://www.cs.huji.ac.il/~shais/papers/KmeansICA_ICML2014.pdf">"K-means Recovers ICA Filters when Independent Components are Sparse"</a> <span style="font-size:85%;">(PDF)</span>. <i>Proc. of Int'l Conf. Machine Learning (ICML 2014)</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=K-means+Recovers+ICA+Filters+when+Independent+Components+are+Sparse&amp;rft.date=2014&amp;rft.genre=article&amp;rft_id=http%3A%2F%2Fwww.cs.huji.ac.il%2F~shais%2Fpapers%2FKmeansICA_ICML2014.pdf&amp;rft.jtitle=Proc.+of+Int%27l+Conf.+Machine+Learning+%28ICML+2014%29&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span> <span class="citation-comment" style="display:none; color:#33aa33">CS1 maint: Uses authors parameter (<a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">link</a>)</span></span></li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text"><cite class="citation journal">Kriegel, Hans-Peter; Schubert, Erich; Zimek, Arthur (2016). "The (black) art of runtime evaluation: Are we comparing algorithms or implementations?". <i>Knowledge and Information Systems</i>. <a href="https://en.wikipedia.org/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="http://dx.doi.org/10.1007%2Fs10115-016-1004-2">10.1007/s10115-016-1004-2</a>. <a href="https://en.wikipedia.org/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="http://www.worldcat.org/issn/0219-1377">0219-1377</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AK-means+clustering&amp;rft.atitle=The+%28black%29+art+of+runtime+evaluation%3A+Are+we+comparing+algorithms+or+implementations%3F&amp;rft.aufirst=Hans-Peter&amp;rft.aulast=Kriegel&amp;rft.au=Schubert%2C+Erich&amp;rft.au=Zimek%2C+Arthur&amp;rft.date=2016&amp;rft.genre=article&amp;rft_id=info%3Adoi%2F10.1007%2Fs10115-016-1004-2&amp;rft.issn=0219-1377&amp;rft.jtitle=Knowledge+and+Information+Systems&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
</ol>
</div>


<!-- 
NewPP limit report
Parsed by mw1177
Cached time: 20161231010412
Cache expiry: 2592000
Dynamic content: false
CPU time usage: 0.532 seconds
Real time usage: 0.901 seconds
Preprocessor visited node count: 2630/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 106484/2097152 bytes
Template argument size: 1863/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 3/500
Lua time usage: 0.257/10.000 seconds
Lua memory usage: 5.24 MB/50 MB
-->

<!-- 
Transclusion expansion time report (%,ms,calls,template)
100.00%  477.963      1 - -total
 62.83%  300.317      1 - Template:Reflist
 24.19%  115.619     23 - Template:Cite_journal
 14.70%   70.245      9 - Template:Cite_conference
 12.98%   62.036      2 - Template:Citation_needed
 11.91%   56.925      2 - Template:Fix
  9.16%   43.796      1 - Template:Machine_learning_bar
  8.74%   41.756      1 - Template:Sidebar_with_collapsible_lists
  6.35%   30.357      4 - Template:Category_handler
  4.84%   23.118      2 - Template:Delink
-->

<!-- Saved in parser cache with key enwiki:pcache:idhash:1860407-0!*!0!!en!4!*!math=5 and timestamp 20161231010411 and revision id 756148216
 -->
<noscript><img src="http://en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;oldid=756148216">https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;oldid=756148216</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="https://en.wikipedia.org/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="https://en.wikipedia.org/wiki/Category:Data_clustering_algorithms" title="Category:Data clustering algorithms">Data clustering algorithms</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Statistical_algorithms" title="Category:Statistical algorithms">Statistical algorithms</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="https://en.wikipedia.org/wiki/Category:CS1_French-language_sources_(fr)" title="Category:CS1 French-language sources (fr)">CS1 French-language sources (fr)</a></li><li><a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Multiple_names:_authors_list" title="Category:CS1 maint: Multiple names: authors list">CS1 maint: Multiple names: authors list</a></li><li><a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_editors_parameter" title="Category:CS1 maint: Uses editors parameter">CS1 maint: Uses editors parameter</a></li><li><a href="https://en.wikipedia.org/wiki/Category:CS1_maint:_Uses_authors_parameter" title="Category:CS1 maint: Uses authors parameter">CS1 maint: Uses authors parameter</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_November_2016" title="Category:Articles with unsourced statements from November 2016">Articles with unsourced statements from November 2016</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_with_unsourced_statements_from_March_2014" title="Category:Articles with unsourced statements from March 2014">Articles with unsourced statements from March 2014</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Wikipedia_articles_needing_clarification_from_February_2016" title="Category:Wikipedia articles needing clarification from February 2016">Wikipedia articles needing clarification from February 2016</a></li><li><a href="https://en.wikipedia.org/wiki/Category:All_Wikipedia_articles_needing_clarification" title="Category:All Wikipedia articles needing clarification">All Wikipedia articles needing clarification</a></li><li><a href="https://en.wikipedia.org/wiki/Category:Articles_using_small_message_boxes" title="Category:Articles using small message boxes">Articles using small message boxes</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>

			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="https://en.wikipedia.org/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="https://en.wikipedia.org/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="https://en.wikipedia.org/w/index.php?title=Special:CreateAccount&amp;returnto=K-means+clustering" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="https://en.wikipedia.org/w/index.php?title=Special:UserLogin&amp;returnto=K-means+clustering" title="You're encouraged to log in; however, it's not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
															<li  id="ca-nstab-main" class="selected"><span><a href="K-means_clustering.html"  title="View the content page [c]" accesskey="c">Article</a></span></li>
															<li  id="ca-talk"><span><a href="https://en.wikipedia.org/wiki/Talk:K-means_clustering"  title="Discussion about the content page [t]" accesskey="t" rel="discussion">Talk</a></span></li>
													</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<h3 id="p-variants-label">
							<span>Variants</span><a href="#"></a>
						</h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
															<li id="ca-view" class="selected"><span><a href="K-means_clustering.html" >Read</a></span></li>
															<li id="ca-edit"><span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=edit"  title="Edit this page [e]" accesskey="e">Edit</a></span></li>
															<li id="ca-history" class="collapsible"><span><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=history"  title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>
													</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<h3 id="p-cactions-label"><span>More</span><a href="#"></a></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>

						<form action="https://en.wikipedia.org/w/index.php" id="searchform">
							<div id="simpleSearch">
							<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="https://en.wikipedia.org/wiki/Main_Page"  title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id='p-navigation' aria-labelledby='p-navigation-label'>
			<h3 id='p-navigation-label'>Navigation</h3>

			<div class="body">
									<ul>
						<li id="n-mainpage-description"><a href="https://en.wikipedia.org/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="https://en.wikipedia.org/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="https://en.wikipedia.org/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="https://en.wikipedia.org/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="https://en.wikipedia.org/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="http://shop.wikimedia.org/" title="Visit the Wikipedia store">Wikipedia store</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-interaction' aria-labelledby='p-interaction-label'>
			<h3 id='p-interaction-label'>Interaction</h3>

			<div class="body">
									<ul>
						<li id="n-help"><a href="https://en.wikipedia.org/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="https://en.wikipedia.org/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="https://en.wikipedia.org/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="http://en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-tb' aria-labelledby='p-tb-label'>
			<h3 id='p-tb-label'>Tools</h3>

			<div class="body">
									<ul>
						<li id="t-whatlinkshere"><a href="https://en.wikipedia.org/wiki/Special:WhatLinksHere/K-means_clustering" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="https://en.wikipedia.org/wiki/Special:RecentChangesLinked/K-means_clustering" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="https://en.wikipedia.org/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="https://en.wikipedia.org/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;oldid=756148216" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Q310401" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="https://en.wikipedia.org/w/index.php?title=Special:CiteThisPage&amp;page=K-means_clustering&amp;id=756148216" title="Information on how to cite this page">Cite this page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-coll-print_export' aria-labelledby='p-coll-print_export-label'>
			<h3 id='p-coll-print_export-label'>Print/export</h3>

			<div class="body">
									<ul>
						<li id="coll-create_a_book"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=K-means+clustering">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="https://en.wikipedia.org/w/index.php?title=Special:Book&amp;bookcmd=render_article&amp;arttitle=K-means+clustering&amp;returnto=K-means+clustering&amp;oldid=756148216&amp;writer=rdf2latex">Download as PDF</a></li><li id="t-print"><a href="https://en.wikipedia.org/w/index.php?title=K-means_clustering&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-lang' aria-labelledby='p-lang-label'>
			<h3 id='p-lang-label'>Languages</h3>

			<div class="body">
									<ul>
						<li class="interlanguage-link interwiki-ar"><a href="https://ar.wikipedia.org/wiki/خوارزمية_تصنيفية" title="خوارزمية تصنيفية – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target">العربية</a></li><li class="interlanguage-link interwiki-ca"><a href="https://ca.wikipedia.org/wiki/Algorisme_k-means" title="Algorisme k-means – Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target">Català</a></li><li class="interlanguage-link interwiki-cs"><a href="https://cs.wikipedia.org/wiki/K-means" title="K-means – Czech" lang="cs" hreflang="cs" class="interlanguage-link-target">Čeština</a></li><li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/K-Means-Algorithmus" title="K-Means-Algorithmus – German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li><li class="interlanguage-link interwiki-el"><a href="https://el.wikipedia.org/wiki/Ομαδοποίηση_Κ-μέσων" title="Ομαδοποίηση Κ-μέσων – Greek" lang="el" hreflang="el" class="interlanguage-link-target">Ελληνικά</a></li><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/K-means" title="K-means – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/K-moyennes" title="K-moyennes – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li><li class="interlanguage-link interwiki-ko"><a href="https://ko.wikipedia.org/wiki/K-평균_알고리즘" title="K-평균 알고리즘 – Korean" lang="ko" hreflang="ko" class="interlanguage-link-target">한국어</a></li><li class="interlanguage-link interwiki-id"><a href="https://id.wikipedia.org/wiki/K-means" title="K-means – Indonesian" lang="id" hreflang="id" class="interlanguage-link-target">Bahasa Indonesia</a></li><li class="interlanguage-link interwiki-it"><a href="https://it.wikipedia.org/wiki/K-means" title="K-means – Italian" lang="it" hreflang="it" class="interlanguage-link-target">Italiano</a></li><li class="interlanguage-link interwiki-he"><a href="https://he.wikipedia.org/wiki/אלגוריתם_k-מרכזים" title="אלגוריתם k-מרכזים – Hebrew" lang="he" hreflang="he" class="interlanguage-link-target">עברית</a></li><li class="interlanguage-link interwiki-lt"><a href="https://lt.wikipedia.org/wiki/K-vidurkių_klasterizavimas" title="K-vidurkių klasterizavimas – Lithuanian" lang="lt" hreflang="lt" class="interlanguage-link-target">Lietuvių</a></li><li class="interlanguage-link interwiki-ja"><a href="https://ja.wikipedia.org/wiki/K平均法" title="K平均法 – Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target">日本語</a></li><li class="interlanguage-link interwiki-no"><a href="https://no.wikipedia.org/wiki/K-means" title="K-means – Norwegian" lang="no" hreflang="no" class="interlanguage-link-target">Norsk bokmål</a></li><li class="interlanguage-link interwiki-pl"><a href="https://pl.wikipedia.org/wiki/Algorytm_centroidów" title="Algorytm centroidów – Polish" lang="pl" hreflang="pl" class="interlanguage-link-target">Polski</a></li><li class="interlanguage-link interwiki-pt"><a href="https://pt.wikipedia.org/wiki/K-means" title="K-means – Portuguese" lang="pt" hreflang="pt" class="interlanguage-link-target">Português</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/K-means" title="K-means – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">Русский</a></li><li class="interlanguage-link interwiki-sr"><a href="https://sr.wikipedia.org/wiki/Klasterizacija_metodom_K-srednjih_vrednosti" title="Klasterizacija metodom K-srednjih vrednosti – Serbian" lang="sr" hreflang="sr" class="interlanguage-link-target">Српски / srpski</a></li><li class="interlanguage-link interwiki-th"><a href="https://th.wikipedia.org/wiki/การแบ่งกลุ่มข้อมูลแบบเคมีน" title="การแบ่งกลุ่มข้อมูลแบบเคมีน – Thai" lang="th" hreflang="th" class="interlanguage-link-target">ไทย</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/Кластеризація_методом_к–середніх" title="Кластеризація методом к–середніх – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target">Українська</a></li><li class="interlanguage-link interwiki-ur"><a href="https://ur.wikipedia.org/wiki/K-means_clustering" title="K-means clustering – Urdu" lang="ur" hreflang="ur" class="interlanguage-link-target">اردو</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/K-平均算法" title="K-平均算法 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li><li class="uls-p-lang-dummy"><a href="#"></a></li>					</ul>
				<div class='after-portlet after-portlet-lang'><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Q310401#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last modified on 22 December 2016, at 08:54.</li>
											<li id="footer-info-copyright">Text is available under the <a rel="license" href="http://en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="http://creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="http://wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="http://wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="http://www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
									</ul>
							<ul id="footer-places">
											<li id="footer-places-privacy"><a href="https://wikimediafoundation.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
											<li id="footer-places-about"><a href="https://en.wikipedia.org/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
											<li id="footer-places-disclaimer"><a href="https://en.wikipedia.org/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
											<li id="footer-places-contact"><a href="http://en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
											<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
											<li id="footer-places-cookiestatement"><a href="https://wikimediafoundation.org/wiki/Cookie_statement">Cookie statement</a></li>
											<li id="footer-places-mobileview"><a href="http://en.m.wikipedia.org/w/index.php?title=K-means_clustering&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
									</ul>
										<ul id="footer-icons" class="noprint">
											<li id="footer-copyrightico">
							<a href="https://wikimediafoundation.org/"><img src="https://en.wikipedia.org/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>						</li>
											<li id="footer-poweredbyico">
							<a href="http://www.mediawiki.org/"><img src="https://en.wikipedia.org/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>						</li>
									</ul>
						<div style="clear:both"></div>
		</div>
		<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":1059,"wgHostname":"mw1177"});});</script>
	</body>

Providence Salumu
</html>
